Fujitsu C/C++ Version 4.7.0   Fri Dec  2 03:55:18 2022
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj2
  Source file       : ../src/ComputeSYMGS.cpp
(line-no.)(optimize)
        1             /*
        2              *
        3              *  SPDX-License-Identifier: Apache-2.0
        4              *
        5              *  Copyright (C) 2019, Arm Limited and contributors
        6              *
        7              *  Licensed under the Apache License, Version 2.0 (the "License");
        8              *  you may not use this file except in compliance with the License.
        9              *  You may obtain a copy of the License at
       10              *
       11              *      http://www.apache.org/licenses/LICENSE-2.0
       12              *
       13              *  Unless required by applicable law or agreed to in writing, software
       14              *  distributed under the License is distributed on an "AS IS" BASIS,
       15              *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       16              *  See the License for the specific language governing permissions and
       17              *  limitations under the License.
       18              *
       19              */
       20             
       21             //@HEADER
       22             // ***************************************************
       23             //
       24             // HPCG: High Performance Conjugate Gradient Benchmark
       25             //
       26             // Contact:
       27             // Michael A. Heroux ( maherou@sandia.gov)
       28             // Jack Dongarra     (dongarra@eecs.utk.edu)
       29             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       30             //
       31             // ***************************************************
       32             //@HEADER
       33             
       34             /*!
       35              @file ComputeSYMGS.cpp
       36             
       37              HPCG routine
       38              */
       39             
       40             #include "ComputeSYMGS.hpp"
       41             #include "ComputeSYMGS_ref.hpp"
       42             #ifndef HPCG_NO_MPI
       43             #include "ExchangeHalo.hpp"
       44             #endif
       45             
       46             #include "likwid_instrumentation.hpp"
       47             
       48             #ifdef HPCG_MAN_OPT_SCHEDULE_ON
       49             	#define SCHEDULE(T)	schedule(T)
       50             #else
       51             	#define SCHEDULE(T)
       52             #endif
       53             
       54             /**************************************************************************************************/
       55             /**************************************************************************************************/
       56             /**************************************************************************************************/
       57             /* SVE IMPLEMENTATIONS                                                                            */
       58             /**************************************************************************************************/
       59             /**************************************************************************************************/
       60             /**************************************************************************************************/
       61             
       62             #ifdef HPCG_USE_SVE
       63             #include "arm_sve.h"
       64             
       65             /*
       66              * TDG VERSION
       67              */
       68             int ComputeSYMGS_TDG_SVE(const SparseMatrix & A, const Vector & r, Vector & x, TraceData &trace) {
       69             	assert(x.localLength == A.localNumberOfColumns);
       70             
       71             #ifndef HPCG_NO_MPI
       72             	ExchangeHalo(A, x);
       73             #endif
       74             
       75             	const double * const rv = r.values;
       76             	double * const xv = x.values;
       77             	double **matrixDiagonal = A.matrixDiagonal;
       78             
       79             LIKWID_START(trace.enabled, "symgs_tdg");
       80             
       81             #pragma statement scache_isolate_way L2=10
       82             #pragma statement scache_isolate_assign xv
       83             	/*
       84             	 * FORWARD SWEEP
       85             	 */
       86    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
       87             #ifndef HPCG_NO_OPENMP
       88             #pragma omp parallel for SCHEDULE(runtime)
       89             #endif
       90             		//#pragma loop nounroll
       91             		#pragma loop nounroll_and_jam
       92   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
       93   p         			local_int_t row = A.tdg[l][i];
       94   p         			const double * const currentValues = A.matrixValues[row];
       95   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
       96   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
       97   p         			const double currentDiagonal = matrixDiagonal[row][0];
       98   p         			svfloat64_t contribs = svdup_f64(0.0);
       99             
      100             			//#pragma loop nounroll
      101             			#pragma loop nounroll_and_jam
      102   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      103   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      104             				
      105   p      s  				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      106   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      107   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      108             
      109   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      110   p      s  			}
      111             
      112   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      113   p         			double sum = rv[row] - totalContribution;
      114             
      115   p         			sum += xv[row] * currentDiagonal;
      116   p         			xv[row] = sum / currentDiagonal;
      117   pi        		}
      118    i        	}
      119             
      120             	/*
      121             	 * BACKWARD SWEEP
      122             	 */
      123    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      124             #ifndef HPCG_NO_OPENMP
      125             #pragma omp parallel for SCHEDULE(runtime)
      126             #endif
      127             		#pragma loop nounroll_and_jam
      128   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      129   p         			local_int_t row = A.tdg[l][i];
      130   p         			const double * const currentValues = A.matrixValues[row];
      131   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      132   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      133   p         			const double currentDiagonal = matrixDiagonal[row][0];
      134   p         			svfloat64_t contribs = svdup_f64(0.0);
      135             
      136             			//#pragma loop nounroll
      137             			#pragma loop nounroll_and_jam
      138   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      139   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      140             				
      141   p      s  				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      142   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      143   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      144             
      145   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      146   p      s  			}
      147             
      148   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      149   p         			double sum = rv[row] - totalContribution;
      150             
      151   p         			sum += xv[row] * currentDiagonal;
      152   p         			xv[row] = sum / currentDiagonal;
      153   p         		}
      154             	}
      155             #pragma statement end_scache_isolate_assign
      156             #pragma statement end_scache_isolate_way
      157             
      158             LIKWID_STOP(trace.enabled, "symgs_tdg");
      159             
      160             	return 0;
      161             }
      162             /*
      163              * END OF TDG VERSION
      164              */
      165             
      166             /*
      167              * TDG FUSED SYMGS-SPMV VERSION
      168              */
      169             int ComputeFusedSYMGS_SPMV_SVE(const SparseMatrix & A, const Vector & r, Vector & x, Vector & y, TraceData& trace) {
      170             	assert(x.localLength == A.localNumberOfColumns);
      171             
      172             #ifndef HPCG_NO_MPI
      173             	ExchangeHalo(A, x);
      174             #endif
      175             
      176             	const double * const rv = r.values;
      177             	double * const xv = x.values;
      178             	double **matrixDiagonal = A.matrixDiagonal;
      179             	double * const yv = y.values;
      180             
      181             	/*
      182             	 * FORWARD SWEEP
      183             	 */
      184    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      185             #ifndef HPCG_NO_OPENMP
      186             #pragma omp parallel for SCHEDULE(runtime)
      187             #endif
      188   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      189   p         			local_int_t row = A.tdg[l][i];
      190   p         			const double * const currentValues = A.matrixValues[row];
      191   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      192   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      193   p         			const double currentDiagonal = matrixDiagonal[row][0];
      194   p         			svfloat64_t contribs = svdup_f64(0.0);
      195             
      196   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      197   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      198             				
      199   p      s  				svfloat64_t mtxValues = svld1(pg, &currentValues[j]);
      200   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      201   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      202             
      203   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      204   p      s  			}
      205             
      206   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      207   p         			double sum = rv[row] - totalContribution;
      208             
      209   p         			sum += xv[row] * currentDiagonal;
      210   p         			xv[row] = sum / currentDiagonal;
      211   pi        		}
      212    i        	}
      213             
      214             	/*
      215             	 * BACKWARD SWEEP
      216             	 */
      217    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      218             #ifndef HPCG_NO_OPENMP
      219             #pragma omp parallel for SCHEDULE(runtime)
      220             #endif
      221   pi        		for ( local_int_t i = A.tdg[l].size(); i >= 0; i-- ) {
      222   p         			local_int_t row = A.tdg[l][i];
      223   p         			const double * const currentValues = A.matrixValues[row];
      224   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      225   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      226   p         			const double currentDiagonal = matrixDiagonal[row][0];
      227   p         			svfloat64_t contribs = svdup_f64(0.0);
      228             
      229   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      230   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      231             				
      232   p      s  				svfloat64_t mtxValues = svld1(pg, &currentValues[j]);
      233   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      234   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      235             
      236   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      237   p      s  			}
      238             
      239   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      240   p         			totalContribution -= xv[row] * currentDiagonal; // remove diagonal contribution
      241   p         			double sum = rv[row] - totalContribution; // substract contributions from RHS
      242   p         			xv[row] = sum / currentDiagonal; // update row
      243             
      244             			// SPMV part
      245   p         			totalContribution += xv[row] * currentDiagonal; // add updated diagonal contribution
      246   p         			yv[row] = totalContribution; // update SPMV output vector
      247             			
      248   p         		}
      249             	}
      250             
      251             	return 0;
      252             }
      253             /*
      254              * END OF TDG FUSED SYMGS-SPMV VERSION
      255              */
      256             
      257             /*
      258              * BLOCK COLORED VERSION
      259              */
      260             int ComputeSYMGS_BLOCK_SVE(const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
      261             	assert(x.localLength >= A.localNumberOfColumns);
      262             
      263             #ifndef HPCG_NO_MPI
      264             	ExchangeHalo(A, x);
      265             #endif
      266             
      267             	double **matrixDiagonal = A.matrixDiagonal;
      268             	const double * const rv = r.values;
      269             	double * const xv = x.values;
      270             	local_int_t firstBlock = 0;
      271    i        	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
      272             
      273             LIKWID_START(trace.enabled, "symgs_bc");		
      274             
      275             	/*
      276             	 * FORWARD SWEEP
      277             	 */
      278             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) { // for each color
      279             		if ( color > 0 ) {
      280    i        			firstBlock += A.numberOfBlocksInColor[color-1];
      281    i        			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
      282             		}
      283             #ifndef HPCG_NO_OPENMP
      284             #pragma omp parallel for SCHEDULE(runtime)
      285             #endif
      286   p         		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) { // for each superblock with the same color
      287   p         			local_int_t firstRow = block * A.blockSize;
      288   p         			local_int_t firstChunk = firstRow / A.chunkSize;
      289   p         			local_int_t lastChunk = (firstRow + A.blockSize * A.chunkSize) / A.chunkSize;
      290             
      291   p         			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) { // for each chunk of this super block
      292   p         				local_int_t first = A.chunkSize * chunk;
      293   p         				local_int_t last = first + A.chunkSize;
      294   p         				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      295   p         				local_int_t i = first;
      296   p         				if ( A.chunkSize == 4 ) {
      297   p         					const double * const currentValues0 = A.matrixValues[i  ];
      298   p         					const double * const currentValues1 = A.matrixValues[i+1];
      299   p         					const double * const currentValues2 = A.matrixValues[i+2];
      300   p         					const double * const currentValues3 = A.matrixValues[i+3];
      301             
      302   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      303   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      304   p         					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      305   p         					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      306             
      307   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      308   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      309   p         					const double currentDiagonal2 = matrixDiagonal[i+2][0];
      310   p         					const double currentDiagonal3 = matrixDiagonal[i+3][0];
      311             
      312   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      313   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      314   p         					svfloat64_t contribs2 = svdup_f64(0.0);
      315   p         					svfloat64_t contribs3 = svdup_f64(0.0);
      316             
      317   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      318   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      319             
      320   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      321   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      322   p      s  						svfloat64_t values2 = svld1_f64(pg, &currentValues2[j]);
      323   p      s  						svfloat64_t values3 = svld1_f64(pg, &currentValues3[j]);
      324             
      325   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      326   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      327   p      s  						svuint64_t indices2 = svld1sw_u64(pg, &currentColIndices2[j]);
      328   p      s  						svuint64_t indices3 = svld1sw_u64(pg, &currentColIndices3[j]);
      329             
      330   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      331   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      332   p      s  						svfloat64_t xvv2 = svld1_gather_u64index_f64(pg, xv, indices2);
      333   p      s  						svfloat64_t xvv3 = svld1_gather_u64index_f64(pg, xv, indices3);
      334             
      335   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      336   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1);
      337   p      s  						contribs2 = svmla_f64_m(pg, contribs2, xvv2, values2);
      338   p      s  						contribs3 = svmla_f64_m(pg, contribs3, xvv3, values3);
      339   p      s  					}
      340             
      341   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      342   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      343   p         					double totalContribution2 = svaddv_f64(svptrue_b64(), contribs2);
      344   p         					double totalContribution3 = svaddv_f64(svptrue_b64(), contribs3);
      345             
      346   p         					double sum0 = rv[i  ] - totalContribution0;
      347   p         					double sum1 = rv[i+1] - totalContribution1;
      348   p         					double sum2 = rv[i+2] - totalContribution2;
      349   p         					double sum3 = rv[i+3] - totalContribution3;
      350             
      351   p         					sum0 += xv[i  ] * currentDiagonal0;
      352   p         					sum1 += xv[i+1] * currentDiagonal1;
      353   p         					sum2 += xv[i+2] * currentDiagonal2;
      354   p         					sum3 += xv[i+3] * currentDiagonal3;
      355             
      356   p         					xv[i  ] = sum0 / currentDiagonal0;
      357   p         					xv[i+1] = sum1 / currentDiagonal1;
      358   p         					xv[i+2] = sum2 / currentDiagonal2;
      359   p         					xv[i+3] = sum3 / currentDiagonal3;
      360   p         				} else if ( A.chunkSize == 2 ) {
      361   p         					const double * const currentValues0 = A.matrixValues[i  ];
      362   p         					const double * const currentValues1 = A.matrixValues[i+1];
      363             
      364   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      365   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      366             
      367   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      368   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      369             
      370   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      371   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      372             
      373   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      374   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      375             
      376   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      377   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      378             
      379   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      380   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      381             
      382   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      383   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      384             
      385   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      386   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1);
      387   p      s  					}
      388             
      389   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      390   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      391             
      392   p         					double sum0 = rv[i  ] - totalContribution0;
      393   p         					double sum1 = rv[i+1] - totalContribution1;
      394             
      395   p         					sum0 += xv[i  ] * currentDiagonal0;
      396   p         					sum1 += xv[i+1] * currentDiagonal1;
      397             
      398   p         					xv[i  ] = sum0 / currentDiagonal0;
      399   p         					xv[i+1] = sum1 / currentDiagonal1;
      400   p         				} else { //A.chunkSize == 1
      401   p         					const double * const currentValues0 = A.matrixValues[i  ];
      402             
      403   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      404             
      405   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      406             
      407   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      408             
      409   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      410   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      411             
      412   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      413             
      414   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      415             
      416   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      417             
      418   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      419   p      s  					}
      420             
      421   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      422             
      423   p         					double sum0 = rv[i  ] - totalContribution0;
      424             
      425   p         					sum0 += xv[i  ] * currentDiagonal0;
      426             
      427   p         					xv[i  ] = sum0 / currentDiagonal0;
      428   p         				}
      429   p         			}
      430   p         		}
      431             	}
      432             
      433             	firstBlock = A.numberOfBlocks-1;
      434    i        	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
      435             	/*
      436             	 * BACKWARD SWEEP
      437             	 */
      438             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
      439             		if ( color < A.numberOfColors-1 ) {
      440    i        			firstBlock -= A.numberOfBlocksInColor[color+1];
      441    i        			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
      442             		}
      443             #ifndef HPCG_NO_OPENMP
      444             #pragma omp parallel for SCHEDULE(runtime)
      445             #endif
      446   p         		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) {
      447   p         			local_int_t firstRow = ((block+1) * A.blockSize) - 1;
      448   p         			local_int_t firstChunk = firstRow / A.chunkSize;
      449   p         			local_int_t lastChunk = (firstRow - A.blockSize * A.chunkSize) / A.chunkSize;
      450             
      451   p         			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
      452   p         				local_int_t first = A.chunkSize * chunk;
      453   p         				local_int_t last = first + A.chunkSize;
      454             
      455   p         				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      456   p         				local_int_t i = first;
      457   p         				if ( A.chunkSize == 4 ) {
      458   p         					const double * const currentValues3 = A.matrixValues[i+3];
      459   p         					const double * const currentValues2 = A.matrixValues[i+2];
      460   p         					const double * const currentValues1 = A.matrixValues[i+1];
      461   p         					const double * const currentValues0 = A.matrixValues[i  ];
      462             
      463   p         					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      464   p         					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      465   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      466   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      467             
      468   p         					const double currentDiagonal3 = matrixDiagonal[i+3][0];
      469   p         					const double currentDiagonal2 = matrixDiagonal[i+2][0];
      470   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      471   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      472             
      473   p         					svfloat64_t contribs3 = svdup_f64(0.0);
      474   p         					svfloat64_t contribs2 = svdup_f64(0.0);
      475   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      476   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      477             
      478   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      479   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      480             
      481   p      s  						svfloat64_t values3 = svld1_f64(pg, &currentValues3[j]);
      482   p      s  						svfloat64_t values2 = svld1_f64(pg, &currentValues2[j]);
      483   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      484   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      485             
      486   p      s  						svuint64_t indices3 = svld1sw_u64(pg, &currentColIndices3[j]);
      487   p      s  						svuint64_t indices2 = svld1sw_u64(pg, &currentColIndices2[j]);
      488   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      489   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      490             
      491   p      s  						svfloat64_t xvv3 = svld1_gather_u64index_f64(pg, xv, indices3);
      492   p      s  						svfloat64_t xvv2 = svld1_gather_u64index_f64(pg, xv, indices2);
      493   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      494   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      495             
      496   p      s  						contribs3 = svmla_f64_m(pg, contribs3, xvv3, values3 );
      497   p      s  						contribs2 = svmla_f64_m(pg, contribs2, xvv2, values2 );
      498   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1 );
      499   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      500   p      s  					}
      501             
      502   p         					double totalContribution3 = svaddv_f64(svptrue_b64(), contribs3);
      503   p         					double totalContribution2 = svaddv_f64(svptrue_b64(), contribs2);
      504   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      505   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      506             
      507   p         					double sum3 = rv[i+3] - totalContribution3;
      508   p         					double sum2 = rv[i+2] - totalContribution2;
      509   p         					double sum1 = rv[i+1] - totalContribution1;
      510   p         					double sum0 = rv[i  ] - totalContribution0;
      511             
      512   p         					sum3 += xv[i+3] * currentDiagonal3;
      513   p         					sum2 += xv[i+2] * currentDiagonal2;
      514   p         					sum1 += xv[i+1] * currentDiagonal1;
      515   p         					sum0 += xv[i  ] * currentDiagonal0;
      516             					
      517   p         					xv[i+3] = sum3 / currentDiagonal3;
      518   p         					xv[i+2] = sum2 / currentDiagonal2;
      519   p         					xv[i+1] = sum1 / currentDiagonal1;
      520   p         					xv[i  ] = sum0 / currentDiagonal0;
      521   p         				} else if ( A.chunkSize == 2 ) {
      522   p         					const double * const currentValues1 = A.matrixValues[i+1];
      523   p         					const double * const currentValues0 = A.matrixValues[i  ];
      524             
      525   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      526   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      527             
      528   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      529   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      530             
      531   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      532   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      533             
      534   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      535   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      536             
      537   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      538   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      539             
      540   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      541   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      542             
      543   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      544   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      545             
      546   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1 );
      547   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      548   p      s  					}
      549             
      550   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      551   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      552             
      553   p         					double sum1 = rv[i+1] - totalContribution1;
      554   p         					double sum0 = rv[i  ] - totalContribution0;
      555             
      556   p         					sum1 += xv[i+1] * currentDiagonal1;
      557   p         					sum0 += xv[i  ] * currentDiagonal0;
      558             					
      559   p         					xv[i+1] = sum1 / currentDiagonal1;
      560   p         					xv[i  ] = sum0 / currentDiagonal0;
      561   p         				} else { // A.chunkSize == 1
      562   p         					const double * const currentValues0 = A.matrixValues[i  ];
      563             
      564   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      565             
      566   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      567             
      568   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      569             
      570   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      571   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      572             
      573   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      574             
      575   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      576             
      577   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      578             
      579   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      580   p      s  					}
      581             
      582   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      583             
      584   p         					double sum0 = rv[i  ] - totalContribution0;
      585             
      586   p         					sum0 += xv[i  ] * currentDiagonal0;
      587             					
      588   p         				}
      589   p         			}
      590   p         		}
      591             	}
      592             LIKWID_STOP(trace.enabled, "symgs_bc");			
      593             
      594             	return 0;
      595             }
      596             /*
      597              * END OF BLOCK COLORED VERSION
      598              */
      599             #elif defined(HPCG_USE_NEON)
      600             
      601             /**************************************************************************************************/
      602             /**************************************************************************************************/
      603             /**************************************************************************************************/
      604             /* NEON IMPLEMENTATIONS                                                                           */
      605             /**************************************************************************************************/
      606             /**************************************************************************************************/
      607             /**************************************************************************************************/
      608             
      609             #include "arm_neon.h"
      610             
      611             /*
      612              * TDG VERSION
      613              */
      614             int ComputeSYMGS_TDG_NEON(const SparseMatrix & A, const Vector & r, Vector & x) {
      615             	assert(x.localLength == A.localNumberOfColumns);
      616             
      617             #ifndef HPCG_NO_MPI
      618             	ExchangeHalo(A, x);
      619             #endif
      620             
      621             	const double * const rv = r.values;
      622             	double * const xv = x.values;
      623             	double **matrixDiagonal = A.matrixDiagonal;
      624             
      625             	/*
      626             	 * FORWARD
      627             	 */
      628             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      629             #ifndef HPCG_NO_OPENMP
      630             #pragma omp parallel for SCHEDULE(runtime)
      631             #endif
      632             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      633             			local_int_t row = A.tdg[l][i];
      634             			const double * const currentValues = A.matrixValues[row];
      635             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      636             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      637             			const double currentDiagonal = matrixDiagonal[row][0];
      638             			float64x2_t contribs = vdupq_n_f64(0.0);
      639             
      640             			local_int_t j = 0;
      641             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      642             				// Load the needed j values
      643             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      644             				// Load the needed x values
      645             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      646             				float64x2_t xvv = vld1q_f64(aux);
      647             				// Add the contribution
      648             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      649             			}
      650             			// reduce contributions
      651             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      652             			double sum = rv[row] - totalContribution;
      653             			// Add missing values from last loop
      654             			if ( j < currentNumberOfNonzeros ) {
      655             				sum -= currentValues[j] * xv[currentColIndices[j]];
      656             			}
      657             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      658             			xv[row] = sum / currentDiagonal; // update row
      659             		}
      660             	}
      661             
      662             	/*
      663             	 * BACKWARD
      664             	 */
      665             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      666             #ifndef HPCG_NO_OPENMP
      667             #pragma omp parallel for SCHEDULE(runtime)
      668             #endif
      669             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      670             			local_int_t row = A.tdg[l][i];
      671             			const double * const currentValues = A.matrixValues[row];
      672             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      673             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      674             			const double currentDiagonal = matrixDiagonal[row][0];
      675             			float64x2_t contribs = vdupq_n_f64(0.0);
      676             
      677             			local_int_t j = 0;
      678             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      679             				// Load the needed j values
      680             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      681             				// Load the needed x values
      682             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      683             				float64x2_t xvv = vld1q_f64(aux);
      684             				// Add the contribution
      685             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      686             			}
      687             			// reduce contributions
      688             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      689             			double sum = rv[row] - totalContribution;
      690             			// Add missing values from last loop
      691             			if ( j < currentNumberOfNonzeros ) {
      692             				sum -= currentValues[j] * xv[currentColIndices[j]];
      693             			}
      694             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      695             			xv[row] = sum / currentDiagonal; // update row
      696             		}
      697             	}
      698             
      699             	return 0;
      700             }
      701             /*
      702              *
      703              */
      704             ////////////////////////////////////////////////////////////////////////////////
      705             ////////////////////////////////////////////////////////////////////////////////
      706             ////////////////////////////////////////////////////////////////////////////////
      707             /*
      708              * TDG FUSED VERSION
      709              */
      710             int ComputeFusedSYMGS_SPMV_NEON(const SparseMatrix & A, const Vector & r, Vector & x, Vector & y) {
      711             	assert(x.localLength == A.localNumberOfColumns);
      712             
      713             #ifndef HPCG_NO_MPI
      714             	ExchangeHalo(A, x);
      715             #endif
      716             
      717             	const double * const rv = r.values;
      718             	double * const xv = x.values;
      719             	double * const yv = y.values;
      720             	double **matrixDiagonal = A.matrixDiagonal;
      721             
      722             	/*
      723             	 * FORWARD
      724             	 */
      725             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      726             #ifndef HPCG_NO_OPENMP
      727             #pragma omp parallel for SCHEDULE(runtime)
      728             #endif
      729             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      730             			local_int_t row = A.tdg[l][i];
      731             			const double * const currentValues = A.matrixValues[row];
      732             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      733             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      734             			const double currentDiagonal = matrixDiagonal[row][0];
      735             			float64x2_t contribs = vdupq_n_f64(0.0);
      736             
      737             			local_int_t j = 0;
      738             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      739             				// Load the needed j values
      740             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      741             				// Load the needed x values
      742             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      743             				float64x2_t xvv = vld1q_f64(aux);
      744             				// Add the contribution
      745             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      746             			}
      747             			// reduce contributions
      748             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      749             			double sum = rv[row] - totalContribution;
      750             			// Add missing values from last loop
      751             			if ( j < currentNumberOfNonzeros ) {
      752             				sum -= currentValues[j] * xv[currentColIndices[j]];
      753             			}
      754             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      755             			xv[row] = sum / currentDiagonal; // update row
      756             		}
      757             	}
      758             
      759             	/*
      760             	 * BACKWARD (fusing SYMGS and SPMV)
      761             	 */
      762             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      763             #ifndef HPCG_NO_OPENMP
      764             #pragma omp parallel for SCHEDULE(runtime)
      765             #endif
      766             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      767             			local_int_t row = A.tdg[l][i];
      768             			const double * const currentValues = A.matrixValues[row];
      769             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      770             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      771             			const double currentDiagonal = matrixDiagonal[row][0];
      772             			float64x2_t contribs = vdupq_n_f64(0.0);
      773             
      774             			local_int_t j = 0;
      775             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      776             				// Load the needed j values
      777             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      778             				// Load the needed x values
      779             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      780             				float64x2_t xvv = vld1q_f64(aux);
      781             				// Add the contribution
      782             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      783             			}
      784             			// reduce contributions
      785             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      786             			// Add missing values from last loop
      787             			if ( j < currentNumberOfNonzeros ) {
      788             				totalContribution += currentValues[j] * xv[currentColIndices[j]];
      789             			}
      790             			totalContribution -= xv[row] * currentDiagonal; // remove diagonal contribution
      791             			double sum = rv[row] - totalContribution; // substract contributions from RHS
      792             			xv[row] = sum / currentDiagonal; // update row
      793             			// Fusion part
      794             			totalContribution += xv[row] * currentDiagonal; // add updated diagonal contribution
      795             			yv[row] = totalContribution; // update SPMV output vector
      796             		}
      797             	}
      798             
      799             	return 0;
      800             }
      801             /*
      802              *
      803              */
      804             ////////////////////////////////////////////////////////////////////////////////
      805             ////////////////////////////////////////////////////////////////////////////////
      806             ////////////////////////////////////////////////////////////////////////////////
      807             /*
      808              * BLOCK COLORED VERSION
      809              */
      810             int ComputeSYMGS_BLOCK_NEON(const SparseMatrix & A, const Vector & r, Vector & x) {
      811             
      812             	assert(x.localLength >= A.localNumberOfColumns);
      813             	
      814             #ifndef HPCG_NO_MPI
      815             	ExchangeHalo(A, x);
      816             #endif
      817             
      818             	double **matrixDiagonal = A.matrixDiagonal;
      819             	const double * const rv = r.values;
      820             	double * const xv = x.values;
      821             
      822             	local_int_t firstBlock = 0;
      823             	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
      824             	/*
      825             	 * FORWARD
      826             	 */
      827             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) { // for each color
      828             		if ( color > 0 ) {
      829             			firstBlock += A.numberOfBlocksInColor[color-1];
      830             			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
      831             		}
      832             #ifndef HPCG_NO_OPENMP
      833             #pragma omp parallel for SCHEDULE(runtime)
      834             #endif
      835             		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) { // for each super block with the same color
      836             			local_int_t firstRow = block * A.blockSize;
      837             			local_int_t firstChunk = firstRow / A.chunkSize;
      838             			local_int_t lastChunk = (firstRow + A.blockSize*A.chunkSize) / A.chunkSize;
      839             
      840             			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) { // for each chunk of this super block
      841             				local_int_t first = A.chunkSize * chunk;
      842             				local_int_t last = first + A.chunkSize;
      843             
      844             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      845             				local_int_t i = first;
      846             				if ( A.chunkSize == 4 ) {
      847             					const double * const currentValues0 = A.matrixValues[i  ];
      848             					const double * const currentValues1 = A.matrixValues[i+1];
      849             					const double * const currentValues2 = A.matrixValues[i+2];
      850             					const double * const currentValues3 = A.matrixValues[i+3];
      851             
      852             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      853             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      854             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      855             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      856             
      857             					const double currentDiagonal[4] = { matrixDiagonal[i  ][0],\
      858             														matrixDiagonal[i+1][0],\
      859             														matrixDiagonal[i+2][0],\
      860             														matrixDiagonal[i+3][0]};
      861             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
      862             					float64x2_t diagonal23 = vld1q_f64(&currentDiagonal[2]);
      863             
      864             					float64x2_t contribs0 = vdupq_n_f64(0.0);
      865             					float64x2_t contribs1 = vdupq_n_f64(0.0);
      866             					float64x2_t contribs2 = vdupq_n_f64(0.0);
      867             					float64x2_t contribs3 = vdupq_n_f64(0.0);
      868             
      869             					float64x2_t vrv01 = vld1q_f64(&rv[i]);
      870             					float64x2_t vrv23 = vld1q_f64(&rv[i+2]);
      871             
      872             					float64x2_t vxv01 = vld1q_f64(&xv[i]);
      873             					float64x2_t vxv23 = vld1q_f64(&xv[i+2]);
      874             
      875             					local_int_t j = 0;
      876             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
      877             						// Load values
      878             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
      879             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
      880             						float64x2_t values2 = vld1q_f64(&currentValues2[j]);
      881             						float64x2_t values3 = vld1q_f64(&currentValues3[j]);
      882             
      883             						// Load x
      884             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
      885             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
      886             						float64x2_t vxv2 = { xv[currentColIndices2[j]], xv[currentColIndices2[j+1]] };
      887             						float64x2_t vxv3 = { xv[currentColIndices3[j]], xv[currentColIndices3[j+1]] };
      888             
      889             						// Add contribution
      890             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
      891             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
      892             						contribs2 = vfmaq_f64(contribs2, values2, vxv2);
      893             						contribs3 = vfmaq_f64(contribs3, values3, vxv3);
      894             					}
      895             					// Reduce contribution
      896             					// First for i and i+1
      897             					float64x2_t totalContribution01;
      898             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
      899             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
      900             
      901             					// Then for i+2 and i+3
      902             					float64x2_t totalContribution23;
      903             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs2), totalContribution23, 0);
      904             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs3), totalContribution23, 1);
      905             
      906             					// Substract contributions from RHS
      907             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
      908             					float64x2_t sum23 = vsubq_f64(vrv23, totalContribution23);
      909             
      910             					// Add contributions from missing elements (if any)
      911             					if ( j < currentNumberOfNonzeros ) {
      912             						// Load current values
      913             						float64x2_t values01 = { currentValues0[j], currentValues1[j] };
      914             						float64x2_t values23 = { currentValues2[j], currentValues3[j] };
      915             
      916             						// Load x
      917             						float64x2_t vx01 = { xv[currentColIndices0[j]], xv[currentColIndices1[j]] };
      918             						float64x2_t vx23 = { xv[currentColIndices2[j]], xv[currentColIndices3[j]] };
      919             
      920             						// Add contributions
      921             						sum01 = vfmsq_f64(sum01, values01, vx01);
      922             						sum23 = vfmsq_f64(sum23, values23, vx23);
      923             					}
      924             
      925             					// Remove diagonal contribution and update rows i and i+1
      926             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
      927             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
      928             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
      929             
      930             					// Remove diagonal contribution and update rows i+2 and i+3
      931             					sum23 = vfmaq_f64(sum23, vxv23, diagonal23);
      932             					xv[i+2] = vgetq_lane_f64(sum23, 0) / currentDiagonal[2];
      933             					xv[i+3] = vgetq_lane_f64(sum23, 1) / currentDiagonal[3];
      934             				} else if ( A.chunkSize == 2 ) {
      935             					const double * const currentValues0 = A.matrixValues[i  ];
      936             					const double * const currentValues1 = A.matrixValues[i+1];
      937             
      938             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      939             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      940             
      941             					const double currentDiagonal[2] = { matrixDiagonal[i  ][0],\
      942             														matrixDiagonal[i+1][0]};
      943             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
      944             
      945             					float64x2_t contribs0 = vdupq_n_f64(0.0);
      946             					float64x2_t contribs1 = vdupq_n_f64(0.0);
      947             
      948             					float64x2_t vrv01 = vld1q_f64(&rv[i]);
      949             
      950             					float64x2_t vxv01 = vld1q_f64(&xv[i]);
      951             
      952             					local_int_t j = 0;
      953             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
      954             						// Load values
      955             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
      956             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
      957             
      958             						// Load x
      959             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
      960             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
      961             
      962             						// Add contribution
      963             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
      964             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
      965             					}
      966             					// Reduce contribution
      967             					// First for i and i+1
      968             					float64x2_t totalContribution01;
      969             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
      970             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
      971             
      972             					// Substract contributions from RHS
      973             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
      974             
      975             					// Add contributions from missing elements (if any)
      976             					if ( j < currentNumberOfNonzeros ) {
      977             						// Load current values
      978             						float64x2_t values01 = { currentValues0[j], currentValues1[j] };
      979             
      980             						// Load x
      981             						float64x2_t vx01 = { xv[currentColIndices0[j]], xv[currentColIndices1[j]] };
      982             
      983             						// Add contributions
      984             						sum01 = vfmsq_f64(sum01, values01, vx01);
      985             					}
      986             
      987             					// Remove diagonal contribution and update rows i and i+1
      988             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
      989             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
      990             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
      991             				} else { // A.chunkSize == 1
      992             					const double * const currentValues = A.matrixValues[i];
      993             					const local_int_t * const currentColIndices = A.mtxIndL[i];
      994             					const double currentDiagonal = matrixDiagonal[i][0];
      995             					float64x2_t contribs = vdupq_n_f64(0.0);
      996             
      997             					local_int_t j = 0;
      998             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
      999             						// Load values
     1000             						float64x2_t values = vld1q_f64(&currentValues[j]);
     1001             
     1002             						// Load x
     1003             						float64x2_t vxv = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
     1004             
     1005             						// Add contribution
     1006             						contribs = vfmaq_f64(contribs, values, vxv);
     1007             					}
     1008             					// Reduce contribution
     1009             					// First for i and i+1
     1010             					double totalContribution;
     1011             					totalContribution = vaddvq_f64(contribs);
     1012             
     1013             					// Substract contributions from RHS
     1014             					double sum = rv[i] - totalContribution;
     1015             
     1016             					// Add contributions from missing elements (if any)
     1017             					if ( j < currentNumberOfNonzeros ) {
     1018             						sum -= currentValues[j] * xv[currentColIndices[j]];
     1019             					}
     1020             
     1021             					// Remove diagonal contribution and update rows i and i+1
     1022             					sum += xv[i] * currentDiagonal;
     1023             					xv[i] = sum / currentDiagonal;
     1024             				}
     1025             			}
     1026             		}
     1027             	}
     1028             
     1029             	firstBlock = A.numberOfBlocks-1;
     1030             	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
     1031             	/*
     1032             	 * BACKWARD
     1033             	 */
     1034             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
     1035             		if ( color < A.numberOfColors-1 ) {
     1036             			firstBlock -= A.numberOfBlocksInColor[color+1];
     1037             			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
     1038             		}
     1039             #ifndef HPCG_NO_OPENMP
     1040             #pragma omp parallel for SCHEDULE(runtime)
     1041             #endif
     1042             		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) { // we skip a whole superblock on each iteration
     1043             			local_int_t firstRow = ((block+1) * A.blockSize) - 1; // this is the last row of the last block (i.e., next block first row - 1)
     1044             			local_int_t firstChunk = firstRow / A.chunkSize; // this is the  chunk of the row above
     1045             			local_int_t lastChunk = (firstRow - A.blockSize*A.chunkSize) / A.chunkSize; 
     1046             
     1047             			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
     1048             				local_int_t first = A.chunkSize * chunk;
     1049             				local_int_t last = first + A.chunkSize;
     1050             
     1051             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
     1052             				if ( A.chunkSize == 4 ) {
     1053             					local_int_t i = last-1-3;
     1054             
     1055             					const double * const currentValues3 = A.matrixValues[i+3];
     1056             					const double * const currentValues2 = A.matrixValues[i+2];
     1057             					const double * const currentValues1 = A.matrixValues[i+1];
     1058             					const double * const currentValues0 = A.matrixValues[i  ];
     1059             
     1060             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
     1061             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
     1062             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1063             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1064             
     1065             					const double currentDiagonal[4] = {\
     1066             							matrixDiagonal[i  ][0],\
     1067             							matrixDiagonal[i+1][0],\
     1068             							matrixDiagonal[i+2][0],\
     1069             							matrixDiagonal[i+3][0]};
     1070             
     1071             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1072             					float64x2_t diagonal23 = vld1q_f64(&currentDiagonal[2]);
     1073             
     1074             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1075             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1076             					float64x2_t contribs2 = vdupq_n_f64(0.0);
     1077             					float64x2_t contribs3 = vdupq_n_f64(0.0);
     1078             
     1079             					float64x2_t vrv23 = vld1q_f64(&rv[i+2]);
     1080             					float64x2_t vrv01 = vld1q_f64(&rv[i  ]);
     1081             
     1082             					float64x2_t vxv23 = vld1q_f64(&xv[i+2]);
     1083             					float64x2_t vxv01 = vld1q_f64(&xv[i  ]);
     1084             
     1085             					local_int_t j = 0;
     1086             					for ( j = currentNumberOfNonzeros-2; j >= 0; j -= 2 ) {
     1087             						// Load values
     1088             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1089             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1090             						float64x2_t values2 = vld1q_f64(&currentValues2[j]);
     1091             						float64x2_t values3 = vld1q_f64(&currentValues3[j]);
     1092             
     1093             						// Load x
     1094             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1095             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1096             						float64x2_t vxv2 = { xv[currentColIndices2[j]], xv[currentColIndices2[j+1]] };
     1097             						float64x2_t vxv3 = { xv[currentColIndices3[j]], xv[currentColIndices3[j+1]] };
     1098             
     1099             						// Add contribution
     1100             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1101             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1102             						contribs2 = vfmaq_f64(contribs2, values2, vxv2);
     1103             						contribs3 = vfmaq_f64(contribs3, values3, vxv3);
     1104             					}
     1105             					// Reduce contribution
     1106             					// First for i and i-1
     1107             					float64x2_t totalContribution01;
     1108             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1109             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1110             
     1111             					// Then for i-2 and i-3
     1112             					float64x2_t totalContribution23;
     1113             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs2), totalContribution23, 0);
     1114             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs3), totalContribution23, 1);
     1115             
     1116             					// Substract contributions from RHS
     1117             					float64x2_t sum23 = vsubq_f64(vrv23, totalContribution23);
     1118             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1119             
     1120             					// Add contributions from missing elements (if any)
     1121             					if ( j == -1 ) {
     1122             						// Load current values
     1123             						float64x2_t values23 = { currentValues2[j+1], currentValues3[j+1] };
     1124             						float64x2_t values01 = { currentValues0[j+1], currentValues1[j+1] };
     1125             
     1126             						// Load x
     1127             						float64x2_t vx23 = { xv[currentColIndices2[j+1]], xv[currentColIndices3[j+1]] };
     1128             						float64x2_t vx01 = { xv[currentColIndices0[j+1]], xv[currentColIndices1[j+1]] };
     1129             
     1130             						// Add contributions
     1131             						sum23 = vfmsq_f64(sum23, values23, vx23);
     1132             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1133             					}
     1134             
     1135             					// Remove diagonal contribution and update rows i-2 and i-3
     1136             					sum23 = vfmaq_f64(sum23, vxv23, diagonal23);
     1137             					xv[i+3] = vgetq_lane_f64(sum23, 1) / currentDiagonal[3];
     1138             					xv[i+2] = vgetq_lane_f64(sum23, 0) / currentDiagonal[2];
     1139             
     1140             					// Remove diagonal contribution and update rows i and i-1
     1141             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1142             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1143             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1144             				} else if ( A.chunkSize == 2 ) {
     1145             					local_int_t i = last-1-1;
     1146             
     1147             					const double * const currentValues1 = A.matrixValues[i+1];
     1148             					const double * const currentValues0 = A.matrixValues[i  ];
     1149             
     1150             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1151             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1152             
     1153             					const double currentDiagonal[2] = {\
     1154             							matrixDiagonal[i  ][0],\
     1155             							matrixDiagonal[i+1][0]};
     1156             
     1157             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1158             
     1159             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1160             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1161             
     1162             					float64x2_t vrv01 = vld1q_f64(&rv[i  ]);
     1163             
     1164             					float64x2_t vxv01 = vld1q_f64(&xv[i  ]);
     1165             
     1166             					local_int_t j = 0;
     1167             					for ( j = currentNumberOfNonzeros-2; j >= 0; j -= 2 ) {
     1168             						// Load values
     1169             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1170             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1171             
     1172             						// Load x
     1173             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1174             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1175             
     1176             						// Add contribution
     1177             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1178             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1179             					}
     1180             					// Reduce contribution
     1181             					// First for i and i-1
     1182             					float64x2_t totalContribution01;
     1183             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1184             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1185             
     1186             					// Substract contributions from RHS
     1187             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1188             
     1189             					// Add contributions from missing elements (if any)
     1190             					if ( j == -1 ) {
     1191             						// Load current values
     1192             						float64x2_t values01 = { currentValues0[j+1], currentValues1[j+1] };
     1193             
     1194             						// Load x
     1195             						float64x2_t vx01 = { xv[currentColIndices0[j+1]], xv[currentColIndices1[j+1]] };
     1196             
     1197             						// Add contributions
     1198             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1199             					}
     1200             
     1201             					// Remove diagonal contribution and update rows i and i-1
     1202             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1203             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1204             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1205             				} else { // A.chunkSize == 1
     1206             					local_int_t i = last - 1; // == first
     1207             					const double * const currentValues = A.matrixValues[i];
     1208             					const local_int_t * const currentColIndices = A.mtxIndL[i];
     1209             					const double currentDiagonal = matrixDiagonal[i][0];
     1210             
     1211             					float64x2_t contribs = vdupq_n_f64(0.0);
     1212             
     1213             					local_int_t j = 0;
     1214             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1215             						// Load values
     1216             						float64x2_t values = vld1q_f64(&currentValues[j]);
     1217             
     1218             						// Load x
     1219             						float64x2_t vxv = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
     1220             
     1221             						// Add contribution
     1222             						contribs = vfmaq_f64(contribs, values, vxv);
     1223             					}
     1224             					// Reduce contribution
     1225             					double totalContribution = vaddvq_f64(contribs);
     1226             
     1227             					// Substract contribution from RHS
     1228             					double sum = rv[i] - totalContribution;
     1229             
     1230             					// Add contributions from missing elements (if any)
     1231             					if ( j < currentNumberOfNonzeros ) {
     1232             						sum -= currentValues[j] * xv[currentColIndices[j]];
     1233             					}
     1234             
     1235             					// Remove diagonal contribution and updated row i
     1236             					sum += xv[i] * currentDiagonal;
     1237             					xv[i] = sum / currentDiagonal;
     1238             				}
     1239             			}
     1240             		}
     1241             	}
     1242             
     1243             	return 0;
     1244             }
     1245             /*
     1246              *
     1247              */
     1248             #endif
     1249             //#else // !HPCG_USE_SVE ! HPCG_USE_NEON
     1250             
     1251             int ComputeFusedSYMGS_SPMV ( const SparseMatrix & A, const Vector & r, Vector & x, Vector & y ) {
     1252             	assert(x.localLength == A.localNumberOfColumns);
     1253             
     1254             #ifndef HPCG_NO_MPI
     1255             	ExchangeHalo(A, x);
     1256             #endif
     1257             
     1258             	const double * const rv = r.values;
     1259             	double * const xv = x.values;
     1260             	double * const yv = y.values;
     1261             	double **matrixDiagonal = A.matrixDiagonal;
     1262             
     1263             	/*
     1264             	 * FORWARD
     1265             	 */
     1266    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1267             #ifndef HPCG_NO_OPENMP
     1268             #pragma omp parallel for SCHEDULE(runtime)
     1269             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1270   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
     1271   p         			local_int_t row = A.tdg[l][i];
     1272   p         			const double * const currentValues = A.matrixValues[row];
     1273   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1274   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1275   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1276   p         			double sum = rv[row];
     1277             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1278   p     8v  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j++ ) {
     1279   p     8v  				local_int_t curCol = currentColIndices[j];
     1280   p     8v  				sum -= currentValues[j] * xv[curCol];
     1281   p     8v  			}
     1282   p         			sum += xv[row] * currentDiagonal;
     1283   p         			xv[row] = sum / currentDiagonal;
     1284   pi        		}
     1285    i        	}
     1286             
     1287             	/*
     1288             	 * BACKWARD (fusing SYMGS and SPMV)
     1289             	 */
     1290    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1291             #ifndef HPCG_NO_OPENMP
     1292             #pragma omp parallel for SCHEDULE(runtime)
     1293             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1294   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
     1295   p         			local_int_t row = A.tdg[l][i];
     1296   p         			const double * const currentValues = A.matrixValues[row];
     1297   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1298   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1299   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1300   p         			double sum = 0.0;
     1301             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1302   p     8v  			for ( local_int_t j = currentNumberOfNonzeros-1; j >= 0; j-- ) {
     1303   p     8v  				local_int_t curCol = currentColIndices[j];
     1304   p     8v  				sum += currentValues[j] * xv[curCol];
     1305   p     8v  			}
     1306   p         			sum -= xv[row] * currentDiagonal;
     1307   p         			xv[row] = (rv[row] - sum) / currentDiagonal;
     1308   p         			sum += xv[row] * currentDiagonal;
     1309   p         			yv[row] = sum;
     1310   p         		}
     1311             	}
     1312             
     1313             	return 0;
     1314             }
     1315             
     1316             int ComputeSYMGS_TDG ( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
     1317             
     1318             	assert( x.localLength == A.localNumberOfColumns);
     1319             
     1320             #ifndef HPCG_NO_MPI
     1321             	ExchangeHalo(A,x);
     1322             #endif
     1323             
     1324             	const double * const rv = r.values;
     1325             	double * const xv = x.values;
     1326             	double **matrixDiagonal = A.matrixDiagonal;
     1327             
     1328             /*#ifndef HPCG_NO_OPENMP
     1329             #pragma omp parallel SCHEDULE(runtime)
     1330             {
     1331             #endif
     1332             */
     1333             #pragma statement scache_isolate_way L2=10
     1334             #pragma statement scache_isolate_assign xv
     1335             
     1336             	/*
     1337             	 * FORWARD
     1338             	 */
     1339    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1340             #ifndef HPCG_NO_OPENMP
     1341             #pragma omp parallel for SCHEDULE(runtime)
     1342             #endif
     1343             		#pragma loop unroll_and_jam(4)
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1344   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
     1345   p         			local_int_t row = A.tdg[l][i];
     1346   p         			const double * const currentValues = A.matrixValues[row];
     1347   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1348   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1349   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1350   p         			double sum = rv[row];
     1351             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1352   p     8v  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j++ ) {
     1353   p     8v  				local_int_t curCol = currentColIndices[j];
     1354   p     8v  				sum -= currentValues[j] * xv[curCol];
     1355   p     8v  			}
     1356   p         			sum += xv[row] * currentDiagonal;
     1357   p         			xv[row] = sum / currentDiagonal;
     1358   pi        		}
     1359    i        	}
     1360             
     1361             	/*
     1362             	 * BACKWARD
     1363             	 */
     1364    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1365             #ifndef HPCG_NO_OPENMP
     1366             #pragma omp parallel for SCHEDULE(runtime)
     1367             #endif
     1368             		#pragma loop unroll_and_jam(4)
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1369   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
     1370   p         			local_int_t row = A.tdg[l][i];
     1371   p         			const double * const currentValues = A.matrixValues[row];
     1372   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1373   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1374   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1375   p         			double sum = rv[row];
     1376             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1377   p     8v  			for ( local_int_t j = currentNumberOfNonzeros-1; j >= 0; j-- ) {
     1378   p     8v  				local_int_t curCol = currentColIndices[j];
     1379   p     8v  				sum -= currentValues[j] * xv[curCol];
     1380   p     8v  			}
     1381   p         			sum += xv[row] * currentDiagonal;
     1382   p         			xv[row] = sum / currentDiagonal;
     1383   p         		}
     1384             	}
     1385             
     1386             	#pragma statement end_scache_isolate_assign
     1387             	#pragma statement end_scache_isolate_way
     1388             /*#ifndef HPCG_NO_OPENMP
     1389             }
     1390             #endif*/
     1391             
     1392             	return 0;
     1393             }
     1394             
     1395             int ComputeSYMGS_BLOCK( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
     1396             
     1397             	assert(x.localLength >= A.localNumberOfColumns);
     1398             	
     1399             #ifndef HPCG_NO_MPI
     1400             	ExchangeHalo(A, x);
     1401             #endif
     1402             
     1403             	const local_int_t nrow = A.localNumberOfRows;
     1404             	double **matrixDiagonal = A.matrixDiagonal;
     1405             	const double * const rv = r.values;
     1406             	double * const xv = x.values;
     1407             
     1408             	local_int_t firstBlock = 0;
     1409    i        	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
     1410             	/*
     1411             	 * FORWARD
     1412             	 */
     1413             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) {
     1414             		if ( color > 0 ) {
     1415    i        			firstBlock += A.numberOfBlocksInColor[color-1];
     1416    i        			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
     1417             		}
     1418             #ifndef HPCG_NO_OPENMP
     1419             #pragma omp parallel for SCHEDULE(runtime)
     1420             #endif
     1421   p         		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) {
     1422   p         			local_int_t firstRow = block * A.blockSize;
     1423   p         			local_int_t firstChunk = firstRow / A.chunkSize;
     1424   p         			local_int_t lastChunk = (firstRow + A.blockSize*A.chunkSize) / A.chunkSize;
     1425             
     1426   p         			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) {
     1427   p         				local_int_t first = A.chunkSize * chunk;
     1428   p         				local_int_t last = first + A.chunkSize;
     1429             
     1430             				//for ( local_int_t i = first; i < last; i+= (A.chunkSize/2)) {
     1431   p         				local_int_t i = first;
     1432   p         				if ( A.chunkSize == 4 ) {
     1433   p         					double sum0 = rv[i+0];
     1434   p         					double sum1 = rv[i+1];
     1435   p         					double sum2 = rv[i+2];
     1436   p         					double sum3 = rv[i+3];
     1437             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1438   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1439   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1440   p      s  						sum1 -= A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
     1441   p      s  						sum2 -= A.matrixValues[i+2][j] * xv[A.mtxIndL[i+2][j]];
     1442   p      s  						sum3 -= A.matrixValues[i+3][j] * xv[A.mtxIndL[i+3][j]];
     1443   pi     s  					}
     1444   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1445   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1446   p         					sum1 += matrixDiagonal[i+1][0] * xv[i+1];
     1447   p         					xv[i+1] = sum1 / matrixDiagonal[i+1][0];
     1448   p         					sum2 += matrixDiagonal[i+2][0] * xv[i+2];
     1449   p         					xv[i+2] = sum2 / matrixDiagonal[i+2][0];
     1450   p         					sum3 += matrixDiagonal[i+3][0] * xv[i+3];
     1451   p         					xv[i+3] = sum3 / matrixDiagonal[i+3][0];
     1452   p         				} else if ( A.chunkSize == 2 ) {
     1453   p         					double sum0 = rv[i+0];
     1454   p         					double sum1 = rv[i+1];
     1455             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1456   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1457   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1458   p      s  						sum1 -= A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
     1459   pi     s  					}
     1460   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1461   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1462   p         					sum1 += matrixDiagonal[i+1][0] * xv[i+1];
     1463   p         					xv[i+1] = sum1 / matrixDiagonal[i+1][0];
     1464   p         				} else { // A.chunkSize == 1
     1465   p         					double sum0 = rv[i+0];
     1466             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1467   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1468   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1469   pi     s  					}
     1470   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1471   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1472   p         				}
     1473   p         			}
     1474   p         		}
     1475             	}
     1476             
     1477             	firstBlock = A.numberOfBlocks-1;
     1478    i        	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
     1479             	/*
     1480             	 * BACKWARD
     1481             	 */
     1482             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
     1483             		if ( color < A.numberOfColors-1 ) {
     1484    i        			firstBlock -= A.numberOfBlocksInColor[color+1];
     1485    i        			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
     1486             		}
     1487             #ifndef HPCG_NO_OPENMP
     1488             #pragma omp parallel for SCHEDULE(runtime)
     1489             #endif
     1490   p         		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) {
     1491   p         			local_int_t firstRow = ((block+1) * A.blockSize) - 1; // this is the last row of the last block
     1492   p         			local_int_t firstChunk = firstRow / A.chunkSize; // this is the  chunk of the row above
     1493   p         			local_int_t lastChunk = (firstRow - A.blockSize*A.chunkSize) / A.chunkSize; 
     1494             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1495   p         			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
     1496   p         				local_int_t first = A.chunkSize * chunk;
     1497   p         				local_int_t last = first + A.chunkSize;
     1498             
     1499             				//for ( local_int_t i = last-1; i >= first; i -= (A.chunkSize/2)) {
     1500   p         				local_int_t i = last-1;
     1501   p         				if ( A.chunkSize == 4 ) {
     1502   p         					double sum3 = rv[i-3];
     1503   p         					double sum2 = rv[i-2];
     1504   p         					double sum1 = rv[i-1];
     1505   p         					double sum0 = rv[i  ];
     1506             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.28, ITR: 32, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1507   pi     v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1508   p      v  						sum3 -= A.matrixValues[i-3][j] * xv[A.mtxIndL[i-3][j]];
     1509   p      v  						sum2 -= A.matrixValues[i-2][j] * xv[A.mtxIndL[i-2][j]];
     1510   p      v  						sum1 -= A.matrixValues[i-1][j] * xv[A.mtxIndL[i-1][j]];
     1511   p      v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1512   p      v  					}
     1513   p         					sum3 += matrixDiagonal[i-3][0] * xv[i-3];
     1514   p         					xv[i-3] = sum3 / matrixDiagonal[i-3][0];
     1515             
     1516   p         					sum2 += matrixDiagonal[i-2][0] * xv[i-2];
     1517   p         					xv[i-2] = sum2 / matrixDiagonal[i-2][0];
     1518             
     1519   p         					sum1 += matrixDiagonal[i-1][0] * xv[i-1];
     1520   p         					xv[i-1] = sum1 / matrixDiagonal[i-1][0];
     1521             
     1522   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1523   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1524   p         				} else if ( A.chunkSize == 2 ) {
     1525   p         					double sum1 = rv[i-1];
     1526   p         					double sum0 = rv[i  ];
     1527             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 96, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1528   pi    4v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1529   p     4v  						sum1 -= A.matrixValues[i-1][j] * xv[A.mtxIndL[i-1][j]];
     1530   p     4v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1531   p     4v  					}
     1532             
     1533   p         					sum1 += matrixDiagonal[i-1][0] * xv[i-1];
     1534   p         					xv[i-1] = sum1 / matrixDiagonal[i-1][0];
     1535             
     1536   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1537   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1538   p         				} else { // A.chunkSize == 1
     1539   p         					double sum0 = rv[i  ];
     1540             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1541   pi    8v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1542   p     8v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1543   p     8v  					}
     1544             
     1545   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1546   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1547   p         				}
     1548   p         			}
     1549   p         		}
     1550             	}
     1551             
     1552             	return 0;
     1553             }
     1554             //#endif
     1555             
     1556             
     1557             
     1558             /*!
     1559               Routine to compute one step of symmetric Gauss-Seidel:
     1560             
     1561               Assumption about the structure of matrix A:
     1562               - Each row 'i' of the matrix has nonzero diagonal value whose address is matrixDiagonal[i]
     1563               - Entries in row 'i' are ordered such that:
     1564                    - lower triangular terms are stored before the diagonal element.
     1565                    - upper triangular terms are stored after the diagonal element.
     1566                    - No other assumptions are made about entry ordering.
     1567             
     1568               Symmetric Gauss-Seidel notes:
     1569               - We use the input vector x as the RHS and start with an initial guess for y of all zeros.
     1570               - We perform one forward sweep.  Since y is initially zero we can ignore the upper triangular terms of A.
     1571               - We then perform one back sweep.
     1572                    - For simplicity we include the diagonal contribution in the for-j loop, then correct the sum after
     1573             
     1574               @param[in] A the known system matrix
     1575               @param[in] r the input vector
     1576               @param[inout] x On entry, x should contain relevant values, on exit x contains the result of one symmetric GS sweep with r as the RHS.
     1577             
     1578               @return returns 0 upon success and non-zero otherwise
     1579             
     1580               @warning Early versions of this kernel (Version 1.1 and earlier) had the r and x arguments in reverse order, and out of sync with other kernels.
     1581             
     1582               @see ComputeSYMGS_ref
     1583             */
     1584             int ComputeSYMGS( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace) {
     1585             
     1586             	// This function is just a stub right now which decides which implementation of the SYMGS will be executed (TDG or block coloring)
     1587             	if ( A.TDG ) {
     1588             #ifdef HPCG_USE_NEON
     1589             		return ComputeSYMGS_TDG_NEON(A, r, x);
     1590             #elif defined HPCG_USE_SVE
     1591             		return ComputeSYMGS_TDG_SVE(A, r, x, trace);
     1592             #else
     1593             		return ComputeSYMGS_TDG(A, r, x, trace);
     1594             #endif
     1595             	}
     1596             #ifdef HPCG_USE_NEON
     1597             	return ComputeSYMGS_BLOCK_NEON(A, r, x);
     1598             #elif defined HPCG_USE_SVE
     1599             	return ComputeSYMGS_BLOCK_SVE(A, r, x, trace);
     1600             #else
     1601             	return ComputeSYMGS_BLOCK(A, r, x, trace);
     1602             #endif
     1603             }
Total prefetch num: 0
Optimization messages
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 86: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 92: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 92: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 93: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 93: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 102: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 102: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 117: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 117: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 118: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 123: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 128: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 128: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 129: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 129: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 138: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 138: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 184: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 188: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 188: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 189: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 189: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 196: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 196: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 211: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 211: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 212: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 217: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 221: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 221: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 222: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 222: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 229: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 229: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 271: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 280: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 281: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 294: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 317: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 317: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 373: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 373: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 409: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 409: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 434: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 440: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 441: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 455: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 478: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 478: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 534: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 534: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 570: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 570: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1266: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1270: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1270: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1271: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1271: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1278: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1278: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1278: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1280: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1284: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1284: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1285: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1290: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1294: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1294: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1295: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1295: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1302: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1302: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1302: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1304: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1339: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1344: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1344: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1345: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1345: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1352: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1352: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1352: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1354: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1358: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1358: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1359: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1364: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1369: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1369: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1370: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1370: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1377: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1377: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1377: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1379: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1409: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1415: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1416: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1438: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1438: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1438: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1443: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1456: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1456: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1456: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1459: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1467: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1467: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1467: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1469: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1478: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1484: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1485: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1507: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1507: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1507: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1507: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 32.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1528: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1528: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1528: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1528: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 96.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1529: Method of calculating sum or product is changed.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1530: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1541: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1541: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1541: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1541: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1542: Method of calculating sum or product is changed.
Statistics information
  Option information
    Command line options : -c -DHPCG_CONTIGUOUS_ARRAYS -DHPCG_NO_MPI -DENABLE_MG_COUNTERS -DHPCG_USE_SVE -DHPCG_MAN_OPT_SCHEDULE_ON -DENABLE_MG_COUNTERS -I./src -I./src/OOKAMI_OMP_FJ -I/lustre/software/arm/22.1/armpl-22.1.0_AArch64_RHEL-8_arm-linux-compiler_aarch64-linux/include -Kfast -KSVE -Kopenmp -Koptmsg=2 -Nlst=t -Kocl -I../src -o src/ComputeSYMGS.o
    Effective options    : -g0 -mt -Qy -std=gnu++14 -x- -x=quick -O3 -Knoalias_const
                           -Kalign_loops -Knoarray_declaration_opt -Kassume=noshortloop
                           -Kassume=nomemory_bandwidth -Kassume=notime_saving_compilation
                           -Kcmodel=small -Keval -Keval_noconcurrent
                           -Knoextract_stride_store -Kfast_matmul -Knofenv_access
                           -Kfp_contract -Kfp_relaxed -Kfsimple -Kfz -Khpctag
                           -Kilfunc=procedure -Klargepage -Klib -Kloop_blocking
                           -Kloop_fission -Kloop_nofission_stripmining
                           -Kloop_fission_threshold=50 -Kloop_fusion -Kloop_interchange
                           -Kloop_part_simd -Kloop_perfect_nest -Kloop_noversioning
                           -Klooptype=f -Knomemalias -Kmfunc=1 -Kocl -Komitfp -Kopenmp
                           -Kopenmp_noassume_norecurrence
                           -Kopenmp_nocollapse_except_innermost
                           -Kopenmp_loop_variable=private -Kopenmp_noordered_reduction
                           -Knoopenmp_simd -Knooptlib_string -Koptmsg=2
                           -Knopc_relative_literal_loads -Knoparallel
                           -Kparallel_nofp_precision -Knopreex -Kprefetch_cache_level=all
                           -Kprefetch_noconditional -Kprefetch_noindirect -Kprefetch_noinfer
                           -Kprefetch_sequential=auto -Kprefetch_nostride -Kprefetch_strong
                           -Kprefetch_strong_L2 -Knopreload -Krdconv=1
                           -Kremove_inlinefunction -Knorestp -Ksch_post_ra -Ksch_pre_ra
                           -Ksibling_calls -Ksimd=auto -Ksimd_packed_promotion
                           -Ksimd_reduction_product -Ksimd_reg_size=512
                           -Ksimd_nouncounted_loop -Ksimd_use_multiple_structures
                           -Knostrict_aliasing -Knostriping -KA64FX -KARMV8_3_A -KSVE -Kswp
                           -Kswp_freg_rate=100 -Kswp_ireg_rate=100 -Kswp_preg_rate=100
                           -Kswp_policy=auto -Kunroll -Knounroll_and_jam -Knozfill
                           -Ncancel_overtime_compilation -Nnocoverage -Nexceptions -Nnofjcex
                           -Nfjprof -Nnohook_func -Nnohook_time -Nlibomp -Nline -Nlst=p
                           -Nlst=t -Nquickdbg=noheapchk -Nquickdbg=nosubchk -NRnotrap
                           -Nnoreordered_variable_stack -Nrt_notune -Nsetvalue=noheap
                           -Nsetvalue=nostack -Nsetvalue=noscalar -Nsetvalue=noarray
                           -Nsetvalue=nostruct -Nsrc -Nsta
