Fujitsu C/C++ Version 4.7.0   Fri Jul 14 04:25:57 2023
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj2
  Source file       : ../src/ComputeSYMGS.cpp
(line-no.)(optimize)
        1             /*
        2              *
        3              *  SPDX-License-Identifier: Apache-2.0
        4              *
        5              *  Copyright (C) 2019, Arm Limited and contributors
        6              *
        7              *  Licensed under the Apache License, Version 2.0 (the "License");
        8              *  you may not use this file except in compliance with the License.
        9              *  You may obtain a copy of the License at
       10              *
       11              *      http://www.apache.org/licenses/LICENSE-2.0
       12              *
       13              *  Unless required by applicable law or agreed to in writing, software
       14              *  distributed under the License is distributed on an "AS IS" BASIS,
       15              *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       16              *  See the License for the specific language governing permissions and
       17              *  limitations under the License.
       18              *
       19              */
       20             
       21             //@HEADER
       22             // ***************************************************
       23             //
       24             // HPCG: High Performance Conjugate Gradient Benchmark
       25             //
       26             // Contact:
       27             // Michael A. Heroux ( maherou@sandia.gov)
       28             // Jack Dongarra     (dongarra@eecs.utk.edu)
       29             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       30             //
       31             // ***************************************************
       32             //@HEADER
       33             
       34             /*!
       35              @file ComputeSYMGS.cpp
       36             
       37              HPCG routine
       38              */
       39             
       40             #include "ComputeSYMGS.hpp"
       41             #include "ComputeSYMGS_ref.hpp"
       42             #ifndef HPCG_NO_MPI
       43             #include "ExchangeHalo.hpp"
       44             #endif
       45             
       46             #include "likwid_instrumentation.hpp"
       47             
       48             #ifdef HPCG_MAN_OPT_SCHEDULE_ON
       49             	#define SCHEDULE(T)	schedule(T)
       50             #else
       51             	#define SCHEDULE(T)
       52             #endif
       53             
       54             #define xxxxMANUAL_TASK_DISTRIBUTION
       55             #ifndef HPCG_NO_OPENMP
       56             	#include "omp.h"
       57             #endif
       58             
       59             /**************************************************************************************************/
       60             /**************************************************************************************************/
       61             /**************************************************************************************************/
       62             /* SVE IMPLEMENTATIONS                                                                            */
       63             /**************************************************************************************************/
       64             /**************************************************************************************************/
       65             /**************************************************************************************************/
       66             
       67             #include "arm_sve.h"
       68             #ifdef HPCG_USE_SVE
       69             #include "arm_sve.h"
       70             
       71             inline void SYMGS_VERSION_1(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-2
       72             inline void SYMGS_VERSION_2(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-2 V2
       73             inline void SYMGS_VERSION_3(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-4 - OPTIMUM
       74             inline void SYMGS_VERSION_4(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-6
       75             #include "ComputeSYMGS_OPT.cpp"
       76             #include "ComputeSYMGS_OPT2.cpp" 
       77             
       78             /*
       79              * TDG VERSION
       80              */
       81             int ComputeSYMGS_TDG_SVE(const SparseMatrix & A, const Vector & r, Vector & x, TraceData &trace) {
       82             	assert(x.localLength == A.localNumberOfColumns);
       83             
       84             #ifndef HPCG_NO_MPI
       85             	ExchangeHalo(A, x);
       86             #endif
       87             
       88             	const double * const rv = r.values;
       89             	double * const xv = x.values;
       90             	double **matrixDiagonal = A.matrixDiagonal;
       91             
       92             LIKWID_START(trace.enabled, "symgs_tdg");
       93             
       94             #ifdef UNROLLING_4_A
       95             	SYMGS_VERSION_5(A, xv, rv); 
       96             #elif defined(UNROLLING_4_B)
       97             	#define MANUAL_TASK_DISTRIBUTION
       98             	SYMGS_VERSION_5(A, xv, rv); 
       99             #elif defined(UNROLLING_6_A)
      100             	SYMGS_VERSION_4(A, xv, rv); 
      101             #elif defined(TEST)
      102             	SYMGS_VERSION_4(A, xv, rv); 
      103             #elif defined(REF_UNROLLING_4)
      104             	SYMGS_VERSION_3(A, xv, rv); 
      105             #else
      106             
      107             //#pragma statement scache_isolate_way L2=10
      108             //#pragma statement scache_isolate_assign xv
      109             	/*
      110             	 * FORWARD SWEEP
      111             	 */
      112             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      113             
      114             		local_int_t totalSize = A.tdg[l].size();
      115             		local_int_t size1 = 2*(totalSize/2);
      116             		//#pragma loop nounroll
      117             		//#pragma loop nounroll_and_jam
      118             		//if((A.tdg[l].size()%2) == 0) {
      119             #ifndef HPCG_NO_OPENMP
      120             #pragma omp parallel
      121             {
      122             #pragma omp for nowait SCHEDULE(runtime)
      123             #endif
      124             		for ( local_int_t i = 0; i < size1; i+=2 ) {
      125             			local_int_t row_1 = A.tdg[l][i];
      126             			local_int_t row_2 = A.tdg[l][i+1];
      127             			const double * const currentValues_1 = A.matrixValues[row_1];
      128             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
      129             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
      130             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
      131             			svfloat64_t contribs_1 = svdup_f64(0.0);
      132             
      133             			const double * const currentValues_2 = A.matrixValues[row_2];
      134             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
      135             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
      136             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
      137             			svfloat64_t contribs_2 = svdup_f64(0.0);
      138             			
      139             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
      140             
      141             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
      142             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
      143             				
      144             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
      145             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
      146             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
      147             
      148             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
      149             
      150             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
      151             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
      152             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
      153             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
      154             
      155             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
      156             			}
      157             
      158             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
      159             			double sum_1 = rv[row_1] - totalContribution_1;
      160             
      161             			sum_1 += xv[row_1] * currentDiagonal_1;
      162             			xv[row_1] = sum_1 / currentDiagonal_1;
      163             
      164             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
      165             			double sum_2 = rv[row_2] - totalContribution_2;
      166             
      167             			sum_2 += xv[row_2] * currentDiagonal_2;
      168             			xv[row_2] = sum_2 / currentDiagonal_2;
      169             		}
      170             		//}
      171             		//else
      172             		//{
      173             #ifndef HPCG_NO_OPENMP
      174             //#pragma omp parallel for SCHEDULE(runtime)
      175             #pragma omp single 
      176             {
      177             #endif
      178             		if (size1 < totalSize) {
      179             			local_int_t i = size1;
      180             		//for ( local_int_t i = size1; i < totalSize; i++ ) {
      181             			local_int_t row = A.tdg[l][i];
      182             			const double * const currentValues = A.matrixValues[row];
      183             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      184             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      185             			const double currentDiagonal = matrixDiagonal[row][0];
      186             			svfloat64_t contribs = svdup_f64(0.0);
      187             
      188             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      189             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      190             				
      191             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      192             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      193             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      194             
      195             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      196             			}
      197             
      198             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      199             			double sum = rv[row] - totalContribution;
      200             
      201             			sum += xv[row] * currentDiagonal;
      202             			xv[row] = sum / currentDiagonal;
      203             		//}
      204             		}
      205             #ifndef HPCG_NO_OPENMP
      206             }
      207             }
      208             #endif
      209             	}
      210             
      211             	/*
      212             	 * BACKWARD SWEEP
      213             	 */
      214             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      215             #ifndef HPCG_NO_OPENMP
      216             #pragma omp parallel for SCHEDULE(runtime)
      217             #endif
      218             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      219             			local_int_t row = A.tdg[l][i];
      220             			const double * const currentValues = A.matrixValues[row];
      221             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      222             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      223             			const double currentDiagonal = matrixDiagonal[row][0];
      224             			svfloat64_t contribs = svdup_f64(0.0);
      225             
      226             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      227             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      228             				
      229             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      230             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      231             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      232             
      233             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      234             			}
      235             
      236             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      237             			double sum = rv[row] - totalContribution;
      238             
      239             			sum += xv[row] * currentDiagonal;
      240             			xv[row] = sum / currentDiagonal;
      241             		}
      242             
      243             /*#ifndef HPCG_NO_OPENMP
      244             #pragma omp parallel for SCHEDULE(runtime)
      245             #endif
      246             		for ( local_int_t i = size1-1; i >= 0; i-= 2 ) {
      247             			local_int_t row_1 = A.tdg[l][i];
      248             			local_int_t row_2 = A.tdg[l][i-1];
      249             			const double * const currentValues_1 = A.matrixValues[row_1];
      250             			const double * const currentValues_2 = A.matrixValues[row_2];
      251             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
      252             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
      253             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
      254             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
      255             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
      256             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
      257             			svfloat64_t contribs_1 = svdup_f64(0.0);
      258             			svfloat64_t contribs_2 = svdup_f64(0.0);
      259             
      260             			//#pragma loop nounroll
      261             			//#pragma loop nounroll_and_jam
      262             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      263             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      264             				
      265             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      266             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      267             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      268             
      269             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      270             			}
      271             
      272             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      273             			double sum = rv[row] - totalContribution;
      274             
      275             			sum += xv[row] * currentDiagonal;
      276             			xv[row] = sum / currentDiagonal;
      277             		}*/
      278             	}
      279             //#pragma statement end_scache_isolate_assign
      280             //#pragma statement end_scache_isolate_way
      281             
      282             #endif //TEST_XX
      283             
      284             LIKWID_STOP(trace.enabled, "symgs_tdg");
      285             
      286             	return 0;
      287             }
      288             /*
      289              * END OF TDG VERSION
      290              */
      291             
      292             /*
      293              * TDG FUSED SYMGS-SPMV VERSION
      294              */
      295             int ComputeFusedSYMGS_SPMV_SVE(const SparseMatrix & A, const Vector & r, Vector & x, Vector & y, TraceData& trace) {
      296             	assert(x.localLength == A.localNumberOfColumns);
      297             
      298             #ifndef HPCG_NO_MPI
      299             	ExchangeHalo(A, x);
      300             #endif
      301             
      302             	const double * const rv = r.values;
      303             	double * const xv = x.values;
      304             	double **matrixDiagonal = A.matrixDiagonal;
      305             	double * const yv = y.values;
      306             
      307             	/*
      308             	 * FORWARD SWEEP
      309             	 */
      310    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      311             #ifndef HPCG_NO_OPENMP
      312             #pragma omp parallel for SCHEDULE(runtime)
      313             #endif
      314   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      315   p         			local_int_t row = A.tdg[l][i];
      316   p         			const double * const currentValues = A.matrixValues[row];
      317   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      318   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      319   p         			const double currentDiagonal = matrixDiagonal[row][0];
      320   p         			svfloat64_t contribs = svdup_f64(0.0);
      321             
      322   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      323   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      324             				
      325   p      s  				svfloat64_t mtxValues = svld1(pg, &currentValues[j]);
      326   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      327   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      328             
      329   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      330   p      s  			}
      331             
      332   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      333   p         			double sum = rv[row] - totalContribution;
      334             
      335   p         			sum += xv[row] * currentDiagonal;
      336   p         			xv[row] = sum / currentDiagonal;
      337   pi        		}
      338    i        	}
      339             
      340             	/*
      341             	 * BACKWARD SWEEP
      342             	 */
      343    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      344             #ifndef HPCG_NO_OPENMP
      345             #pragma omp parallel for SCHEDULE(runtime)
      346             #endif
      347   pi        		for ( local_int_t i = A.tdg[l].size(); i >= 0; i-- ) {
      348   p         			local_int_t row = A.tdg[l][i];
      349   p         			const double * const currentValues = A.matrixValues[row];
      350   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      351   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      352   p         			const double currentDiagonal = matrixDiagonal[row][0];
      353   p         			svfloat64_t contribs = svdup_f64(0.0);
      354             
      355   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      356   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      357             				
      358   p      s  				svfloat64_t mtxValues = svld1(pg, &currentValues[j]);
      359   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      360   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      361             
      362   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      363   p      s  			}
      364             
      365   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      366   p         			totalContribution -= xv[row] * currentDiagonal; // remove diagonal contribution
      367   p         			double sum = rv[row] - totalContribution; // substract contributions from RHS
      368   p         			xv[row] = sum / currentDiagonal; // update row
      369             
      370             			// SPMV part
      371   p         			totalContribution += xv[row] * currentDiagonal; // add updated diagonal contribution
      372   p         			yv[row] = totalContribution; // update SPMV output vector
      373             			
      374   p         		}
      375             	}
      376             
      377             	return 0;
      378             }
      379             /*
      380              * END OF TDG FUSED SYMGS-SPMV VERSION
      381              */
      382             
      383             /*
      384              * BLOCK COLORED VERSION
      385              */
      386             int ComputeSYMGS_BLOCK_SVE(const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
      387             	assert(x.localLength >= A.localNumberOfColumns);
      388             
      389             #ifndef HPCG_NO_MPI
      390             	ExchangeHalo(A, x);
      391             #endif
      392             
      393             	double **matrixDiagonal = A.matrixDiagonal;
      394             	const double * const rv = r.values;
      395             	double * const xv = x.values;
      396             	local_int_t firstBlock = 0;
      397    i        	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
      398             
      399             LIKWID_START(trace.enabled, "symgs_bc");		
      400             
      401             	/*
      402             	 * FORWARD SWEEP
      403             	 */
      404             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) { // for each color
      405             		if ( color > 0 ) {
      406    i        			firstBlock += A.numberOfBlocksInColor[color-1];
      407    i        			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
      408             		}
      409             #ifndef HPCG_NO_OPENMP
      410             #pragma omp parallel for SCHEDULE(runtime)
      411             #endif
      412   p         		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) { // for each superblock with the same color
      413   p         			local_int_t firstRow = block * A.blockSize;
      414   p         			local_int_t firstChunk = firstRow / A.chunkSize;
      415   p         			local_int_t lastChunk = (firstRow + A.blockSize * A.chunkSize) / A.chunkSize;
      416             
      417   p         			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) { // for each chunk of this super block
      418   p         				local_int_t first = A.chunkSize * chunk;
      419   p         				local_int_t last = first + A.chunkSize;
      420   p         				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      421   p         				local_int_t i = first;
      422   p         				if ( A.chunkSize == 4 ) {
      423   p         					const double * const currentValues0 = A.matrixValues[i  ];
      424   p         					const double * const currentValues1 = A.matrixValues[i+1];
      425   p         					const double * const currentValues2 = A.matrixValues[i+2];
      426   p         					const double * const currentValues3 = A.matrixValues[i+3];
      427             
      428   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      429   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      430   p         					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      431   p         					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      432             
      433   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      434   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      435   p         					const double currentDiagonal2 = matrixDiagonal[i+2][0];
      436   p         					const double currentDiagonal3 = matrixDiagonal[i+3][0];
      437             
      438   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      439   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      440   p         					svfloat64_t contribs2 = svdup_f64(0.0);
      441   p         					svfloat64_t contribs3 = svdup_f64(0.0);
      442             
      443   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      444   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      445             
      446   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      447   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      448   p      s  						svfloat64_t values2 = svld1_f64(pg, &currentValues2[j]);
      449   p      s  						svfloat64_t values3 = svld1_f64(pg, &currentValues3[j]);
      450             
      451   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      452   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      453   p      s  						svuint64_t indices2 = svld1sw_u64(pg, &currentColIndices2[j]);
      454   p      s  						svuint64_t indices3 = svld1sw_u64(pg, &currentColIndices3[j]);
      455             
      456   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      457   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      458   p      s  						svfloat64_t xvv2 = svld1_gather_u64index_f64(pg, xv, indices2);
      459   p      s  						svfloat64_t xvv3 = svld1_gather_u64index_f64(pg, xv, indices3);
      460             
      461   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      462   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1);
      463   p      s  						contribs2 = svmla_f64_m(pg, contribs2, xvv2, values2);
      464   p      s  						contribs3 = svmla_f64_m(pg, contribs3, xvv3, values3);
      465   p      s  					}
      466             
      467   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      468   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      469   p         					double totalContribution2 = svaddv_f64(svptrue_b64(), contribs2);
      470   p         					double totalContribution3 = svaddv_f64(svptrue_b64(), contribs3);
      471             
      472   p         					double sum0 = rv[i  ] - totalContribution0;
      473   p         					double sum1 = rv[i+1] - totalContribution1;
      474   p         					double sum2 = rv[i+2] - totalContribution2;
      475   p         					double sum3 = rv[i+3] - totalContribution3;
      476             
      477   p         					sum0 += xv[i  ] * currentDiagonal0;
      478   p         					sum1 += xv[i+1] * currentDiagonal1;
      479   p         					sum2 += xv[i+2] * currentDiagonal2;
      480   p         					sum3 += xv[i+3] * currentDiagonal3;
      481             
      482   p         					xv[i  ] = sum0 / currentDiagonal0;
      483   p         					xv[i+1] = sum1 / currentDiagonal1;
      484   p         					xv[i+2] = sum2 / currentDiagonal2;
      485   p         					xv[i+3] = sum3 / currentDiagonal3;
      486   p         				} else if ( A.chunkSize == 2 ) {
      487   p         					const double * const currentValues0 = A.matrixValues[i  ];
      488   p         					const double * const currentValues1 = A.matrixValues[i+1];
      489             
      490   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      491   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      492             
      493   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      494   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      495             
      496   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      497   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      498             
      499   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      500   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      501             
      502   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      503   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      504             
      505   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      506   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      507             
      508   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      509   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      510             
      511   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      512   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1);
      513   p      s  					}
      514             
      515   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      516   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      517             
      518   p         					double sum0 = rv[i  ] - totalContribution0;
      519   p         					double sum1 = rv[i+1] - totalContribution1;
      520             
      521   p         					sum0 += xv[i  ] * currentDiagonal0;
      522   p         					sum1 += xv[i+1] * currentDiagonal1;
      523             
      524   p         					xv[i  ] = sum0 / currentDiagonal0;
      525   p         					xv[i+1] = sum1 / currentDiagonal1;
      526   p         				} else { //A.chunkSize == 1
      527   p         					const double * const currentValues0 = A.matrixValues[i  ];
      528             
      529   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      530             
      531   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      532             
      533   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      534             
      535   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      536   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      537             
      538   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      539             
      540   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      541             
      542   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      543             
      544   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      545   p      s  					}
      546             
      547   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      548             
      549   p         					double sum0 = rv[i  ] - totalContribution0;
      550             
      551   p         					sum0 += xv[i  ] * currentDiagonal0;
      552             
      553   p         					xv[i  ] = sum0 / currentDiagonal0;
      554   p         				}
      555   p         			}
      556   p         		}
      557             	}
      558             
      559             	firstBlock = A.numberOfBlocks-1;
      560    i        	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
      561             	/*
      562             	 * BACKWARD SWEEP
      563             	 */
      564             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
      565             		if ( color < A.numberOfColors-1 ) {
      566    i        			firstBlock -= A.numberOfBlocksInColor[color+1];
      567    i        			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
      568             		}
      569             #ifndef HPCG_NO_OPENMP
      570             #pragma omp parallel for SCHEDULE(runtime)
      571             #endif
      572   p         		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) {
      573   p         			local_int_t firstRow = ((block+1) * A.blockSize) - 1;
      574   p         			local_int_t firstChunk = firstRow / A.chunkSize;
      575   p         			local_int_t lastChunk = (firstRow - A.blockSize * A.chunkSize) / A.chunkSize;
      576             
      577   p         			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
      578   p         				local_int_t first = A.chunkSize * chunk;
      579   p         				local_int_t last = first + A.chunkSize;
      580             
      581   p         				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      582   p         				local_int_t i = first;
      583   p         				if ( A.chunkSize == 4 ) {
      584   p         					const double * const currentValues3 = A.matrixValues[i+3];
      585   p         					const double * const currentValues2 = A.matrixValues[i+2];
      586   p         					const double * const currentValues1 = A.matrixValues[i+1];
      587   p         					const double * const currentValues0 = A.matrixValues[i  ];
      588             
      589   p         					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      590   p         					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      591   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      592   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      593             
      594   p         					const double currentDiagonal3 = matrixDiagonal[i+3][0];
      595   p         					const double currentDiagonal2 = matrixDiagonal[i+2][0];
      596   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      597   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      598             
      599   p         					svfloat64_t contribs3 = svdup_f64(0.0);
      600   p         					svfloat64_t contribs2 = svdup_f64(0.0);
      601   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      602   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      603             
      604   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      605   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      606             
      607   p      s  						svfloat64_t values3 = svld1_f64(pg, &currentValues3[j]);
      608   p      s  						svfloat64_t values2 = svld1_f64(pg, &currentValues2[j]);
      609   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      610   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      611             
      612   p      s  						svuint64_t indices3 = svld1sw_u64(pg, &currentColIndices3[j]);
      613   p      s  						svuint64_t indices2 = svld1sw_u64(pg, &currentColIndices2[j]);
      614   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      615   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      616             
      617   p      s  						svfloat64_t xvv3 = svld1_gather_u64index_f64(pg, xv, indices3);
      618   p      s  						svfloat64_t xvv2 = svld1_gather_u64index_f64(pg, xv, indices2);
      619   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      620   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      621             
      622   p      s  						contribs3 = svmla_f64_m(pg, contribs3, xvv3, values3 );
      623   p      s  						contribs2 = svmla_f64_m(pg, contribs2, xvv2, values2 );
      624   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1 );
      625   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      626   p      s  					}
      627             
      628   p         					double totalContribution3 = svaddv_f64(svptrue_b64(), contribs3);
      629   p         					double totalContribution2 = svaddv_f64(svptrue_b64(), contribs2);
      630   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      631   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      632             
      633   p         					double sum3 = rv[i+3] - totalContribution3;
      634   p         					double sum2 = rv[i+2] - totalContribution2;
      635   p         					double sum1 = rv[i+1] - totalContribution1;
      636   p         					double sum0 = rv[i  ] - totalContribution0;
      637             
      638   p         					sum3 += xv[i+3] * currentDiagonal3;
      639   p         					sum2 += xv[i+2] * currentDiagonal2;
      640   p         					sum1 += xv[i+1] * currentDiagonal1;
      641   p         					sum0 += xv[i  ] * currentDiagonal0;
      642             					
      643   p         					xv[i+3] = sum3 / currentDiagonal3;
      644   p         					xv[i+2] = sum2 / currentDiagonal2;
      645   p         					xv[i+1] = sum1 / currentDiagonal1;
      646   p         					xv[i  ] = sum0 / currentDiagonal0;
      647   p         				} else if ( A.chunkSize == 2 ) {
      648   p         					const double * const currentValues1 = A.matrixValues[i+1];
      649   p         					const double * const currentValues0 = A.matrixValues[i  ];
      650             
      651   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      652   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      653             
      654   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      655   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      656             
      657   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      658   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      659             
      660   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      661   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      662             
      663   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      664   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      665             
      666   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      667   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      668             
      669   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      670   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      671             
      672   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1 );
      673   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      674   p      s  					}
      675             
      676   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      677   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      678             
      679   p         					double sum1 = rv[i+1] - totalContribution1;
      680   p         					double sum0 = rv[i  ] - totalContribution0;
      681             
      682   p         					sum1 += xv[i+1] * currentDiagonal1;
      683   p         					sum0 += xv[i  ] * currentDiagonal0;
      684             					
      685   p         					xv[i+1] = sum1 / currentDiagonal1;
      686   p         					xv[i  ] = sum0 / currentDiagonal0;
      687   p         				} else { // A.chunkSize == 1
      688   p         					const double * const currentValues0 = A.matrixValues[i  ];
      689             
      690   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      691             
      692   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      693             
      694   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      695             
      696   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      697   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      698             
      699   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      700             
      701   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      702             
      703   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      704             
      705   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      706   p      s  					}
      707             
      708   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      709             
      710   p         					double sum0 = rv[i  ] - totalContribution0;
      711             
      712   p         					sum0 += xv[i  ] * currentDiagonal0;
      713             					
      714   p         				}
      715   p         			}
      716   p         		}
      717             	}
      718             LIKWID_STOP(trace.enabled, "symgs_bc");			
      719             
      720             	return 0;
      721             }
      722             /*
      723              * END OF BLOCK COLORED VERSION
      724              */
      725             #elif defined(HPCG_USE_NEON)
      726             
      727             /**************************************************************************************************/
      728             /**************************************************************************************************/
      729             /**************************************************************************************************/
      730             /* NEON IMPLEMENTATIONS                                                                           */
      731             /**************************************************************************************************/
      732             /**************************************************************************************************/
      733             /**************************************************************************************************/
      734             
      735             #include "arm_neon.h"
      736             
      737             /*
      738              * TDG VERSION
      739              */
      740             int ComputeSYMGS_TDG_NEON(const SparseMatrix & A, const Vector & r, Vector & x) {
      741             	assert(x.localLength == A.localNumberOfColumns);
      742             
      743             #ifndef HPCG_NO_MPI
      744             	ExchangeHalo(A, x);
      745             #endif
      746             
      747             	const double * const rv = r.values;
      748             	double * const xv = x.values;
      749             	double **matrixDiagonal = A.matrixDiagonal;
      750             
      751             	/*
      752             	 * FORWARD
      753             	 */
      754             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      755             #ifndef HPCG_NO_OPENMP
      756             #pragma omp parallel for SCHEDULE(runtime)
      757             #endif
      758             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      759             			local_int_t row = A.tdg[l][i];
      760             			const double * const currentValues = A.matrixValues[row];
      761             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      762             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      763             			const double currentDiagonal = matrixDiagonal[row][0];
      764             			float64x2_t contribs = vdupq_n_f64(0.0);
      765             
      766             			local_int_t j = 0;
      767             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      768             				// Load the needed j values
      769             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      770             				// Load the needed x values
      771             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      772             				float64x2_t xvv = vld1q_f64(aux);
      773             				// Add the contribution
      774             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      775             			}
      776             			// reduce contributions
      777             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      778             			double sum = rv[row] - totalContribution;
      779             			// Add missing values from last loop
      780             			if ( j < currentNumberOfNonzeros ) {
      781             				sum -= currentValues[j] * xv[currentColIndices[j]];
      782             			}
      783             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      784             			xv[row] = sum / currentDiagonal; // update row
      785             		}
      786             	}
      787             
      788             	/*
      789             	 * BACKWARD
      790             	 */
      791             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      792             #ifndef HPCG_NO_OPENMP
      793             #pragma omp parallel for SCHEDULE(runtime)
      794             #endif
      795             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      796             			local_int_t row = A.tdg[l][i];
      797             			const double * const currentValues = A.matrixValues[row];
      798             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      799             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      800             			const double currentDiagonal = matrixDiagonal[row][0];
      801             			float64x2_t contribs = vdupq_n_f64(0.0);
      802             
      803             			local_int_t j = 0;
      804             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      805             				// Load the needed j values
      806             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      807             				// Load the needed x values
      808             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      809             				float64x2_t xvv = vld1q_f64(aux);
      810             				// Add the contribution
      811             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      812             			}
      813             			// reduce contributions
      814             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      815             			double sum = rv[row] - totalContribution;
      816             			// Add missing values from last loop
      817             			if ( j < currentNumberOfNonzeros ) {
      818             				sum -= currentValues[j] * xv[currentColIndices[j]];
      819             			}
      820             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      821             			xv[row] = sum / currentDiagonal; // update row
      822             		}
      823             	}
      824             
      825             	return 0;
      826             }
      827             /*
      828              *
      829              */
      830             ////////////////////////////////////////////////////////////////////////////////
      831             ////////////////////////////////////////////////////////////////////////////////
      832             ////////////////////////////////////////////////////////////////////////////////
      833             /*
      834              * TDG FUSED VERSION
      835              */
      836             int ComputeFusedSYMGS_SPMV_NEON(const SparseMatrix & A, const Vector & r, Vector & x, Vector & y) {
      837             	assert(x.localLength == A.localNumberOfColumns);
      838             
      839             #ifndef HPCG_NO_MPI
      840             	ExchangeHalo(A, x);
      841             #endif
      842             
      843             	const double * const rv = r.values;
      844             	double * const xv = x.values;
      845             	double * const yv = y.values;
      846             	double **matrixDiagonal = A.matrixDiagonal;
      847             
      848             	/*
      849             	 * FORWARD
      850             	 */
      851             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      852             #ifndef HPCG_NO_OPENMP
      853             #pragma omp parallel for SCHEDULE(runtime)
      854             #endif
      855             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      856             			local_int_t row = A.tdg[l][i];
      857             			const double * const currentValues = A.matrixValues[row];
      858             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      859             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      860             			const double currentDiagonal = matrixDiagonal[row][0];
      861             			float64x2_t contribs = vdupq_n_f64(0.0);
      862             
      863             			local_int_t j = 0;
      864             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      865             				// Load the needed j values
      866             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      867             				// Load the needed x values
      868             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      869             				float64x2_t xvv = vld1q_f64(aux);
      870             				// Add the contribution
      871             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      872             			}
      873             			// reduce contributions
      874             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      875             			double sum = rv[row] - totalContribution;
      876             			// Add missing values from last loop
      877             			if ( j < currentNumberOfNonzeros ) {
      878             				sum -= currentValues[j] * xv[currentColIndices[j]];
      879             			}
      880             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      881             			xv[row] = sum / currentDiagonal; // update row
      882             		}
      883             	}
      884             
      885             	/*
      886             	 * BACKWARD (fusing SYMGS and SPMV)
      887             	 */
      888             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      889             #ifndef HPCG_NO_OPENMP
      890             #pragma omp parallel for SCHEDULE(runtime)
      891             #endif
      892             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      893             			local_int_t row = A.tdg[l][i];
      894             			const double * const currentValues = A.matrixValues[row];
      895             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      896             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      897             			const double currentDiagonal = matrixDiagonal[row][0];
      898             			float64x2_t contribs = vdupq_n_f64(0.0);
      899             
      900             			local_int_t j = 0;
      901             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      902             				// Load the needed j values
      903             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      904             				// Load the needed x values
      905             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      906             				float64x2_t xvv = vld1q_f64(aux);
      907             				// Add the contribution
      908             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      909             			}
      910             			// reduce contributions
      911             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      912             			// Add missing values from last loop
      913             			if ( j < currentNumberOfNonzeros ) {
      914             				totalContribution += currentValues[j] * xv[currentColIndices[j]];
      915             			}
      916             			totalContribution -= xv[row] * currentDiagonal; // remove diagonal contribution
      917             			double sum = rv[row] - totalContribution; // substract contributions from RHS
      918             			xv[row] = sum / currentDiagonal; // update row
      919             			// Fusion part
      920             			totalContribution += xv[row] * currentDiagonal; // add updated diagonal contribution
      921             			yv[row] = totalContribution; // update SPMV output vector
      922             		}
      923             	}
      924             
      925             	return 0;
      926             }
      927             /*
      928              *
      929              */
      930             ////////////////////////////////////////////////////////////////////////////////
      931             ////////////////////////////////////////////////////////////////////////////////
      932             ////////////////////////////////////////////////////////////////////////////////
      933             /*
      934              * BLOCK COLORED VERSION
      935              */
      936             int ComputeSYMGS_BLOCK_NEON(const SparseMatrix & A, const Vector & r, Vector & x) {
      937             
      938             	assert(x.localLength >= A.localNumberOfColumns);
      939             	
      940             #ifndef HPCG_NO_MPI
      941             	ExchangeHalo(A, x);
      942             #endif
      943             
      944             	double **matrixDiagonal = A.matrixDiagonal;
      945             	const double * const rv = r.values;
      946             	double * const xv = x.values;
      947             
      948             	local_int_t firstBlock = 0;
      949             	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
      950             	/*
      951             	 * FORWARD
      952             	 */
      953             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) { // for each color
      954             		if ( color > 0 ) {
      955             			firstBlock += A.numberOfBlocksInColor[color-1];
      956             			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
      957             		}
      958             #ifndef HPCG_NO_OPENMP
      959             #pragma omp parallel for SCHEDULE(runtime)
      960             #endif
      961             		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) { // for each super block with the same color
      962             			local_int_t firstRow = block * A.blockSize;
      963             			local_int_t firstChunk = firstRow / A.chunkSize;
      964             			local_int_t lastChunk = (firstRow + A.blockSize*A.chunkSize) / A.chunkSize;
      965             
      966             			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) { // for each chunk of this super block
      967             				local_int_t first = A.chunkSize * chunk;
      968             				local_int_t last = first + A.chunkSize;
      969             
      970             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      971             				local_int_t i = first;
      972             				if ( A.chunkSize == 4 ) {
      973             					const double * const currentValues0 = A.matrixValues[i  ];
      974             					const double * const currentValues1 = A.matrixValues[i+1];
      975             					const double * const currentValues2 = A.matrixValues[i+2];
      976             					const double * const currentValues3 = A.matrixValues[i+3];
      977             
      978             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      979             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      980             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      981             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      982             
      983             					const double currentDiagonal[4] = { matrixDiagonal[i  ][0],\
      984             														matrixDiagonal[i+1][0],\
      985             														matrixDiagonal[i+2][0],\
      986             														matrixDiagonal[i+3][0]};
      987             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
      988             					float64x2_t diagonal23 = vld1q_f64(&currentDiagonal[2]);
      989             
      990             					float64x2_t contribs0 = vdupq_n_f64(0.0);
      991             					float64x2_t contribs1 = vdupq_n_f64(0.0);
      992             					float64x2_t contribs2 = vdupq_n_f64(0.0);
      993             					float64x2_t contribs3 = vdupq_n_f64(0.0);
      994             
      995             					float64x2_t vrv01 = vld1q_f64(&rv[i]);
      996             					float64x2_t vrv23 = vld1q_f64(&rv[i+2]);
      997             
      998             					float64x2_t vxv01 = vld1q_f64(&xv[i]);
      999             					float64x2_t vxv23 = vld1q_f64(&xv[i+2]);
     1000             
     1001             					local_int_t j = 0;
     1002             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1003             						// Load values
     1004             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1005             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1006             						float64x2_t values2 = vld1q_f64(&currentValues2[j]);
     1007             						float64x2_t values3 = vld1q_f64(&currentValues3[j]);
     1008             
     1009             						// Load x
     1010             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1011             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1012             						float64x2_t vxv2 = { xv[currentColIndices2[j]], xv[currentColIndices2[j+1]] };
     1013             						float64x2_t vxv3 = { xv[currentColIndices3[j]], xv[currentColIndices3[j+1]] };
     1014             
     1015             						// Add contribution
     1016             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1017             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1018             						contribs2 = vfmaq_f64(contribs2, values2, vxv2);
     1019             						contribs3 = vfmaq_f64(contribs3, values3, vxv3);
     1020             					}
     1021             					// Reduce contribution
     1022             					// First for i and i+1
     1023             					float64x2_t totalContribution01;
     1024             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1025             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1026             
     1027             					// Then for i+2 and i+3
     1028             					float64x2_t totalContribution23;
     1029             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs2), totalContribution23, 0);
     1030             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs3), totalContribution23, 1);
     1031             
     1032             					// Substract contributions from RHS
     1033             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1034             					float64x2_t sum23 = vsubq_f64(vrv23, totalContribution23);
     1035             
     1036             					// Add contributions from missing elements (if any)
     1037             					if ( j < currentNumberOfNonzeros ) {
     1038             						// Load current values
     1039             						float64x2_t values01 = { currentValues0[j], currentValues1[j] };
     1040             						float64x2_t values23 = { currentValues2[j], currentValues3[j] };
     1041             
     1042             						// Load x
     1043             						float64x2_t vx01 = { xv[currentColIndices0[j]], xv[currentColIndices1[j]] };
     1044             						float64x2_t vx23 = { xv[currentColIndices2[j]], xv[currentColIndices3[j]] };
     1045             
     1046             						// Add contributions
     1047             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1048             						sum23 = vfmsq_f64(sum23, values23, vx23);
     1049             					}
     1050             
     1051             					// Remove diagonal contribution and update rows i and i+1
     1052             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1053             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1054             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1055             
     1056             					// Remove diagonal contribution and update rows i+2 and i+3
     1057             					sum23 = vfmaq_f64(sum23, vxv23, diagonal23);
     1058             					xv[i+2] = vgetq_lane_f64(sum23, 0) / currentDiagonal[2];
     1059             					xv[i+3] = vgetq_lane_f64(sum23, 1) / currentDiagonal[3];
     1060             				} else if ( A.chunkSize == 2 ) {
     1061             					const double * const currentValues0 = A.matrixValues[i  ];
     1062             					const double * const currentValues1 = A.matrixValues[i+1];
     1063             
     1064             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1065             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1066             
     1067             					const double currentDiagonal[2] = { matrixDiagonal[i  ][0],\
     1068             														matrixDiagonal[i+1][0]};
     1069             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1070             
     1071             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1072             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1073             
     1074             					float64x2_t vrv01 = vld1q_f64(&rv[i]);
     1075             
     1076             					float64x2_t vxv01 = vld1q_f64(&xv[i]);
     1077             
     1078             					local_int_t j = 0;
     1079             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1080             						// Load values
     1081             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1082             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1083             
     1084             						// Load x
     1085             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1086             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1087             
     1088             						// Add contribution
     1089             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1090             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1091             					}
     1092             					// Reduce contribution
     1093             					// First for i and i+1
     1094             					float64x2_t totalContribution01;
     1095             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1096             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1097             
     1098             					// Substract contributions from RHS
     1099             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1100             
     1101             					// Add contributions from missing elements (if any)
     1102             					if ( j < currentNumberOfNonzeros ) {
     1103             						// Load current values
     1104             						float64x2_t values01 = { currentValues0[j], currentValues1[j] };
     1105             
     1106             						// Load x
     1107             						float64x2_t vx01 = { xv[currentColIndices0[j]], xv[currentColIndices1[j]] };
     1108             
     1109             						// Add contributions
     1110             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1111             					}
     1112             
     1113             					// Remove diagonal contribution and update rows i and i+1
     1114             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1115             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1116             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1117             				} else { // A.chunkSize == 1
     1118             					const double * const currentValues = A.matrixValues[i];
     1119             					const local_int_t * const currentColIndices = A.mtxIndL[i];
     1120             					const double currentDiagonal = matrixDiagonal[i][0];
     1121             					float64x2_t contribs = vdupq_n_f64(0.0);
     1122             
     1123             					local_int_t j = 0;
     1124             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1125             						// Load values
     1126             						float64x2_t values = vld1q_f64(&currentValues[j]);
     1127             
     1128             						// Load x
     1129             						float64x2_t vxv = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
     1130             
     1131             						// Add contribution
     1132             						contribs = vfmaq_f64(contribs, values, vxv);
     1133             					}
     1134             					// Reduce contribution
     1135             					// First for i and i+1
     1136             					double totalContribution;
     1137             					totalContribution = vaddvq_f64(contribs);
     1138             
     1139             					// Substract contributions from RHS
     1140             					double sum = rv[i] - totalContribution;
     1141             
     1142             					// Add contributions from missing elements (if any)
     1143             					if ( j < currentNumberOfNonzeros ) {
     1144             						sum -= currentValues[j] * xv[currentColIndices[j]];
     1145             					}
     1146             
     1147             					// Remove diagonal contribution and update rows i and i+1
     1148             					sum += xv[i] * currentDiagonal;
     1149             					xv[i] = sum / currentDiagonal;
     1150             				}
     1151             			}
     1152             		}
     1153             	}
     1154             
     1155             	firstBlock = A.numberOfBlocks-1;
     1156             	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
     1157             	/*
     1158             	 * BACKWARD
     1159             	 */
     1160             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
     1161             		if ( color < A.numberOfColors-1 ) {
     1162             			firstBlock -= A.numberOfBlocksInColor[color+1];
     1163             			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
     1164             		}
     1165             #ifndef HPCG_NO_OPENMP
     1166             #pragma omp parallel for SCHEDULE(runtime)
     1167             #endif
     1168             		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) { // we skip a whole superblock on each iteration
     1169             			local_int_t firstRow = ((block+1) * A.blockSize) - 1; // this is the last row of the last block (i.e., next block first row - 1)
     1170             			local_int_t firstChunk = firstRow / A.chunkSize; // this is the  chunk of the row above
     1171             			local_int_t lastChunk = (firstRow - A.blockSize*A.chunkSize) / A.chunkSize; 
     1172             
     1173             			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
     1174             				local_int_t first = A.chunkSize * chunk;
     1175             				local_int_t last = first + A.chunkSize;
     1176             
     1177             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
     1178             				if ( A.chunkSize == 4 ) {
     1179             					local_int_t i = last-1-3;
     1180             
     1181             					const double * const currentValues3 = A.matrixValues[i+3];
     1182             					const double * const currentValues2 = A.matrixValues[i+2];
     1183             					const double * const currentValues1 = A.matrixValues[i+1];
     1184             					const double * const currentValues0 = A.matrixValues[i  ];
     1185             
     1186             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
     1187             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
     1188             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1189             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1190             
     1191             					const double currentDiagonal[4] = {\
     1192             							matrixDiagonal[i  ][0],\
     1193             							matrixDiagonal[i+1][0],\
     1194             							matrixDiagonal[i+2][0],\
     1195             							matrixDiagonal[i+3][0]};
     1196             
     1197             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1198             					float64x2_t diagonal23 = vld1q_f64(&currentDiagonal[2]);
     1199             
     1200             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1201             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1202             					float64x2_t contribs2 = vdupq_n_f64(0.0);
     1203             					float64x2_t contribs3 = vdupq_n_f64(0.0);
     1204             
     1205             					float64x2_t vrv23 = vld1q_f64(&rv[i+2]);
     1206             					float64x2_t vrv01 = vld1q_f64(&rv[i  ]);
     1207             
     1208             					float64x2_t vxv23 = vld1q_f64(&xv[i+2]);
     1209             					float64x2_t vxv01 = vld1q_f64(&xv[i  ]);
     1210             
     1211             					local_int_t j = 0;
     1212             					for ( j = currentNumberOfNonzeros-2; j >= 0; j -= 2 ) {
     1213             						// Load values
     1214             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1215             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1216             						float64x2_t values2 = vld1q_f64(&currentValues2[j]);
     1217             						float64x2_t values3 = vld1q_f64(&currentValues3[j]);
     1218             
     1219             						// Load x
     1220             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1221             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1222             						float64x2_t vxv2 = { xv[currentColIndices2[j]], xv[currentColIndices2[j+1]] };
     1223             						float64x2_t vxv3 = { xv[currentColIndices3[j]], xv[currentColIndices3[j+1]] };
     1224             
     1225             						// Add contribution
     1226             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1227             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1228             						contribs2 = vfmaq_f64(contribs2, values2, vxv2);
     1229             						contribs3 = vfmaq_f64(contribs3, values3, vxv3);
     1230             					}
     1231             					// Reduce contribution
     1232             					// First for i and i-1
     1233             					float64x2_t totalContribution01;
     1234             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1235             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1236             
     1237             					// Then for i-2 and i-3
     1238             					float64x2_t totalContribution23;
     1239             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs2), totalContribution23, 0);
     1240             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs3), totalContribution23, 1);
     1241             
     1242             					// Substract contributions from RHS
     1243             					float64x2_t sum23 = vsubq_f64(vrv23, totalContribution23);
     1244             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1245             
     1246             					// Add contributions from missing elements (if any)
     1247             					if ( j == -1 ) {
     1248             						// Load current values
     1249             						float64x2_t values23 = { currentValues2[j+1], currentValues3[j+1] };
     1250             						float64x2_t values01 = { currentValues0[j+1], currentValues1[j+1] };
     1251             
     1252             						// Load x
     1253             						float64x2_t vx23 = { xv[currentColIndices2[j+1]], xv[currentColIndices3[j+1]] };
     1254             						float64x2_t vx01 = { xv[currentColIndices0[j+1]], xv[currentColIndices1[j+1]] };
     1255             
     1256             						// Add contributions
     1257             						sum23 = vfmsq_f64(sum23, values23, vx23);
     1258             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1259             					}
     1260             
     1261             					// Remove diagonal contribution and update rows i-2 and i-3
     1262             					sum23 = vfmaq_f64(sum23, vxv23, diagonal23);
     1263             					xv[i+3] = vgetq_lane_f64(sum23, 1) / currentDiagonal[3];
     1264             					xv[i+2] = vgetq_lane_f64(sum23, 0) / currentDiagonal[2];
     1265             
     1266             					// Remove diagonal contribution and update rows i and i-1
     1267             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1268             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1269             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1270             				} else if ( A.chunkSize == 2 ) {
     1271             					local_int_t i = last-1-1;
     1272             
     1273             					const double * const currentValues1 = A.matrixValues[i+1];
     1274             					const double * const currentValues0 = A.matrixValues[i  ];
     1275             
     1276             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1277             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1278             
     1279             					const double currentDiagonal[2] = {\
     1280             							matrixDiagonal[i  ][0],\
     1281             							matrixDiagonal[i+1][0]};
     1282             
     1283             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1284             
     1285             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1286             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1287             
     1288             					float64x2_t vrv01 = vld1q_f64(&rv[i  ]);
     1289             
     1290             					float64x2_t vxv01 = vld1q_f64(&xv[i  ]);
     1291             
     1292             					local_int_t j = 0;
     1293             					for ( j = currentNumberOfNonzeros-2; j >= 0; j -= 2 ) {
     1294             						// Load values
     1295             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1296             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1297             
     1298             						// Load x
     1299             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1300             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1301             
     1302             						// Add contribution
     1303             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1304             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1305             					}
     1306             					// Reduce contribution
     1307             					// First for i and i-1
     1308             					float64x2_t totalContribution01;
     1309             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1310             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1311             
     1312             					// Substract contributions from RHS
     1313             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1314             
     1315             					// Add contributions from missing elements (if any)
     1316             					if ( j == -1 ) {
     1317             						// Load current values
     1318             						float64x2_t values01 = { currentValues0[j+1], currentValues1[j+1] };
     1319             
     1320             						// Load x
     1321             						float64x2_t vx01 = { xv[currentColIndices0[j+1]], xv[currentColIndices1[j+1]] };
     1322             
     1323             						// Add contributions
     1324             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1325             					}
     1326             
     1327             					// Remove diagonal contribution and update rows i and i-1
     1328             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1329             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1330             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1331             				} else { // A.chunkSize == 1
     1332             					local_int_t i = last - 1; // == first
     1333             					const double * const currentValues = A.matrixValues[i];
     1334             					const local_int_t * const currentColIndices = A.mtxIndL[i];
     1335             					const double currentDiagonal = matrixDiagonal[i][0];
     1336             
     1337             					float64x2_t contribs = vdupq_n_f64(0.0);
     1338             
     1339             					local_int_t j = 0;
     1340             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1341             						// Load values
     1342             						float64x2_t values = vld1q_f64(&currentValues[j]);
     1343             
     1344             						// Load x
     1345             						float64x2_t vxv = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
     1346             
     1347             						// Add contribution
     1348             						contribs = vfmaq_f64(contribs, values, vxv);
     1349             					}
     1350             					// Reduce contribution
     1351             					double totalContribution = vaddvq_f64(contribs);
     1352             
     1353             					// Substract contribution from RHS
     1354             					double sum = rv[i] - totalContribution;
     1355             
     1356             					// Add contributions from missing elements (if any)
     1357             					if ( j < currentNumberOfNonzeros ) {
     1358             						sum -= currentValues[j] * xv[currentColIndices[j]];
     1359             					}
     1360             
     1361             					// Remove diagonal contribution and updated row i
     1362             					sum += xv[i] * currentDiagonal;
     1363             					xv[i] = sum / currentDiagonal;
     1364             				}
     1365             			}
     1366             		}
     1367             	}
     1368             
     1369             	return 0;
     1370             }
     1371             /*
     1372              *
     1373              */
     1374             #endif
     1375             //#else // !HPCG_USE_SVE ! HPCG_USE_NEON
     1376             
     1377             int ComputeFusedSYMGS_SPMV ( const SparseMatrix & A, const Vector & r, Vector & x, Vector & y ) {
     1378             	assert(x.localLength == A.localNumberOfColumns);
     1379             
     1380             #ifndef HPCG_NO_MPI
     1381             	ExchangeHalo(A, x);
     1382             #endif
     1383             
     1384             	const double * const rv = r.values;
     1385             	double * const xv = x.values;
     1386             	double * const yv = y.values;
     1387             	double **matrixDiagonal = A.matrixDiagonal;
     1388             
     1389             	/*
     1390             	 * FORWARD
     1391             	 */
     1392    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1393             #ifndef HPCG_NO_OPENMP
     1394             #pragma omp parallel for SCHEDULE(runtime)
     1395             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1396   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
     1397   p         			local_int_t row = A.tdg[l][i];
     1398   p         			const double * const currentValues = A.matrixValues[row];
     1399   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1400   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1401   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1402   p         			double sum = rv[row];
     1403             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1404   p     8v  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j++ ) {
     1405   p     8v  				local_int_t curCol = currentColIndices[j];
     1406   p     8v  				sum -= currentValues[j] * xv[curCol];
     1407   p     8v  			}
     1408   p         			sum += xv[row] * currentDiagonal;
     1409   p         			xv[row] = sum / currentDiagonal;
     1410   pi        		}
     1411    i        	}
     1412             
     1413             	/*
     1414             	 * BACKWARD (fusing SYMGS and SPMV)
     1415             	 */
     1416    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1417             #ifndef HPCG_NO_OPENMP
     1418             #pragma omp parallel for SCHEDULE(runtime)
     1419             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1420   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
     1421   p         			local_int_t row = A.tdg[l][i];
     1422   p         			const double * const currentValues = A.matrixValues[row];
     1423   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1424   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1425   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1426   p         			double sum = 0.0;
     1427             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1428   p     8v  			for ( local_int_t j = currentNumberOfNonzeros-1; j >= 0; j-- ) {
     1429   p     8v  				local_int_t curCol = currentColIndices[j];
     1430   p     8v  				sum += currentValues[j] * xv[curCol];
     1431   p     8v  			}
     1432   p         			sum -= xv[row] * currentDiagonal;
     1433   p         			xv[row] = (rv[row] - sum) / currentDiagonal;
     1434   p         			sum += xv[row] * currentDiagonal;
     1435   p         			yv[row] = sum;
     1436   p         		}
     1437             	}
     1438             
     1439             	return 0;
     1440             }
     1441             
     1442             int ComputeSYMGS_TDG ( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
     1443             
     1444             	assert( x.localLength == A.localNumberOfColumns);
     1445             
     1446             #ifndef HPCG_NO_MPI
     1447             	ExchangeHalo(A,x);
     1448             #endif
     1449             
     1450             	const double * const rv = r.values;
     1451             	double * const xv = x.values;
     1452             	double **matrixDiagonal = A.matrixDiagonal;
     1453             
     1454             /*#ifndef HPCG_NO_OPENMP
     1455             #pragma omp parallel SCHEDULE(runtime)
     1456             {
     1457             #endif
     1458             */
     1459             #pragma statement scache_isolate_way L2=10
     1460             #pragma statement scache_isolate_assign xv
     1461             
     1462             	/*
     1463             	 * FORWARD
     1464             	 */
     1465    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1466             #ifndef HPCG_NO_OPENMP
     1467             #pragma omp parallel for SCHEDULE(runtime)
     1468             #endif
     1469             		#pragma loop unroll_and_jam(4)
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1470   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
     1471   p         			local_int_t row = A.tdg[l][i];
     1472   p         			const double * const currentValues = A.matrixValues[row];
     1473   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1474   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1475   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1476   p         			double sum = rv[row];
     1477             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1478   p     8v  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j++ ) {
     1479   p     8v  				local_int_t curCol = currentColIndices[j];
     1480   p     8v  				sum -= currentValues[j] * xv[curCol];
     1481   p     8v  			}
     1482   p         			sum += xv[row] * currentDiagonal;
     1483   p         			xv[row] = sum / currentDiagonal;
     1484   pi        		}
     1485    i        	}
     1486             
     1487             	/*
     1488             	 * BACKWARD
     1489             	 */
     1490    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1491             #ifndef HPCG_NO_OPENMP
     1492             #pragma omp parallel for SCHEDULE(runtime)
     1493             #endif
     1494             		#pragma loop unroll_and_jam(4)
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1495   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
     1496   p         			local_int_t row = A.tdg[l][i];
     1497   p         			const double * const currentValues = A.matrixValues[row];
     1498   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1499   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1500   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1501   p         			double sum = rv[row];
     1502             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1503   p     8v  			for ( local_int_t j = currentNumberOfNonzeros-1; j >= 0; j-- ) {
     1504   p     8v  				local_int_t curCol = currentColIndices[j];
     1505   p     8v  				sum -= currentValues[j] * xv[curCol];
     1506   p     8v  			}
     1507   p         			sum += xv[row] * currentDiagonal;
     1508   p         			xv[row] = sum / currentDiagonal;
     1509   p         		}
     1510             	}
     1511             
     1512             	#pragma statement end_scache_isolate_assign
     1513             	#pragma statement end_scache_isolate_way
     1514             /*#ifndef HPCG_NO_OPENMP
     1515             }
     1516             #endif*/
     1517             
     1518             	return 0;
     1519             }
     1520             
     1521             int ComputeSYMGS_BLOCK( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
     1522             
     1523             	assert(x.localLength >= A.localNumberOfColumns);
     1524             	
     1525             #ifndef HPCG_NO_MPI
     1526             	ExchangeHalo(A, x);
     1527             #endif
     1528             
     1529             	const local_int_t nrow = A.localNumberOfRows;
     1530             	double **matrixDiagonal = A.matrixDiagonal;
     1531             	const double * const rv = r.values;
     1532             	double * const xv = x.values;
     1533             
     1534             	local_int_t firstBlock = 0;
     1535    i        	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
     1536             	/*
     1537             	 * FORWARD
     1538             	 */
     1539             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) {
     1540             		if ( color > 0 ) {
     1541    i        			firstBlock += A.numberOfBlocksInColor[color-1];
     1542    i        			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
     1543             		}
     1544             #ifndef HPCG_NO_OPENMP
     1545             #pragma omp parallel for SCHEDULE(runtime)
     1546             #endif
     1547   p         		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) {
     1548   p         			local_int_t firstRow = block * A.blockSize;
     1549   p         			local_int_t firstChunk = firstRow / A.chunkSize;
     1550   p         			local_int_t lastChunk = (firstRow + A.blockSize*A.chunkSize) / A.chunkSize;
     1551             
     1552   p         			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) {
     1553   p         				local_int_t first = A.chunkSize * chunk;
     1554   p         				local_int_t last = first + A.chunkSize;
     1555             
     1556             				//for ( local_int_t i = first; i < last; i+= (A.chunkSize/2)) {
     1557   p         				local_int_t i = first;
     1558   p         				if ( A.chunkSize == 4 ) {
     1559   p         					double sum0 = rv[i+0];
     1560   p         					double sum1 = rv[i+1];
     1561   p         					double sum2 = rv[i+2];
     1562   p         					double sum3 = rv[i+3];
     1563             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1564   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1565   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1566   p      s  						sum1 -= A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
     1567   p      s  						sum2 -= A.matrixValues[i+2][j] * xv[A.mtxIndL[i+2][j]];
     1568   p      s  						sum3 -= A.matrixValues[i+3][j] * xv[A.mtxIndL[i+3][j]];
     1569   pi     s  					}
     1570   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1571   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1572   p         					sum1 += matrixDiagonal[i+1][0] * xv[i+1];
     1573   p         					xv[i+1] = sum1 / matrixDiagonal[i+1][0];
     1574   p         					sum2 += matrixDiagonal[i+2][0] * xv[i+2];
     1575   p         					xv[i+2] = sum2 / matrixDiagonal[i+2][0];
     1576   p         					sum3 += matrixDiagonal[i+3][0] * xv[i+3];
     1577   p         					xv[i+3] = sum3 / matrixDiagonal[i+3][0];
     1578   p         				} else if ( A.chunkSize == 2 ) {
     1579   p         					double sum0 = rv[i+0];
     1580   p         					double sum1 = rv[i+1];
     1581             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1582   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1583   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1584   p      s  						sum1 -= A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
     1585   pi     s  					}
     1586   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1587   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1588   p         					sum1 += matrixDiagonal[i+1][0] * xv[i+1];
     1589   p         					xv[i+1] = sum1 / matrixDiagonal[i+1][0];
     1590   p         				} else { // A.chunkSize == 1
     1591   p         					double sum0 = rv[i+0];
     1592             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1593   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1594   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1595   pi     s  					}
     1596   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1597   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1598   p         				}
     1599   p         			}
     1600   p         		}
     1601             	}
     1602             
     1603             	firstBlock = A.numberOfBlocks-1;
     1604    i        	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
     1605             	/*
     1606             	 * BACKWARD
     1607             	 */
     1608             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
     1609             		if ( color < A.numberOfColors-1 ) {
     1610    i        			firstBlock -= A.numberOfBlocksInColor[color+1];
     1611    i        			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
     1612             		}
     1613             #ifndef HPCG_NO_OPENMP
     1614             #pragma omp parallel for SCHEDULE(runtime)
     1615             #endif
     1616   p         		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) {
     1617   p         			local_int_t firstRow = ((block+1) * A.blockSize) - 1; // this is the last row of the last block
     1618   p         			local_int_t firstChunk = firstRow / A.chunkSize; // this is the  chunk of the row above
     1619   p         			local_int_t lastChunk = (firstRow - A.blockSize*A.chunkSize) / A.chunkSize; 
     1620             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1621   p         			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
     1622   p         				local_int_t first = A.chunkSize * chunk;
     1623   p         				local_int_t last = first + A.chunkSize;
     1624             
     1625             				//for ( local_int_t i = last-1; i >= first; i -= (A.chunkSize/2)) {
     1626   p         				local_int_t i = last-1;
     1627   p         				if ( A.chunkSize == 4 ) {
     1628   p         					double sum3 = rv[i-3];
     1629   p         					double sum2 = rv[i-2];
     1630   p         					double sum1 = rv[i-1];
     1631   p         					double sum0 = rv[i  ];
     1632             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.28, ITR: 32, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1633   pi     v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1634   p      v  						sum3 -= A.matrixValues[i-3][j] * xv[A.mtxIndL[i-3][j]];
     1635   p      v  						sum2 -= A.matrixValues[i-2][j] * xv[A.mtxIndL[i-2][j]];
     1636   p      v  						sum1 -= A.matrixValues[i-1][j] * xv[A.mtxIndL[i-1][j]];
     1637   p      v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1638   p      v  					}
     1639   p         					sum3 += matrixDiagonal[i-3][0] * xv[i-3];
     1640   p         					xv[i-3] = sum3 / matrixDiagonal[i-3][0];
     1641             
     1642   p         					sum2 += matrixDiagonal[i-2][0] * xv[i-2];
     1643   p         					xv[i-2] = sum2 / matrixDiagonal[i-2][0];
     1644             
     1645   p         					sum1 += matrixDiagonal[i-1][0] * xv[i-1];
     1646   p         					xv[i-1] = sum1 / matrixDiagonal[i-1][0];
     1647             
     1648   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1649   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1650   p         				} else if ( A.chunkSize == 2 ) {
     1651   p         					double sum1 = rv[i-1];
     1652   p         					double sum0 = rv[i  ];
     1653             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 96, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1654   pi    4v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1655   p     4v  						sum1 -= A.matrixValues[i-1][j] * xv[A.mtxIndL[i-1][j]];
     1656   p     4v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1657   p     4v  					}
     1658             
     1659   p         					sum1 += matrixDiagonal[i-1][0] * xv[i-1];
     1660   p         					xv[i-1] = sum1 / matrixDiagonal[i-1][0];
     1661             
     1662   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1663   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1664   p         				} else { // A.chunkSize == 1
     1665   p         					double sum0 = rv[i  ];
     1666             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1667   pi    8v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1668   p     8v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1669   p     8v  					}
     1670             
     1671   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1672   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1673   p         				}
     1674   p         			}
     1675   p         		}
     1676             	}
     1677             
     1678             	return 0;
     1679             }
     1680             //#endif
     1681             
     1682             
     1683             
     1684             /*!
     1685               Routine to compute one step of symmetric Gauss-Seidel:
     1686             
     1687               Assumption about the structure of matrix A:
     1688               - Each row 'i' of the matrix has nonzero diagonal value whose address is matrixDiagonal[i]
     1689               - Entries in row 'i' are ordered such that:
     1690                    - lower triangular terms are stored before the diagonal element.
     1691                    - upper triangular terms are stored after the diagonal element.
     1692                    - No other assumptions are made about entry ordering.
     1693             
     1694               Symmetric Gauss-Seidel notes:
     1695               - We use the input vector x as the RHS and start with an initial guess for y of all zeros.
     1696               - We perform one forward sweep.  Since y is initially zero we can ignore the upper triangular terms of A.
     1697               - We then perform one back sweep.
     1698                    - For simplicity we include the diagonal contribution in the for-j loop, then correct the sum after
     1699             
     1700               @param[in] A the known system matrix
     1701               @param[in] r the input vector
     1702               @param[inout] x On entry, x should contain relevant values, on exit x contains the result of one symmetric GS sweep with r as the RHS.
     1703             
     1704               @return returns 0 upon success and non-zero otherwise
     1705             
     1706               @warning Early versions of this kernel (Version 1.1 and earlier) had the r and x arguments in reverse order, and out of sync with other kernels.
     1707             
     1708               @see ComputeSYMGS_ref
     1709             */
     1710             int ComputeSYMGS( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace) {
     1711             
     1712             	// This function is just a stub right now which decides which implementation of the SYMGS will be executed (TDG or block coloring)
     1713             	if ( A.TDG ) {
     1714             #ifdef HPCG_USE_NEON
     1715             		return ComputeSYMGS_TDG_NEON(A, r, x);
     1716             #elif defined HPCG_USE_SVE
     1717    i        		return ComputeSYMGS_TDG_SVE(A, r, x, trace);
     1718             #else
     1719             		return ComputeSYMGS_TDG(A, r, x, trace);
     1720             #endif
     1721             	}
     1722             #ifdef HPCG_USE_NEON
     1723             	return ComputeSYMGS_BLOCK_NEON(A, r, x);
     1724             #elif defined HPCG_USE_SVE
     1725             	return ComputeSYMGS_BLOCK_SVE(A, r, x, trace);
     1726             #else
     1727             	return ComputeSYMGS_BLOCK(A, r, x, trace);
     1728             #endif
     1729             }
     1730             
     1731             inline void SYMGS_VERSION_1(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     1732             	
     1733             	double **matrixDiagonal = A.matrixDiagonal;
     1734             
     1735             	/*
     1736             	 * FORWARD SWEEP
     1737             	 */
     1738             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1739             		local_int_t tdgLevelSize = A.tdg[l].size();
     1740             		if((tdgLevelSize%2) == 0) {
     1741             #ifndef HPCG_NO_OPENMP
     1742             #pragma omp parallel for SCHEDULE(runtime)
     1743             #endif
     1744             		for ( local_int_t i = 0; i < tdgLevelSize; i+=2 ) {
     1745             			local_int_t row_1 = A.tdg[l][i];
     1746             			local_int_t row_2 = A.tdg[l][i+1];
     1747             			const double * const currentValues_1 = A.matrixValues[row_1];
     1748             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1749             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1750             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1751             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1752             
     1753             			const double * const currentValues_2 = A.matrixValues[row_2];
     1754             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1755             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1756             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1757             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1758             			
     1759             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     1760             
     1761             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1762             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1763             				
     1764             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1765             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1766             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1767             
     1768             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1769             
     1770             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1771             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1772             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1773             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1774             
     1775             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1776             			}
     1777             
     1778             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1779             			double sum_1 = rv[row_1] - totalContribution_1;
     1780             
     1781             			sum_1 += xv[row_1] * currentDiagonal_1;
     1782             			xv[row_1] = sum_1 / currentDiagonal_1;
     1783             
     1784             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1785             			double sum_2 = rv[row_2] - totalContribution_2;
     1786             
     1787             			sum_2 += xv[row_2] * currentDiagonal_2;
     1788             			xv[row_2] = sum_2 / currentDiagonal_2;
     1789             		}
     1790             		}
     1791             		else
     1792             		{
     1793             #ifndef HPCG_NO_OPENMP
     1794             #pragma omp parallel for SCHEDULE(runtime)
     1795             #endif
     1796             		for ( local_int_t i = 0; i < tdgLevelSize; i++ ) {
     1797             			local_int_t row = A.tdg[l][i];
     1798             			const double * const currentValues = A.matrixValues[row];
     1799             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1800             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1801             			const double currentDiagonal = matrixDiagonal[row][0];
     1802             			svfloat64_t contribs = svdup_f64(0.0);
     1803             
     1804             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1805             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1806             				
     1807             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1808             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1809             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1810             
     1811             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1812             			}
     1813             
     1814             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1815             			double sum = rv[row] - totalContribution;
     1816             
     1817             			sum += xv[row] * currentDiagonal;
     1818             			xv[row] = sum / currentDiagonal;
     1819             		}
     1820             		}
     1821             	}
     1822             
     1823             	/*
     1824             	 * BACKWARD SWEEP
     1825             	 */
     1826             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1827             		local_int_t tdgLevelSize = A.tdg[l].size();
     1828             		if((tdgLevelSize%2) == 0) {		
     1829             #ifndef HPCG_NO_OPENMP
     1830             #pragma omp parallel for SCHEDULE(runtime)
     1831             #endif
     1832             		for ( local_int_t i = tdgLevelSize-1; i >= 0; i-= 2 ) {
     1833             			local_int_t row_1 = A.tdg[l][i];
     1834             			local_int_t row_2 = A.tdg[l][i-1];
     1835             			const double * const currentValues_1 = A.matrixValues[row_1];
     1836             			const double * const currentValues_2 = A.matrixValues[row_2];
     1837             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1838             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1839             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1840             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1841             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1842             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1843             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1844             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1845             
     1846             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     1847             							
     1848             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1849             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1850             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1851             				
     1852             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1853             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1854             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1855             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1856             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1857             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1858             
     1859             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1860             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1861             			}
     1862             
     1863             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1864             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1865             			double sum_1 = rv[row_1] - totalContribution_1;
     1866             			double sum_2 = rv[row_2] - totalContribution_2;
     1867             
     1868             			sum_1 += xv[row_1] * currentDiagonal_1;
     1869             			sum_2 += xv[row_2] * currentDiagonal_2;
     1870             			xv[row_1] = sum_1 / currentDiagonal_1;
     1871             			xv[row_2] = sum_2 / currentDiagonal_2;
     1872             		}
     1873             		}
     1874             		else
     1875             		{
     1876             #ifndef HPCG_NO_OPENMP
     1877             #pragma omp parallel for SCHEDULE(runtime)
     1878             #endif
     1879             		for ( local_int_t i = tdgLevelSize-1; i >= 0; i-- ) {
     1880             			local_int_t row = A.tdg[l][i];
     1881             			const double * const currentValues = A.matrixValues[row];
     1882             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1883             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1884             			const double currentDiagonal = matrixDiagonal[row][0];
     1885             			svfloat64_t contribs = svdup_f64(0.0);
     1886             
     1887             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1888             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1889             				
     1890             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1891             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1892             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1893             
     1894             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1895             			}
     1896             
     1897             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1898             			double sum = rv[row] - totalContribution;
     1899             
     1900             			sum += xv[row] * currentDiagonal;
     1901             			xv[row] = sum / currentDiagonal;
     1902             		}
     1903             		}
     1904             	}
     1905             }
     1906             
     1907             inline void SYMGS_VERSION_2(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     1908             	
     1909             	double **matrixDiagonal = A.matrixDiagonal;
     1910             
     1911             	/*
     1912             	 * FORWARD SWEEP
     1913             	 */
     1914             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1915             		local_int_t tdgLevelSize = A.tdg[l].size();
     1916             		local_int_t maxLevelSize = 2*(tdgLevelSize / 2);
     1917             
     1918             #ifndef HPCG_NO_OPENMP
     1919             	#pragma omp parallel
     1920             	{
     1921             	#pragma omp for nowait SCHEDULE(runtime)
     1922             #endif
     1923             		for ( local_int_t i = 0; i < maxLevelSize; i+=2 ) {
     1924             			local_int_t row_1 = A.tdg[l][i];
     1925             			local_int_t row_2 = A.tdg[l][i+1];
     1926             			const double * const currentValues_1 = A.matrixValues[row_1];
     1927             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1928             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1929             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1930             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1931             
     1932             			const double * const currentValues_2 = A.matrixValues[row_2];
     1933             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1934             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1935             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1936             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1937             			
     1938             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     1939             
     1940             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1941             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1942             				
     1943             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1944             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1945             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1946             
     1947             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1948             
     1949             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1950             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1951             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1952             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1953             
     1954             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1955             			}
     1956             
     1957             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1958             			double sum_1 = rv[row_1] - totalContribution_1;
     1959             
     1960             			sum_1 += xv[row_1] * currentDiagonal_1;
     1961             			xv[row_1] = sum_1 / currentDiagonal_1;
     1962             
     1963             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1964             			double sum_2 = rv[row_2] - totalContribution_2;
     1965             
     1966             			sum_2 += xv[row_2] * currentDiagonal_2;
     1967             			xv[row_2] = sum_2 / currentDiagonal_2;
     1968             		}
     1969             
     1970             		#pragma omp single 
     1971             		if (maxLevelSize < tdgLevelSize) {
     1972             			local_int_t i = maxLevelSize;
     1973             
     1974             			local_int_t row = A.tdg[l][i];
     1975             			const double * const currentValues = A.matrixValues[row];
     1976             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1977             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1978             			const double currentDiagonal = matrixDiagonal[row][0];
     1979             			svfloat64_t contribs = svdup_f64(0.0);
     1980             
     1981             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1982             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1983             				
     1984             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1985             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1986             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1987             
     1988             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1989             			}
     1990             
     1991             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1992             			double sum = rv[row] - totalContribution;
     1993             
     1994             			sum += xv[row] * currentDiagonal;
     1995             			xv[row] = sum / currentDiagonal;
     1996             		}
     1997             #ifndef HPCG_NO_OPENMP
     1998             	}
     1999             #endif
     2000             	}
     2001             
     2002             	/*
     2003             	 * BACKWARD SWEEP
     2004             	 */
     2005             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     2006             		local_int_t tdgLevelSize = A.tdg[l].size();
     2007             		local_int_t maxLevelSize = 2*(tdgLevelSize / 2);
     2008             
     2009             #ifndef HPCG_NO_OPENMP
     2010             #pragma omp parallel 
     2011             	{
     2012             		#pragma omp single nowait 
     2013             		{
     2014             #endif
     2015             		if (tdgLevelSize > maxLevelSize) {
     2016             			local_int_t i = maxLevelSize-1;
     2017             
     2018             			local_int_t row = A.tdg[l][i];
     2019             			const double * const currentValues = A.matrixValues[row];
     2020             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2021             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2022             			const double currentDiagonal = matrixDiagonal[row][0];
     2023             			svfloat64_t contribs = svdup_f64(0.0);
     2024             
     2025             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2026             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2027             				
     2028             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2029             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2030             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2031             
     2032             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2033             			}
     2034             
     2035             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2036             			double sum = rv[row] - totalContribution;
     2037             
     2038             			sum += xv[row] * currentDiagonal;
     2039             			xv[row] = sum / currentDiagonal;
     2040             		}
     2041             #ifndef HPCG_NO_OPENMP
     2042             		}
     2043             #pragma omp for SCHEDULE(runtime)
     2044             #endif
     2045             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 2 ) {
     2046             			local_int_t row_1 = A.tdg[l][i];
     2047             			local_int_t row_2 = A.tdg[l][i-1];
     2048             			const double * const currentValues_1 = A.matrixValues[row_1];
     2049             			const double * const currentValues_2 = A.matrixValues[row_2];
     2050             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2051             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2052             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2053             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2054             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2055             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2056             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2057             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2058             
     2059             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2060             							
     2061             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2062             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2063             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2064             				
     2065             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2066             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2067             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2068             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2069             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2070             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2071             
     2072             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2073             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2074             			}
     2075             
     2076             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2077             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2078             			double sum_1 = rv[row_1] - totalContribution_1;
     2079             			double sum_2 = rv[row_2] - totalContribution_2;
     2080             
     2081             			sum_1 += xv[row_1] * currentDiagonal_1;
     2082             			sum_2 += xv[row_2] * currentDiagonal_2;
     2083             			xv[row_1] = sum_1 / currentDiagonal_1;
     2084             			xv[row_2] = sum_2 / currentDiagonal_2;
     2085             		}
     2086             #ifndef HPCG_NO_OPENMP
     2087             	}
     2088             #endif
     2089             	}
     2090             }
     2091             
     2092             inline void SYMGS_VERSION_3(const SparseMatrix& A, double * const& prxv, const double * const& rv) {
     2093             	
     2094             	double **matrixDiagonal = A.matrixDiagonal;
     2095             	double *xv = prxv;
     2096             
     2097             //#pragma statement scache_isolate_way L2=10
     2098             //#pragma statement scache_isolate_assign xv
     2099             #ifndef HPCG_NO_OPENMP
     2100             	#pragma omp parallel
     2101             	{
     2102             		local_int_t numThreads = omp_get_num_threads();
     2103             #endif
     2104             	/*
     2105             	 * FORWARD SWEEP
     2106             	 */
     2107             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     2108             		local_int_t tdgLevelSize = A.tdg[l].size();
     2109             		local_int_t maxLevelSize = 4*(tdgLevelSize / 4);
     2110             
     2111             #ifdef MANUAL_TASK_DISTRIBUTION
     2112             		//at least 8 tasks per thread 
     2113             		local_int_t maxTasksPerThread = std::max((tdgLevelSize+numThreads-1)/numThreads, 8);
     2114             		local_int_t groupedTasksPerThread = ((maxTasksPerThread+7)/8)*8;
     2115             		local_int_t threadId = omp_get_thread_num();
     2116             		local_int_t minValue = groupedTasksPerThread*threadId;
     2117             		local_int_t maxValue = minValue+groupedTasksPerThread;
     2118             		maxLevelSize = std::min(maxValue, maxLevelSize);
     2119             		tdgLevelSize = std::min(maxValue, tdgLevelSize);
     2120             
     2121             		#pragma fj loop zfill			
     2122             		#pragma loop nounroll
     2123             		for ( local_int_t i = minValue; i < maxLevelSize; i+=4 ) {
     2124             #else
     2125             #ifndef HPCG_NO_OPENMP
     2126             	#pragma fj loop zfill			
     2127             	#pragma loop nounroll
     2128             	#pragma omp for nowait SCHEDULE(runtime)
     2129             #endif
     2130             		for ( local_int_t i = 0; i < maxLevelSize; i+=4 ) {
     2131             #endif
     2132             			local_int_t row_1 = A.tdg[l][i];
     2133             			local_int_t row_2 = A.tdg[l][i+1];
     2134             			local_int_t row_3 = A.tdg[l][i+2];
     2135             			local_int_t row_4 = A.tdg[l][i+3];
     2136             			const double * const currentValues_1 = A.matrixValues[row_1];
     2137             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2138             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2139             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2140             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2141             
     2142             			const double * const currentValues_2 = A.matrixValues[row_2];
     2143             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2144             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2145             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2146             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2147             
     2148             			const double * const currentValues_3 = A.matrixValues[row_3];
     2149             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2150             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2151             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2152             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2153             
     2154             			const double * const currentValues_4 = A.matrixValues[row_4];
     2155             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2156             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2157             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2158             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2159             
     2160             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     2161             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);
     2162             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);
     2163             
     2164             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2165             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2166             				
     2167             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2168             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2169             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2170             
     2171             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2172             
     2173             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2174             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2175             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2176             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2177             
     2178             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2179             
     2180             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2181             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2182             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2183             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2184             
     2185             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2186             
     2187             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2188             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2189             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2190             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2191             
     2192             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2193             			}
     2194             
     2195             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2196             			double sum_1 = rv[row_1] - totalContribution_1;
     2197             
     2198             			sum_1 += xv[row_1] * currentDiagonal_1;
     2199             			xv[row_1] = sum_1 / currentDiagonal_1;
     2200             
     2201             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2202             			double sum_2 = rv[row_2] - totalContribution_2;
     2203             
     2204             			sum_2 += xv[row_2] * currentDiagonal_2;
     2205             			xv[row_2] = sum_2 / currentDiagonal_2;
     2206             
     2207             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2208             			double sum_3 = rv[row_3] - totalContribution_3;
     2209             
     2210             			sum_3 += xv[row_3] * currentDiagonal_3;
     2211             			xv[row_3] = sum_3 / currentDiagonal_3;
     2212             
     2213             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2214             			double sum_4 = rv[row_4] - totalContribution_4;
     2215             
     2216             			sum_4 += xv[row_4] * currentDiagonal_4;
     2217             			xv[row_4] = sum_4 / currentDiagonal_4;
     2218             		}
     2219             
     2220             //#pragma omp single
     2221             		if (maxLevelSize < tdgLevelSize) {
     2222             /************
     2223             #ifndef HPCG_NO_OPENMP
     2224             //#pragma loop nounroll
     2225             #pragma omp for SCHEDULE(runtime)
     2226             #endif
     2227             		for ( local_int_t i = maxLevelSize; i < tdgLevelSize; i++ ) {
     2228             
     2229             			local_int_t row = A.tdg[l][i];
     2230             			const double * const currentValues = A.matrixValues[row];
     2231             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2232             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2233             			const double currentDiagonal = matrixDiagonal[row][0];
     2234             			svfloat64_t contribs = svdup_f64(0.0);
     2235             
     2236             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2237             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2238             				
     2239             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2240             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2241             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2242             
     2243             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2244             			}
     2245             
     2246             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2247             			double sum = rv[row] - totalContribution;
     2248             
     2249             			sum += xv[row] * currentDiagonal;
     2250             			xv[row] = sum / currentDiagonal;
     2251             		}
     2252             *******/
     2253             		#pragma omp sections nowait
     2254             		{
     2255             			#pragma fj loop zfill			
     2256             			#pragma omp section 
     2257             			{
     2258             				local_int_t i = maxLevelSize;
     2259             				local_int_t row = A.tdg[l][i];
     2260             				const double * const currentValues = A.matrixValues[row];
     2261             				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2262             				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2263             				const double currentDiagonal = matrixDiagonal[row][0];
     2264             				svfloat64_t contribs = svdup_f64(0.0);
     2265             
     2266             				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2267             					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2268             					
     2269             					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2270             					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2271             					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2272             
     2273             					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2274             				}
     2275             
     2276             				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2277             				double sum = rv[row] - totalContribution;
     2278             
     2279             				sum += xv[row] * currentDiagonal;
     2280             				xv[row] = sum / currentDiagonal;
     2281             			}
     2282             			#pragma fj loop zfill			
     2283             			#pragma omp section 
     2284             			{
     2285             				local_int_t i = maxLevelSize + 1;
     2286             				if (i < tdgLevelSize) {
     2287             				local_int_t row = A.tdg[l][i];
     2288             				const double * const currentValues = A.matrixValues[row];
     2289             				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2290             				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2291             				const double currentDiagonal = matrixDiagonal[row][0];
     2292             				svfloat64_t contribs = svdup_f64(0.0);
     2293             
     2294             				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2295             					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2296             					
     2297             					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2298             					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2299             					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2300             
     2301             					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2302             				}
     2303             
     2304             				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2305             				double sum = rv[row] - totalContribution;
     2306             
     2307             				sum += xv[row] * currentDiagonal;
     2308             				xv[row] = sum / currentDiagonal;
     2309             				}
     2310             			}
     2311             			#pragma fj loop zfill			
     2312             			#pragma omp section 
     2313             			{
     2314             				local_int_t i = maxLevelSize + 2;
     2315             				if (i < tdgLevelSize) {
     2316             				local_int_t row = A.tdg[l][i];
     2317             				const double * const currentValues = A.matrixValues[row];
     2318             				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2319             				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2320             				const double currentDiagonal = matrixDiagonal[row][0];
     2321             				svfloat64_t contribs = svdup_f64(0.0);
     2322             
     2323             				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2324             					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2325             					
     2326             					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2327             					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2328             					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2329             
     2330             					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2331             				}
     2332             
     2333             				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2334             				double sum = rv[row] - totalContribution;
     2335             
     2336             				sum += xv[row] * currentDiagonal;
     2337             				xv[row] = sum / currentDiagonal;
     2338             				}
     2339             			}
     2340             		}
     2341             
     2342             /***********/
     2343             #ifndef HPCG_NO_OPENMP
     2344             	}
     2345             	#pragma omp barrier
     2346             #endif
     2347             	}
     2348             
     2349             	/*
     2350             	 * BACKWARD SWEEP
     2351             	 */
     2352             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     2353             		local_int_t tdgLevelSize = A.tdg[l].size();
     2354             		local_int_t maxLevelSize = 4*(tdgLevelSize / 4);
     2355             
     2356             #ifndef HPCG_NO_OPENMP
     2357             		//#pragma omp single nowait 
     2358             		//{
     2359             		#pragma fj loop zfill			
     2360             		#pragma loop nounroll
     2361             		#pragma omp for nowait SCHEDULE(runtime)
     2362             #endif
     2363             		for ( local_int_t i = tdgLevelSize-1; i >= maxLevelSize; i-- ) {
     2364             			local_int_t row = A.tdg[l][i];
     2365             			const double * const currentValues = A.matrixValues[row];
     2366             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2367             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2368             			const double currentDiagonal = matrixDiagonal[row][0];
     2369             			svfloat64_t contribs = svdup_f64(0.0);
     2370             
     2371             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2372             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2373             				
     2374             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2375             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2376             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2377             
     2378             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2379             			}
     2380             
     2381             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2382             			double sum = rv[row] - totalContribution;
     2383             
     2384             			sum += xv[row] * currentDiagonal;
     2385             			xv[row] = sum / currentDiagonal;
     2386             		}
     2387             
     2388             #ifndef HPCG_NO_OPENMP
     2389             		//}
     2390             #pragma fj loop zfill			
     2391             #pragma loop nounroll
     2392             #pragma omp for SCHEDULE(runtime)
     2393             #endif
     2394             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 4 ) {
     2395             			local_int_t row_1 = A.tdg[l][i];
     2396             			local_int_t row_2 = A.tdg[l][i-1];
     2397             			local_int_t row_3 = A.tdg[l][i-2];
     2398             			local_int_t row_4 = A.tdg[l][i-3];
     2399             			const double * const currentValues_1 = A.matrixValues[row_1];
     2400             			const double * const currentValues_2 = A.matrixValues[row_2];
     2401             			const double * const currentValues_3 = A.matrixValues[row_3];
     2402             			const double * const currentValues_4 = A.matrixValues[row_4];
     2403             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2404             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2405             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2406             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2407             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2408             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2409             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2410             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2411             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2412             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2413             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2414             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2415             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2416             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2417             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2418             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2419             
     2420             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2421             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);				
     2422             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);				
     2423             							
     2424             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2425             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2426             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2427             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2428             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2429             				
     2430             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2431             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2432             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2433             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2434             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2435             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2436             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2437             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2438             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2439             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2440             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2441             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2442             
     2443             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2444             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2445             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2446             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2447             			}
     2448             
     2449             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2450             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2451             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2452             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2453             			double sum_1 = rv[row_1] - totalContribution_1;
     2454             			double sum_2 = rv[row_2] - totalContribution_2;
     2455             			double sum_3 = rv[row_3] - totalContribution_3;
     2456             			double sum_4 = rv[row_4] - totalContribution_4;
     2457             
     2458             			sum_1 += xv[row_1] * currentDiagonal_1;
     2459             			sum_2 += xv[row_2] * currentDiagonal_2;
     2460             			sum_3 += xv[row_3] * currentDiagonal_3;
     2461             			sum_4 += xv[row_4] * currentDiagonal_4;
     2462             			xv[row_1] = sum_1 / currentDiagonal_1;
     2463             			xv[row_2] = sum_2 / currentDiagonal_2;
     2464             			xv[row_3] = sum_3 / currentDiagonal_3;
     2465             			xv[row_4] = sum_4 / currentDiagonal_4;
     2466             		}
     2467             #ifndef HPCG_NO_OPENMP
     2468             	}
     2469             #endif
     2470             	}
     2471             
     2472             //#pragma statement end_scache_isolate_assign
     2473             //#pragma statement end_scache_isolate_way	
     2474             }
     2475             /////////////
     2476             inline void SYMGS_VERSION_4(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     2477             	
     2478             	double **matrixDiagonal = A.matrixDiagonal;
     2479             
     2480             	/*
     2481             	 * FORWARD SWEEP
     2482             	 */
     2483             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     2484             		local_int_t tdgLevelSize = A.tdg[l].size();
     2485             		local_int_t maxLevelSize = 6*(tdgLevelSize / 6);
     2486             
     2487             #ifndef HPCG_NO_OPENMP
     2488             	#pragma omp parallel
     2489             	{
     2490             	#pragma omp for nowait SCHEDULE(runtime)
     2491             #endif
     2492             		for ( local_int_t i = 0; i < maxLevelSize; i+=6 ) {
     2493             			local_int_t row_1 = A.tdg[l][i];
     2494             			local_int_t row_2 = A.tdg[l][i+1];
     2495             			local_int_t row_3 = A.tdg[l][i+2];
     2496             			local_int_t row_4 = A.tdg[l][i+3];
     2497             			local_int_t row_5 = A.tdg[l][i+4];
     2498             			local_int_t row_6 = A.tdg[l][i+5];
     2499             			const double * const currentValues_1 = A.matrixValues[row_1];
     2500             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2501             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2502             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2503             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2504             
     2505             			const double * const currentValues_2 = A.matrixValues[row_2];
     2506             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2507             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2508             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2509             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2510             
     2511             			const double * const currentValues_3 = A.matrixValues[row_3];
     2512             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2513             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2514             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2515             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2516             
     2517             			const double * const currentValues_4 = A.matrixValues[row_4];
     2518             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2519             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2520             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2521             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2522             
     2523             			const double * const currentValues_5 = A.matrixValues[row_5];
     2524             			const local_int_t * const currentColIndices_5 = A.mtxIndL[row_5];
     2525             			const int currentNumberOfNonzeros_5 = A.nonzerosInRow[row_5];
     2526             			const double currentDiagonal_5 = matrixDiagonal[row_5][0];
     2527             			svfloat64_t contribs_5 = svdup_f64(0.0);
     2528             
     2529             			const double * const currentValues_6 = A.matrixValues[row_6];
     2530             			const local_int_t * const currentColIndices_6 = A.mtxIndL[row_6];
     2531             			const int currentNumberOfNonzeros_6 = A.nonzerosInRow[row_6];
     2532             			const double currentDiagonal_6 = matrixDiagonal[row_6][0];
     2533             			svfloat64_t contribs_6 = svdup_f64(0.0);
     2534             
     2535             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     2536             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);
     2537             			const int maxNumberOfNonzeros3 = std::max(currentNumberOfNonzeros_5, currentNumberOfNonzeros_6);
     2538             			const int maxNumberOfNonzeros4 = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);
     2539             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros4, maxNumberOfNonzeros3);
     2540             
     2541             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2542             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);		
     2543             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2544             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2545             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2546             
     2547             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2548             
     2549             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2550             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2551             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2552             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2553             
     2554             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2555             
     2556             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2557             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2558             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2559             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2560             
     2561             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2562             
     2563             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2564             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2565             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2566             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2567             
     2568             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2569             
     2570             				svbool_t pg_5 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_5);
     2571             				svfloat64_t mtxValues_5 = svld1_f64(pg_5, &currentValues_5[j]);
     2572             				svuint64_t indices_5 = svld1sw_u64(pg_5, &currentColIndices_5[j]);
     2573             				svfloat64_t xvv_5 = svld1_gather_u64index_f64(pg_5, xv, indices_5);
     2574             
     2575             				contribs_5 = svmla_f64_m(pg_5, contribs_5, xvv_5, mtxValues_5);
     2576             
     2577             				svbool_t pg_6 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_6);
     2578             				svfloat64_t mtxValues_6 = svld1_f64(pg_6, &currentValues_6[j]);
     2579             				svuint64_t indices_6 = svld1sw_u64(pg_6, &currentColIndices_6[j]);
     2580             				svfloat64_t xvv_6 = svld1_gather_u64index_f64(pg_6, xv, indices_6);
     2581             
     2582             				contribs_6 = svmla_f64_m(pg_6, contribs_6, xvv_6, mtxValues_6);
     2583             			}
     2584             
     2585             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2586             			double sum_1 = rv[row_1] - totalContribution_1;
     2587             
     2588             			sum_1 += xv[row_1] * currentDiagonal_1;
     2589             			xv[row_1] = sum_1 / currentDiagonal_1;
     2590             
     2591             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2592             			double sum_2 = rv[row_2] - totalContribution_2;
     2593             
     2594             			sum_2 += xv[row_2] * currentDiagonal_2;
     2595             			xv[row_2] = sum_2 / currentDiagonal_2;
     2596             
     2597             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2598             			double sum_3 = rv[row_3] - totalContribution_3;
     2599             
     2600             			sum_3 += xv[row_3] * currentDiagonal_3;
     2601             			xv[row_3] = sum_3 / currentDiagonal_3;
     2602             
     2603             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2604             			double sum_4 = rv[row_4] - totalContribution_4;
     2605             
     2606             			sum_4 += xv[row_4] * currentDiagonal_4;
     2607             			xv[row_4] = sum_4 / currentDiagonal_4;
     2608             
     2609             			double totalContribution_5 = svaddv_f64(svptrue_b64(), contribs_5);
     2610             			double sum_5 = rv[row_5] - totalContribution_5;
     2611             
     2612             			sum_5 += xv[row_5] * currentDiagonal_5;
     2613             			xv[row_5] = sum_5 / currentDiagonal_5;
     2614             
     2615             			double totalContribution_6 = svaddv_f64(svptrue_b64(), contribs_6);
     2616             			double sum_6 = rv[row_6] - totalContribution_6;
     2617             
     2618             			sum_6 += xv[row_6] * currentDiagonal_6;
     2619             			xv[row_6] = sum_6 / currentDiagonal_6;
     2620             		}
     2621             
     2622             //#pragma omp single
     2623             		if (maxLevelSize < tdgLevelSize) {
     2624             #ifndef HPCG_NO_OPENMP
     2625             #pragma omp for SCHEDULE(runtime)
     2626             #endif
     2627             		for ( local_int_t i = maxLevelSize; i < tdgLevelSize; i++ ) {
     2628             
     2629             			local_int_t row = A.tdg[l][i];
     2630             			const double * const currentValues = A.matrixValues[row];
     2631             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2632             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2633             			const double currentDiagonal = matrixDiagonal[row][0];
     2634             			svfloat64_t contribs = svdup_f64(0.0);
     2635             
     2636             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2637             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2638             				
     2639             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2640             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2641             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2642             
     2643             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2644             			}
     2645             
     2646             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2647             			double sum = rv[row] - totalContribution;
     2648             
     2649             			sum += xv[row] * currentDiagonal;
     2650             			xv[row] = sum / currentDiagonal;
     2651             		}
     2652             		}
     2653             #ifndef HPCG_NO_OPENMP
     2654             	}
     2655             #endif
     2656             	}
     2657             
     2658             	/*
     2659             	 * BACKWARD SWEEP
     2660             	 */
     2661             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     2662             		local_int_t tdgLevelSize = A.tdg[l].size();
     2663             		local_int_t maxLevelSize = 6*(tdgLevelSize / 6);
     2664             
     2665             #ifndef HPCG_NO_OPENMP
     2666             #pragma omp parallel 
     2667             	{
     2668             		//#pragma omp single nowait 
     2669             		//{
     2670             		#pragma omp for nowait SCHEDULE(runtime)
     2671             #endif
     2672             		for ( local_int_t i = tdgLevelSize-1; i >= maxLevelSize; i-- ) {
     2673             
     2674             			local_int_t row = A.tdg[l][i];
     2675             			const double * const currentValues = A.matrixValues[row];
     2676             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2677             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2678             			const double currentDiagonal = matrixDiagonal[row][0];
     2679             			svfloat64_t contribs = svdup_f64(0.0);
     2680             
     2681             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2682             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2683             				
     2684             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2685             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2686             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2687             
     2688             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2689             			}
     2690             
     2691             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2692             			double sum = rv[row] - totalContribution;
     2693             
     2694             			sum += xv[row] * currentDiagonal;
     2695             			xv[row] = sum / currentDiagonal;
     2696             		}
     2697             #ifndef HPCG_NO_OPENMP
     2698             		//}
     2699             #pragma omp for SCHEDULE(runtime)
     2700             #endif
     2701             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 6 ) {
     2702             			local_int_t row_1 = A.tdg[l][i];
     2703             			local_int_t row_2 = A.tdg[l][i-1];
     2704             			local_int_t row_3 = A.tdg[l][i-2];
     2705             			local_int_t row_4 = A.tdg[l][i-3];
     2706             			local_int_t row_5 = A.tdg[l][i-4];
     2707             			local_int_t row_6 = A.tdg[l][i-5];
     2708             			const double * const currentValues_1 = A.matrixValues[row_1];
     2709             			const double * const currentValues_2 = A.matrixValues[row_2];
     2710             			const double * const currentValues_3 = A.matrixValues[row_3];
     2711             			const double * const currentValues_4 = A.matrixValues[row_4];
     2712             			const double * const currentValues_5 = A.matrixValues[row_5];
     2713             			const double * const currentValues_6 = A.matrixValues[row_6];
     2714             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2715             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2716             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2717             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2718             			const local_int_t * const currentColIndices_5 = A.mtxIndL[row_5];
     2719             			const local_int_t * const currentColIndices_6 = A.mtxIndL[row_6];
     2720             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2721             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2722             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2723             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2724             			const int currentNumberOfNonzeros_5 = A.nonzerosInRow[row_5];
     2725             			const int currentNumberOfNonzeros_6 = A.nonzerosInRow[row_6];
     2726             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2727             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2728             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2729             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2730             			const double currentDiagonal_5 = matrixDiagonal[row_5][0];
     2731             			const double currentDiagonal_6 = matrixDiagonal[row_6][0];
     2732             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2733             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2734             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2735             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2736             			svfloat64_t contribs_5 = svdup_f64(0.0);
     2737             			svfloat64_t contribs_6 = svdup_f64(0.0);
     2738             
     2739             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2740             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);				
     2741             			const int maxNumberOfNonzeros3 = std::max(currentNumberOfNonzeros_5, currentNumberOfNonzeros_6);				
     2742             			const int maxNumberOfNonzeros4 = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);				
     2743             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros3, maxNumberOfNonzeros4);				
     2744             							
     2745             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2746             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2747             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2748             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2749             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2750             				svbool_t pg_5 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_5);
     2751             				svbool_t pg_6 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_6);
     2752             				
     2753             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2754             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2755             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2756             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2757             				svfloat64_t mtxValues_5 = svld1_f64(pg_5, &currentValues_5[j]);
     2758             				svfloat64_t mtxValues_6 = svld1_f64(pg_6, &currentValues_6[j]);
     2759             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2760             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2761             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2762             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2763             				svuint64_t indices_5 = svld1sw_u64(pg_5, &currentColIndices_5[j]);
     2764             				svuint64_t indices_6 = svld1sw_u64(pg_6, &currentColIndices_6[j]);
     2765             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2766             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2767             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2768             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2769             				svfloat64_t xvv_5 = svld1_gather_u64index_f64(pg_5, xv, indices_5);
     2770             				svfloat64_t xvv_6 = svld1_gather_u64index_f64(pg_6, xv, indices_6);
     2771             
     2772             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2773             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2774             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2775             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2776             				contribs_5 = svmla_f64_m(pg_5, contribs_5, xvv_5, mtxValues_5);
     2777             				contribs_6 = svmla_f64_m(pg_6, contribs_6, xvv_6, mtxValues_6);
     2778             			}
     2779             
     2780             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2781             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2782             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2783             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2784             			double totalContribution_5 = svaddv_f64(svptrue_b64(), contribs_5);
     2785             			double totalContribution_6 = svaddv_f64(svptrue_b64(), contribs_6);
     2786             			double sum_1 = rv[row_1] - totalContribution_1;
     2787             			double sum_2 = rv[row_2] - totalContribution_2;
     2788             			double sum_3 = rv[row_3] - totalContribution_3;
     2789             			double sum_4 = rv[row_4] - totalContribution_4;
     2790             			double sum_5 = rv[row_5] - totalContribution_5;
     2791             			double sum_6 = rv[row_6] - totalContribution_6;
     2792             
     2793             			sum_1 += xv[row_1] * currentDiagonal_1;
     2794             			sum_2 += xv[row_2] * currentDiagonal_2;
     2795             			sum_3 += xv[row_3] * currentDiagonal_3;
     2796             			sum_4 += xv[row_4] * currentDiagonal_4;
     2797             			sum_5 += xv[row_5] * currentDiagonal_5;
     2798             			sum_6 += xv[row_6] * currentDiagonal_6;
     2799             			xv[row_1] = sum_1 / currentDiagonal_1;
     2800             			xv[row_2] = sum_2 / currentDiagonal_2;
     2801             			xv[row_3] = sum_3 / currentDiagonal_3;
     2802             			xv[row_4] = sum_4 / currentDiagonal_4;
     2803             			xv[row_5] = sum_5 / currentDiagonal_5;
     2804             			xv[row_6] = sum_6 / currentDiagonal_6;
     2805             		}
     2806             #ifndef HPCG_NO_OPENMP
     2807             	}
     2808             #endif
     2809             	}
     2810             }
Total prefetch num: 0
Optimization messages
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 20: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 21: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 21: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 48: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 48: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 49: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 49: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 50: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 50: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 51: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 51: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 76: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiEERKT_S3_S3_'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 77: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiEERKT_S3_S3_'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 78: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiEERKT_S3_S3_'.
  jwd6142s-i  "../src/ComputeSYMGS_OPT.cpp", line 80: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS_OPT.cpp", line 80: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 144: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 144: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS_OPT.cpp", line 151: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS_OPT.cpp", line 151: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 172: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 172: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS_OPT.cpp", line 179: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS_OPT.cpp", line 179: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 201: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 201: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS_OPT.cpp", line 208: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS_OPT.cpp", line 208: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 231: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 233: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 243: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 247: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 247: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 248: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS_OPT.cpp", line 248: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS_OPT.cpp", line 255: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 310: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 314: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 314: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 315: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 315: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 322: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 322: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 337: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 337: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 338: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 343: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 347: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 347: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 348: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 348: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 355: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 355: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 397: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 406: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 407: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 420: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 443: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 443: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 499: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 499: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 535: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 535: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 560: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 566: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 567: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 581: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 604: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 604: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 660: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 660: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 696: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 696: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1392: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1396: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1396: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1397: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1397: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1404: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1404: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1404: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1406: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1410: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1410: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1411: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1416: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1420: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1420: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1421: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1421: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1428: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1428: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1428: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1430: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1465: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1470: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1470: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1471: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1471: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1478: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1478: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1478: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1480: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1484: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1484: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1485: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1490: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1495: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1495: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1496: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1496: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1503: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1503: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1503: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1505: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1535: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1541: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1542: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1564: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1564: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1564: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1569: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1582: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1582: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1582: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1585: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1593: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1593: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1593: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1595: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1604: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1610: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1611: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1633: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1633: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1633: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1633: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 32.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1654: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1654: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1654: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1654: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 96.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1655: Method of calculating sum or product is changed.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1656: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1667: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1667: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1667: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1667: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1668: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1717: Inline expansion is applied to the user defined function '_Z20ComputeSYMGS_TDG_SVERK19SparseMatrix_STRUCTRK13Vector_STRUCTRS2_R9TraceData'.
  jwd8101o-i  "/opt/FJSVstclanga/cp-1.0.21.01/bin/../include/libc++/v371/algorithm", line 2659: Inline expansion is applied to the user defined function '_ZNKSt3__16__lessIiiEclERKiS3_'.
  jwd8101o-i  "/opt/FJSVstclanga/cp-1.0.21.01/bin/../include/libc++/v371/algorithm", line 2667: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiNS_6__lessIiiEEEERKT_S5_S5_T0_'.
Statistics information
  Option information
    Command line options : -c -DHPCG_CONTIGUOUS_ARRAYS -DHPCG_NO_MPI -DENABLE_MG_COUNTERS -DHPCG_USE_SVE -DHPCG_MAN_OPT_DDOT -DDDOT_2_UNROLL -DWAXPBY_AUTO_OPT -HPCG_MAN_OPT_SCHEDULE_ON -DHPCG_MAN_OPT_SPMV_UNROLL -DSPMV_4_UNROLL -Khpctag -Kzfill -DUNROLLING_4_B -I./src -I./src/OOKAMI_OMP_FJ -Kfast -KSVE -Kopenmp -ffast-math -funroll-loops -std=c++11 -ffp-contract=fast -march=armv8.2-a+sve -Kocl -Koptmsg=2 -Nlst=t -I../src -o src/ComputeSYMGS.o
    Effective options    : -g0 -mt -Qy -std=gnu++11 -x- -x=quick -O3 -Knoalias_const
                           -Kalign_loops -Knoarray_declaration_opt -Kassume=noshortloop
                           -Kassume=nomemory_bandwidth -Kassume=notime_saving_compilation
                           -Kcmodel=small -Keval -Keval_noconcurrent
                           -Knoextract_stride_store -Kfast_matmul -Knofenv_access
                           -Kfp_contract -Kfp_relaxed -Kfsimple -Kfz -Khpctag
                           -Kilfunc=procedure -Klargepage -Klib -Kloop_blocking
                           -Kloop_fission -Kloop_nofission_stripmining
                           -Kloop_fission_threshold=50 -Kloop_fusion -Kloop_interchange
                           -Kloop_part_simd -Kloop_perfect_nest -Kloop_noversioning
                           -Klooptype=f -Knomemalias -Kmfunc=1 -Kocl -Komitfp -Kopenmp
                           -Kopenmp_noassume_norecurrence
                           -Kopenmp_nocollapse_except_innermost
                           -Kopenmp_loop_variable=private -Kopenmp_noordered_reduction
                           -Knoopenmp_simd -Knooptlib_string -Koptmsg=2
                           -Knopc_relative_literal_loads -Knoparallel
                           -Kparallel_nofp_precision -Knopreex -Kprefetch_cache_level=all
                           -Kprefetch_noconditional -Kprefetch_noindirect -Kprefetch_noinfer
                           -Kprefetch_sequential=auto -Kprefetch_nostride -Kprefetch_strong
                           -Kprefetch_strong_L2 -Knopreload -Krdconv=1
                           -Kremove_inlinefunction -Knorestp -Ksch_post_ra -Ksch_pre_ra
                           -Ksibling_calls -Ksimd=auto -Ksimd_packed_promotion
                           -Ksimd_reduction_product -Ksimd_reg_size=512
                           -Ksimd_nouncounted_loop -Ksimd_use_multiple_structures
                           -Knostrict_aliasing -Knostriping -KA64FX -KARMV8_2_A -KSVE -Kswp
                           -Kswp_freg_rate=100 -Kswp_ireg_rate=100 -Kswp_preg_rate=100
                           -Kswp_policy=auto -Kunroll -Knounroll_and_jam -Kzfill
                           -Ncancel_overtime_compilation -Nnocoverage -Nexceptions -Nnofjcex
                           -Nfjprof -Nnohook_func -Nnohook_time -Nlibomp -Nline -Nlst=p
                           -Nlst=t -Nquickdbg=noheapchk -Nquickdbg=nosubchk -NRnotrap
                           -Nnoreordered_variable_stack -Nrt_notune -Nsetvalue=noheap
                           -Nsetvalue=nostack -Nsetvalue=noscalar -Nsetvalue=noarray
                           -Nsetvalue=nostruct -Nsrc -Nsta
