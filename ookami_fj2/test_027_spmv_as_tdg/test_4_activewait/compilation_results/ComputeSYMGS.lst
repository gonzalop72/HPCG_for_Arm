Fujitsu C/C++ Version 4.7.0   Mon Dec 19 10:32:45 2022
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj2
  Source file       : ../src/ComputeSYMGS.cpp
(line-no.)(optimize)
        1             /*
        2              *
        3              *  SPDX-License-Identifier: Apache-2.0
        4              *
        5              *  Copyright (C) 2019, Arm Limited and contributors
        6              *
        7              *  Licensed under the Apache License, Version 2.0 (the "License");
        8              *  you may not use this file except in compliance with the License.
        9              *  You may obtain a copy of the License at
       10              *
       11              *      http://www.apache.org/licenses/LICENSE-2.0
       12              *
       13              *  Unless required by applicable law or agreed to in writing, software
       14              *  distributed under the License is distributed on an "AS IS" BASIS,
       15              *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       16              *  See the License for the specific language governing permissions and
       17              *  limitations under the License.
       18              *
       19              */
       20             
       21             //@HEADER
       22             // ***************************************************
       23             //
       24             // HPCG: High Performance Conjugate Gradient Benchmark
       25             //
       26             // Contact:
       27             // Michael A. Heroux ( maherou@sandia.gov)
       28             // Jack Dongarra     (dongarra@eecs.utk.edu)
       29             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       30             //
       31             // ***************************************************
       32             //@HEADER
       33             
       34             /*!
       35              @file ComputeSYMGS.cpp
       36             
       37              HPCG routine
       38              */
       39             
       40             #include "ComputeSYMGS.hpp"
       41             #include "ComputeSYMGS_ref.hpp"
       42             #ifndef HPCG_NO_MPI
       43             #include "ExchangeHalo.hpp"
       44             #endif
       45             
       46             #include "likwid_instrumentation.hpp"
       47             
       48             #ifdef HPCG_MAN_OPT_SCHEDULE_ON
       49             	#define SCHEDULE(T)	schedule(T)
       50             #else
       51             	#define SCHEDULE(T)
       52             #endif
       53             
       54             /**************************************************************************************************/
       55             /**************************************************************************************************/
       56             /**************************************************************************************************/
       57             /* SVE IMPLEMENTATIONS                                                                            */
       58             /**************************************************************************************************/
       59             /**************************************************************************************************/
       60             /**************************************************************************************************/
       61             
       62             #include "arm_sve.h"
       63             #ifdef HPCG_USE_SVE
       64             #include "arm_sve.h"
       65             
       66             inline void SYMGS_VERSION_1(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-2
       67             inline void SYMGS_VERSION_2(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-2 V2
       68             inline void SYMGS_VERSION_3(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-4 - OPTIMUM
       69             inline void SYMGS_VERSION_4(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-6
       70             
       71             /*
       72              * TDG VERSION
       73              */
       74             int ComputeSYMGS_TDG_SVE(const SparseMatrix & A, const Vector & r, Vector & x, TraceData &trace) {
       75             	assert(x.localLength == A.localNumberOfColumns);
       76             
       77             #ifndef HPCG_NO_MPI
       78             	ExchangeHalo(A, x);
       79             #endif
       80             
       81             	const double * const rv = r.values;
       82             	double * const xv = x.values;
       83             	double **matrixDiagonal = A.matrixDiagonal;
       84             
       85             LIKWID_START(trace.enabled, "symgs_tdg");
       86             
       87             #ifndef TEST_XX
       88             SYMGS_VERSION_3(A, xv, rv);
       89             #else
       90             
       91             //#pragma statement scache_isolate_way L2=10
       92             //#pragma statement scache_isolate_assign xv
       93             	/*
       94             	 * FORWARD SWEEP
       95             	 */
       96             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
       97             
       98             		local_int_t totalSize = A.tdg[l].size();
       99             		local_int_t size1 = 2*(totalSize/2);
      100             		//#pragma loop nounroll
      101             		//#pragma loop nounroll_and_jam
      102             		//if((A.tdg[l].size()%2) == 0) {
      103             #ifndef HPCG_NO_OPENMP
      104             #pragma omp parallel
      105             {
      106             #pragma omp for nowait SCHEDULE(runtime)
      107             #endif
      108             		for ( local_int_t i = 0; i < size1; i+=2 ) {
      109             			local_int_t row_1 = A.tdg[l][i];
      110             			local_int_t row_2 = A.tdg[l][i+1];
      111             			const double * const currentValues_1 = A.matrixValues[row_1];
      112             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
      113             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
      114             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
      115             			svfloat64_t contribs_1 = svdup_f64(0.0);
      116             
      117             			const double * const currentValues_2 = A.matrixValues[row_2];
      118             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
      119             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
      120             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
      121             			svfloat64_t contribs_2 = svdup_f64(0.0);
      122             			
      123             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
      124             
      125             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
      126             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
      127             				
      128             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
      129             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
      130             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
      131             
      132             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
      133             
      134             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
      135             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
      136             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
      137             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
      138             
      139             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
      140             			}
      141             
      142             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
      143             			double sum_1 = rv[row_1] - totalContribution_1;
      144             
      145             			sum_1 += xv[row_1] * currentDiagonal_1;
      146             			xv[row_1] = sum_1 / currentDiagonal_1;
      147             
      148             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
      149             			double sum_2 = rv[row_2] - totalContribution_2;
      150             
      151             			sum_2 += xv[row_2] * currentDiagonal_2;
      152             			xv[row_2] = sum_2 / currentDiagonal_2;
      153             		}
      154             		//}
      155             		//else
      156             		//{
      157             #ifndef HPCG_NO_OPENMP
      158             //#pragma omp parallel for SCHEDULE(runtime)
      159             #pragma omp single 
      160             {
      161             #endif
      162             		if (size1 < totalSize) {
      163             			local_int_t i = size1;
      164             		//for ( local_int_t i = size1; i < totalSize; i++ ) {
      165             			local_int_t row = A.tdg[l][i];
      166             			const double * const currentValues = A.matrixValues[row];
      167             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      168             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      169             			const double currentDiagonal = matrixDiagonal[row][0];
      170             			svfloat64_t contribs = svdup_f64(0.0);
      171             
      172             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      173             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      174             				
      175             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      176             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      177             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      178             
      179             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      180             			}
      181             
      182             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      183             			double sum = rv[row] - totalContribution;
      184             
      185             			sum += xv[row] * currentDiagonal;
      186             			xv[row] = sum / currentDiagonal;
      187             		//}
      188             		}
      189             #ifndef HPCG_NO_OPENMP
      190             }
      191             }
      192             #endif
      193             	}
      194             
      195             	/*
      196             	 * BACKWARD SWEEP
      197             	 */
      198             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      199             #ifndef HPCG_NO_OPENMP
      200             #pragma omp parallel for SCHEDULE(runtime)
      201             #endif
      202             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      203             			local_int_t row = A.tdg[l][i];
      204             			const double * const currentValues = A.matrixValues[row];
      205             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      206             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      207             			const double currentDiagonal = matrixDiagonal[row][0];
      208             			svfloat64_t contribs = svdup_f64(0.0);
      209             
      210             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      211             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      212             				
      213             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      214             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      215             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      216             
      217             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      218             			}
      219             
      220             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      221             			double sum = rv[row] - totalContribution;
      222             
      223             			sum += xv[row] * currentDiagonal;
      224             			xv[row] = sum / currentDiagonal;
      225             		}
      226             
      227             /*#ifndef HPCG_NO_OPENMP
      228             #pragma omp parallel for SCHEDULE(runtime)
      229             #endif
      230             		for ( local_int_t i = size1-1; i >= 0; i-= 2 ) {
      231             			local_int_t row_1 = A.tdg[l][i];
      232             			local_int_t row_2 = A.tdg[l][i-1];
      233             			const double * const currentValues_1 = A.matrixValues[row_1];
      234             			const double * const currentValues_2 = A.matrixValues[row_2];
      235             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
      236             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
      237             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
      238             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
      239             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
      240             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
      241             			svfloat64_t contribs_1 = svdup_f64(0.0);
      242             			svfloat64_t contribs_2 = svdup_f64(0.0);
      243             
      244             			//#pragma loop nounroll
      245             			//#pragma loop nounroll_and_jam
      246             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      247             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      248             				
      249             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      250             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      251             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      252             
      253             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      254             			}
      255             
      256             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      257             			double sum = rv[row] - totalContribution;
      258             
      259             			sum += xv[row] * currentDiagonal;
      260             			xv[row] = sum / currentDiagonal;
      261             		}*/
      262             	}
      263             //#pragma statement end_scache_isolate_assign
      264             //#pragma statement end_scache_isolate_way
      265             
      266             #endif //TEST_XX
      267             
      268             LIKWID_STOP(trace.enabled, "symgs_tdg");
      269             
      270             	return 0;
      271             }
      272             /*
      273              * END OF TDG VERSION
      274              */
      275             
      276             /*
      277              * TDG FUSED SYMGS-SPMV VERSION
      278              */
      279             int ComputeFusedSYMGS_SPMV_SVE(const SparseMatrix & A, const Vector & r, Vector & x, Vector & y, TraceData& trace) {
      280             	assert(x.localLength == A.localNumberOfColumns);
      281             
      282             #ifndef HPCG_NO_MPI
      283             	ExchangeHalo(A, x);
      284             #endif
      285             
      286             	const double * const rv = r.values;
      287             	double * const xv = x.values;
      288             	double **matrixDiagonal = A.matrixDiagonal;
      289             	double * const yv = y.values;
      290             
      291             	/*
      292             	 * FORWARD SWEEP
      293             	 */
      294             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      295             #ifndef HPCG_NO_OPENMP
      296             #pragma omp parallel for SCHEDULE(runtime)
      297             #endif
      298             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      299             			local_int_t row = A.tdg[l][i];
      300             			const double * const currentValues = A.matrixValues[row];
      301             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      302             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      303             			const double currentDiagonal = matrixDiagonal[row][0];
      304             			svfloat64_t contribs = svdup_f64(0.0);
      305             
      306             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      307             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      308             				
      309             				svfloat64_t mtxValues = svld1(pg, &currentValues[j]);
      310             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      311             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      312             
      313             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      314             			}
      315             
      316             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      317             			double sum = rv[row] - totalContribution;
      318             
      319             			sum += xv[row] * currentDiagonal;
      320             			xv[row] = sum / currentDiagonal;
      321             		}
      322             	}
      323             
      324             	/*
      325             	 * BACKWARD SWEEP
      326             	 */
      327             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      328             #ifndef HPCG_NO_OPENMP
      329             #pragma omp parallel for SCHEDULE(runtime)
      330             #endif
      331             		for ( local_int_t i = A.tdg[l].size(); i >= 0; i-- ) {
      332             			local_int_t row = A.tdg[l][i];
      333             			const double * const currentValues = A.matrixValues[row];
      334             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      335             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      336             			const double currentDiagonal = matrixDiagonal[row][0];
      337             			svfloat64_t contribs = svdup_f64(0.0);
      338             
      339             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      340             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      341             				
      342             				svfloat64_t mtxValues = svld1(pg, &currentValues[j]);
      343             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      344             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      345             
      346             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      347             			}
      348             
      349             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      350             			totalContribution -= xv[row] * currentDiagonal; // remove diagonal contribution
      351             			double sum = rv[row] - totalContribution; // substract contributions from RHS
      352             			xv[row] = sum / currentDiagonal; // update row
      353             
      354             			// SPMV part
      355             			totalContribution += xv[row] * currentDiagonal; // add updated diagonal contribution
      356             			yv[row] = totalContribution; // update SPMV output vector
      357             			
      358             		}
      359             	}
      360             
      361             	return 0;
      362             }
      363             /*
      364              * END OF TDG FUSED SYMGS-SPMV VERSION
      365              */
      366             
      367             /*
      368              * BLOCK COLORED VERSION
      369              */
      370             int ComputeSYMGS_BLOCK_SVE(const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
      371             	assert(x.localLength >= A.localNumberOfColumns);
      372             
      373             #ifndef HPCG_NO_MPI
      374             	ExchangeHalo(A, x);
      375             #endif
      376             
      377             	double **matrixDiagonal = A.matrixDiagonal;
      378             	const double * const rv = r.values;
      379             	double * const xv = x.values;
      380             	local_int_t firstBlock = 0;
      381             	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
      382             
      383             LIKWID_START(trace.enabled, "symgs_bc");		
      384             
      385             	/*
      386             	 * FORWARD SWEEP
      387             	 */
      388             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) { // for each color
      389             		if ( color > 0 ) {
      390             			firstBlock += A.numberOfBlocksInColor[color-1];
      391             			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
      392             		}
      393             #ifndef HPCG_NO_OPENMP
      394             #pragma omp parallel for SCHEDULE(runtime)
      395             #endif
      396             		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) { // for each superblock with the same color
      397             			local_int_t firstRow = block * A.blockSize;
      398             			local_int_t firstChunk = firstRow / A.chunkSize;
      399             			local_int_t lastChunk = (firstRow + A.blockSize * A.chunkSize) / A.chunkSize;
      400             
      401             			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) { // for each chunk of this super block
      402             				local_int_t first = A.chunkSize * chunk;
      403             				local_int_t last = first + A.chunkSize;
      404             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      405             				local_int_t i = first;
      406             				if ( A.chunkSize == 4 ) {
      407             					const double * const currentValues0 = A.matrixValues[i  ];
      408             					const double * const currentValues1 = A.matrixValues[i+1];
      409             					const double * const currentValues2 = A.matrixValues[i+2];
      410             					const double * const currentValues3 = A.matrixValues[i+3];
      411             
      412             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      413             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      414             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      415             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      416             
      417             					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      418             					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      419             					const double currentDiagonal2 = matrixDiagonal[i+2][0];
      420             					const double currentDiagonal3 = matrixDiagonal[i+3][0];
      421             
      422             					svfloat64_t contribs0 = svdup_f64(0.0);
      423             					svfloat64_t contribs1 = svdup_f64(0.0);
      424             					svfloat64_t contribs2 = svdup_f64(0.0);
      425             					svfloat64_t contribs3 = svdup_f64(0.0);
      426             
      427             					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      428             						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      429             
      430             						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      431             						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      432             						svfloat64_t values2 = svld1_f64(pg, &currentValues2[j]);
      433             						svfloat64_t values3 = svld1_f64(pg, &currentValues3[j]);
      434             
      435             						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      436             						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      437             						svuint64_t indices2 = svld1sw_u64(pg, &currentColIndices2[j]);
      438             						svuint64_t indices3 = svld1sw_u64(pg, &currentColIndices3[j]);
      439             
      440             						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      441             						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      442             						svfloat64_t xvv2 = svld1_gather_u64index_f64(pg, xv, indices2);
      443             						svfloat64_t xvv3 = svld1_gather_u64index_f64(pg, xv, indices3);
      444             
      445             						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      446             						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1);
      447             						contribs2 = svmla_f64_m(pg, contribs2, xvv2, values2);
      448             						contribs3 = svmla_f64_m(pg, contribs3, xvv3, values3);
      449             					}
      450             
      451             					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      452             					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      453             					double totalContribution2 = svaddv_f64(svptrue_b64(), contribs2);
      454             					double totalContribution3 = svaddv_f64(svptrue_b64(), contribs3);
      455             
      456             					double sum0 = rv[i  ] - totalContribution0;
      457             					double sum1 = rv[i+1] - totalContribution1;
      458             					double sum2 = rv[i+2] - totalContribution2;
      459             					double sum3 = rv[i+3] - totalContribution3;
      460             
      461             					sum0 += xv[i  ] * currentDiagonal0;
      462             					sum1 += xv[i+1] * currentDiagonal1;
      463             					sum2 += xv[i+2] * currentDiagonal2;
      464             					sum3 += xv[i+3] * currentDiagonal3;
      465             
      466             					xv[i  ] = sum0 / currentDiagonal0;
      467             					xv[i+1] = sum1 / currentDiagonal1;
      468             					xv[i+2] = sum2 / currentDiagonal2;
      469             					xv[i+3] = sum3 / currentDiagonal3;
      470             				} else if ( A.chunkSize == 2 ) {
      471             					const double * const currentValues0 = A.matrixValues[i  ];
      472             					const double * const currentValues1 = A.matrixValues[i+1];
      473             
      474             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      475             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      476             
      477             					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      478             					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      479             
      480             					svfloat64_t contribs0 = svdup_f64(0.0);
      481             					svfloat64_t contribs1 = svdup_f64(0.0);
      482             
      483             					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      484             						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      485             
      486             						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      487             						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      488             
      489             						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      490             						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      491             
      492             						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      493             						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      494             
      495             						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      496             						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1);
      497             					}
      498             
      499             					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      500             					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      501             
      502             					double sum0 = rv[i  ] - totalContribution0;
      503             					double sum1 = rv[i+1] - totalContribution1;
      504             
      505             					sum0 += xv[i  ] * currentDiagonal0;
      506             					sum1 += xv[i+1] * currentDiagonal1;
      507             
      508             					xv[i  ] = sum0 / currentDiagonal0;
      509             					xv[i+1] = sum1 / currentDiagonal1;
      510             				} else { //A.chunkSize == 1
      511             					const double * const currentValues0 = A.matrixValues[i  ];
      512             
      513             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      514             
      515             					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      516             
      517             					svfloat64_t contribs0 = svdup_f64(0.0);
      518             
      519             					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      520             						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      521             
      522             						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      523             
      524             						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      525             
      526             						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      527             
      528             						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      529             					}
      530             
      531             					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      532             
      533             					double sum0 = rv[i  ] - totalContribution0;
      534             
      535             					sum0 += xv[i  ] * currentDiagonal0;
      536             
      537             					xv[i  ] = sum0 / currentDiagonal0;
      538             				}
      539             			}
      540             		}
      541             	}
      542             
      543             	firstBlock = A.numberOfBlocks-1;
      544             	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
      545             	/*
      546             	 * BACKWARD SWEEP
      547             	 */
      548             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
      549             		if ( color < A.numberOfColors-1 ) {
      550             			firstBlock -= A.numberOfBlocksInColor[color+1];
      551             			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
      552             		}
      553             #ifndef HPCG_NO_OPENMP
      554             #pragma omp parallel for SCHEDULE(runtime)
      555             #endif
      556             		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) {
      557             			local_int_t firstRow = ((block+1) * A.blockSize) - 1;
      558             			local_int_t firstChunk = firstRow / A.chunkSize;
      559             			local_int_t lastChunk = (firstRow - A.blockSize * A.chunkSize) / A.chunkSize;
      560             
      561             			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
      562             				local_int_t first = A.chunkSize * chunk;
      563             				local_int_t last = first + A.chunkSize;
      564             
      565             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      566             				local_int_t i = first;
      567             				if ( A.chunkSize == 4 ) {
      568             					const double * const currentValues3 = A.matrixValues[i+3];
      569             					const double * const currentValues2 = A.matrixValues[i+2];
      570             					const double * const currentValues1 = A.matrixValues[i+1];
      571             					const double * const currentValues0 = A.matrixValues[i  ];
      572             
      573             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      574             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      575             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      576             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      577             
      578             					const double currentDiagonal3 = matrixDiagonal[i+3][0];
      579             					const double currentDiagonal2 = matrixDiagonal[i+2][0];
      580             					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      581             					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      582             
      583             					svfloat64_t contribs3 = svdup_f64(0.0);
      584             					svfloat64_t contribs2 = svdup_f64(0.0);
      585             					svfloat64_t contribs1 = svdup_f64(0.0);
      586             					svfloat64_t contribs0 = svdup_f64(0.0);
      587             
      588             					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      589             						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      590             
      591             						svfloat64_t values3 = svld1_f64(pg, &currentValues3[j]);
      592             						svfloat64_t values2 = svld1_f64(pg, &currentValues2[j]);
      593             						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      594             						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      595             
      596             						svuint64_t indices3 = svld1sw_u64(pg, &currentColIndices3[j]);
      597             						svuint64_t indices2 = svld1sw_u64(pg, &currentColIndices2[j]);
      598             						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      599             						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      600             
      601             						svfloat64_t xvv3 = svld1_gather_u64index_f64(pg, xv, indices3);
      602             						svfloat64_t xvv2 = svld1_gather_u64index_f64(pg, xv, indices2);
      603             						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      604             						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      605             
      606             						contribs3 = svmla_f64_m(pg, contribs3, xvv3, values3 );
      607             						contribs2 = svmla_f64_m(pg, contribs2, xvv2, values2 );
      608             						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1 );
      609             						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      610             					}
      611             
      612             					double totalContribution3 = svaddv_f64(svptrue_b64(), contribs3);
      613             					double totalContribution2 = svaddv_f64(svptrue_b64(), contribs2);
      614             					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      615             					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      616             
      617             					double sum3 = rv[i+3] - totalContribution3;
      618             					double sum2 = rv[i+2] - totalContribution2;
      619             					double sum1 = rv[i+1] - totalContribution1;
      620             					double sum0 = rv[i  ] - totalContribution0;
      621             
      622             					sum3 += xv[i+3] * currentDiagonal3;
      623             					sum2 += xv[i+2] * currentDiagonal2;
      624             					sum1 += xv[i+1] * currentDiagonal1;
      625             					sum0 += xv[i  ] * currentDiagonal0;
      626             					
      627             					xv[i+3] = sum3 / currentDiagonal3;
      628             					xv[i+2] = sum2 / currentDiagonal2;
      629             					xv[i+1] = sum1 / currentDiagonal1;
      630             					xv[i  ] = sum0 / currentDiagonal0;
      631             				} else if ( A.chunkSize == 2 ) {
      632             					const double * const currentValues1 = A.matrixValues[i+1];
      633             					const double * const currentValues0 = A.matrixValues[i  ];
      634             
      635             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      636             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      637             
      638             					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      639             					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      640             
      641             					svfloat64_t contribs1 = svdup_f64(0.0);
      642             					svfloat64_t contribs0 = svdup_f64(0.0);
      643             
      644             					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      645             						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      646             
      647             						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      648             						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      649             
      650             						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      651             						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      652             
      653             						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      654             						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      655             
      656             						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1 );
      657             						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      658             					}
      659             
      660             					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      661             					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      662             
      663             					double sum1 = rv[i+1] - totalContribution1;
      664             					double sum0 = rv[i  ] - totalContribution0;
      665             
      666             					sum1 += xv[i+1] * currentDiagonal1;
      667             					sum0 += xv[i  ] * currentDiagonal0;
      668             					
      669             					xv[i+1] = sum1 / currentDiagonal1;
      670             					xv[i  ] = sum0 / currentDiagonal0;
      671             				} else { // A.chunkSize == 1
      672             					const double * const currentValues0 = A.matrixValues[i  ];
      673             
      674             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      675             
      676             					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      677             
      678             					svfloat64_t contribs0 = svdup_f64(0.0);
      679             
      680             					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      681             						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      682             
      683             						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      684             
      685             						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      686             
      687             						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      688             
      689             						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      690             					}
      691             
      692             					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      693             
      694             					double sum0 = rv[i  ] - totalContribution0;
      695             
      696             					sum0 += xv[i  ] * currentDiagonal0;
      697             					
      698             				}
      699             			}
      700             		}
      701             	}
      702             LIKWID_STOP(trace.enabled, "symgs_bc");			
      703             
      704             	return 0;
      705             }
      706             /*
      707              * END OF BLOCK COLORED VERSION
      708              */
      709             #elif defined(HPCG_USE_NEON)
      710             
      711             /**************************************************************************************************/
      712             /**************************************************************************************************/
      713             /**************************************************************************************************/
      714             /* NEON IMPLEMENTATIONS                                                                           */
      715             /**************************************************************************************************/
      716             /**************************************************************************************************/
      717             /**************************************************************************************************/
      718             
      719             #include "arm_neon.h"
      720             
      721             /*
      722              * TDG VERSION
      723              */
      724             int ComputeSYMGS_TDG_NEON(const SparseMatrix & A, const Vector & r, Vector & x) {
      725             	assert(x.localLength == A.localNumberOfColumns);
      726             
      727             #ifndef HPCG_NO_MPI
      728             	ExchangeHalo(A, x);
      729             #endif
      730             
      731             	const double * const rv = r.values;
      732             	double * const xv = x.values;
      733             	double **matrixDiagonal = A.matrixDiagonal;
      734             
      735             	/*
      736             	 * FORWARD
      737             	 */
      738             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      739             #ifndef HPCG_NO_OPENMP
      740             #pragma omp parallel for SCHEDULE(runtime)
      741             #endif
      742             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      743             			local_int_t row = A.tdg[l][i];
      744             			const double * const currentValues = A.matrixValues[row];
      745             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      746             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      747             			const double currentDiagonal = matrixDiagonal[row][0];
      748             			float64x2_t contribs = vdupq_n_f64(0.0);
      749             
      750             			local_int_t j = 0;
      751             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      752             				// Load the needed j values
      753             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      754             				// Load the needed x values
      755             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      756             				float64x2_t xvv = vld1q_f64(aux);
      757             				// Add the contribution
      758             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      759             			}
      760             			// reduce contributions
      761             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      762             			double sum = rv[row] - totalContribution;
      763             			// Add missing values from last loop
      764             			if ( j < currentNumberOfNonzeros ) {
      765             				sum -= currentValues[j] * xv[currentColIndices[j]];
      766             			}
      767             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      768             			xv[row] = sum / currentDiagonal; // update row
      769             		}
      770             	}
      771             
      772             	/*
      773             	 * BACKWARD
      774             	 */
      775             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      776             #ifndef HPCG_NO_OPENMP
      777             #pragma omp parallel for SCHEDULE(runtime)
      778             #endif
      779             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      780             			local_int_t row = A.tdg[l][i];
      781             			const double * const currentValues = A.matrixValues[row];
      782             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      783             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      784             			const double currentDiagonal = matrixDiagonal[row][0];
      785             			float64x2_t contribs = vdupq_n_f64(0.0);
      786             
      787             			local_int_t j = 0;
      788             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      789             				// Load the needed j values
      790             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      791             				// Load the needed x values
      792             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      793             				float64x2_t xvv = vld1q_f64(aux);
      794             				// Add the contribution
      795             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      796             			}
      797             			// reduce contributions
      798             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      799             			double sum = rv[row] - totalContribution;
      800             			// Add missing values from last loop
      801             			if ( j < currentNumberOfNonzeros ) {
      802             				sum -= currentValues[j] * xv[currentColIndices[j]];
      803             			}
      804             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      805             			xv[row] = sum / currentDiagonal; // update row
      806             		}
      807             	}
      808             
      809             	return 0;
      810             }
      811             /*
      812              *
      813              */
      814             ////////////////////////////////////////////////////////////////////////////////
      815             ////////////////////////////////////////////////////////////////////////////////
      816             ////////////////////////////////////////////////////////////////////////////////
      817             /*
      818              * TDG FUSED VERSION
      819              */
      820             int ComputeFusedSYMGS_SPMV_NEON(const SparseMatrix & A, const Vector & r, Vector & x, Vector & y) {
      821             	assert(x.localLength == A.localNumberOfColumns);
      822             
      823             #ifndef HPCG_NO_MPI
      824             	ExchangeHalo(A, x);
      825             #endif
      826             
      827             	const double * const rv = r.values;
      828             	double * const xv = x.values;
      829             	double * const yv = y.values;
      830             	double **matrixDiagonal = A.matrixDiagonal;
      831             
      832             	/*
      833             	 * FORWARD
      834             	 */
      835             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      836             #ifndef HPCG_NO_OPENMP
      837             #pragma omp parallel for SCHEDULE(runtime)
      838             #endif
      839             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      840             			local_int_t row = A.tdg[l][i];
      841             			const double * const currentValues = A.matrixValues[row];
      842             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      843             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      844             			const double currentDiagonal = matrixDiagonal[row][0];
      845             			float64x2_t contribs = vdupq_n_f64(0.0);
      846             
      847             			local_int_t j = 0;
      848             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      849             				// Load the needed j values
      850             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      851             				// Load the needed x values
      852             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      853             				float64x2_t xvv = vld1q_f64(aux);
      854             				// Add the contribution
      855             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      856             			}
      857             			// reduce contributions
      858             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      859             			double sum = rv[row] - totalContribution;
      860             			// Add missing values from last loop
      861             			if ( j < currentNumberOfNonzeros ) {
      862             				sum -= currentValues[j] * xv[currentColIndices[j]];
      863             			}
      864             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      865             			xv[row] = sum / currentDiagonal; // update row
      866             		}
      867             	}
      868             
      869             	/*
      870             	 * BACKWARD (fusing SYMGS and SPMV)
      871             	 */
      872             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      873             #ifndef HPCG_NO_OPENMP
      874             #pragma omp parallel for SCHEDULE(runtime)
      875             #endif
      876             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      877             			local_int_t row = A.tdg[l][i];
      878             			const double * const currentValues = A.matrixValues[row];
      879             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      880             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      881             			const double currentDiagonal = matrixDiagonal[row][0];
      882             			float64x2_t contribs = vdupq_n_f64(0.0);
      883             
      884             			local_int_t j = 0;
      885             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      886             				// Load the needed j values
      887             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      888             				// Load the needed x values
      889             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      890             				float64x2_t xvv = vld1q_f64(aux);
      891             				// Add the contribution
      892             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      893             			}
      894             			// reduce contributions
      895             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      896             			// Add missing values from last loop
      897             			if ( j < currentNumberOfNonzeros ) {
      898             				totalContribution += currentValues[j] * xv[currentColIndices[j]];
      899             			}
      900             			totalContribution -= xv[row] * currentDiagonal; // remove diagonal contribution
      901             			double sum = rv[row] - totalContribution; // substract contributions from RHS
      902             			xv[row] = sum / currentDiagonal; // update row
      903             			// Fusion part
      904             			totalContribution += xv[row] * currentDiagonal; // add updated diagonal contribution
      905             			yv[row] = totalContribution; // update SPMV output vector
      906             		}
      907             	}
      908             
      909             	return 0;
      910             }
      911             /*
      912              *
      913              */
      914             ////////////////////////////////////////////////////////////////////////////////
      915             ////////////////////////////////////////////////////////////////////////////////
      916             ////////////////////////////////////////////////////////////////////////////////
      917             /*
      918              * BLOCK COLORED VERSION
      919              */
      920             int ComputeSYMGS_BLOCK_NEON(const SparseMatrix & A, const Vector & r, Vector & x) {
      921             
      922             	assert(x.localLength >= A.localNumberOfColumns);
      923             	
      924             #ifndef HPCG_NO_MPI
      925             	ExchangeHalo(A, x);
      926             #endif
      927             
      928             	double **matrixDiagonal = A.matrixDiagonal;
      929             	const double * const rv = r.values;
      930             	double * const xv = x.values;
      931             
      932             	local_int_t firstBlock = 0;
      933             	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
      934             	/*
      935             	 * FORWARD
      936             	 */
      937             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) { // for each color
      938             		if ( color > 0 ) {
      939             			firstBlock += A.numberOfBlocksInColor[color-1];
      940             			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
      941             		}
      942             #ifndef HPCG_NO_OPENMP
      943             #pragma omp parallel for SCHEDULE(runtime)
      944             #endif
      945             		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) { // for each super block with the same color
      946             			local_int_t firstRow = block * A.blockSize;
      947             			local_int_t firstChunk = firstRow / A.chunkSize;
      948             			local_int_t lastChunk = (firstRow + A.blockSize*A.chunkSize) / A.chunkSize;
      949             
      950             			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) { // for each chunk of this super block
      951             				local_int_t first = A.chunkSize * chunk;
      952             				local_int_t last = first + A.chunkSize;
      953             
      954             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      955             				local_int_t i = first;
      956             				if ( A.chunkSize == 4 ) {
      957             					const double * const currentValues0 = A.matrixValues[i  ];
      958             					const double * const currentValues1 = A.matrixValues[i+1];
      959             					const double * const currentValues2 = A.matrixValues[i+2];
      960             					const double * const currentValues3 = A.matrixValues[i+3];
      961             
      962             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      963             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      964             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      965             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      966             
      967             					const double currentDiagonal[4] = { matrixDiagonal[i  ][0],\
      968             														matrixDiagonal[i+1][0],\
      969             														matrixDiagonal[i+2][0],\
      970             														matrixDiagonal[i+3][0]};
      971             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
      972             					float64x2_t diagonal23 = vld1q_f64(&currentDiagonal[2]);
      973             
      974             					float64x2_t contribs0 = vdupq_n_f64(0.0);
      975             					float64x2_t contribs1 = vdupq_n_f64(0.0);
      976             					float64x2_t contribs2 = vdupq_n_f64(0.0);
      977             					float64x2_t contribs3 = vdupq_n_f64(0.0);
      978             
      979             					float64x2_t vrv01 = vld1q_f64(&rv[i]);
      980             					float64x2_t vrv23 = vld1q_f64(&rv[i+2]);
      981             
      982             					float64x2_t vxv01 = vld1q_f64(&xv[i]);
      983             					float64x2_t vxv23 = vld1q_f64(&xv[i+2]);
      984             
      985             					local_int_t j = 0;
      986             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
      987             						// Load values
      988             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
      989             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
      990             						float64x2_t values2 = vld1q_f64(&currentValues2[j]);
      991             						float64x2_t values3 = vld1q_f64(&currentValues3[j]);
      992             
      993             						// Load x
      994             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
      995             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
      996             						float64x2_t vxv2 = { xv[currentColIndices2[j]], xv[currentColIndices2[j+1]] };
      997             						float64x2_t vxv3 = { xv[currentColIndices3[j]], xv[currentColIndices3[j+1]] };
      998             
      999             						// Add contribution
     1000             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1001             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1002             						contribs2 = vfmaq_f64(contribs2, values2, vxv2);
     1003             						contribs3 = vfmaq_f64(contribs3, values3, vxv3);
     1004             					}
     1005             					// Reduce contribution
     1006             					// First for i and i+1
     1007             					float64x2_t totalContribution01;
     1008             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1009             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1010             
     1011             					// Then for i+2 and i+3
     1012             					float64x2_t totalContribution23;
     1013             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs2), totalContribution23, 0);
     1014             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs3), totalContribution23, 1);
     1015             
     1016             					// Substract contributions from RHS
     1017             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1018             					float64x2_t sum23 = vsubq_f64(vrv23, totalContribution23);
     1019             
     1020             					// Add contributions from missing elements (if any)
     1021             					if ( j < currentNumberOfNonzeros ) {
     1022             						// Load current values
     1023             						float64x2_t values01 = { currentValues0[j], currentValues1[j] };
     1024             						float64x2_t values23 = { currentValues2[j], currentValues3[j] };
     1025             
     1026             						// Load x
     1027             						float64x2_t vx01 = { xv[currentColIndices0[j]], xv[currentColIndices1[j]] };
     1028             						float64x2_t vx23 = { xv[currentColIndices2[j]], xv[currentColIndices3[j]] };
     1029             
     1030             						// Add contributions
     1031             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1032             						sum23 = vfmsq_f64(sum23, values23, vx23);
     1033             					}
     1034             
     1035             					// Remove diagonal contribution and update rows i and i+1
     1036             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1037             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1038             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1039             
     1040             					// Remove diagonal contribution and update rows i+2 and i+3
     1041             					sum23 = vfmaq_f64(sum23, vxv23, diagonal23);
     1042             					xv[i+2] = vgetq_lane_f64(sum23, 0) / currentDiagonal[2];
     1043             					xv[i+3] = vgetq_lane_f64(sum23, 1) / currentDiagonal[3];
     1044             				} else if ( A.chunkSize == 2 ) {
     1045             					const double * const currentValues0 = A.matrixValues[i  ];
     1046             					const double * const currentValues1 = A.matrixValues[i+1];
     1047             
     1048             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1049             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1050             
     1051             					const double currentDiagonal[2] = { matrixDiagonal[i  ][0],\
     1052             														matrixDiagonal[i+1][0]};
     1053             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1054             
     1055             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1056             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1057             
     1058             					float64x2_t vrv01 = vld1q_f64(&rv[i]);
     1059             
     1060             					float64x2_t vxv01 = vld1q_f64(&xv[i]);
     1061             
     1062             					local_int_t j = 0;
     1063             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1064             						// Load values
     1065             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1066             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1067             
     1068             						// Load x
     1069             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1070             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1071             
     1072             						// Add contribution
     1073             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1074             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1075             					}
     1076             					// Reduce contribution
     1077             					// First for i and i+1
     1078             					float64x2_t totalContribution01;
     1079             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1080             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1081             
     1082             					// Substract contributions from RHS
     1083             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1084             
     1085             					// Add contributions from missing elements (if any)
     1086             					if ( j < currentNumberOfNonzeros ) {
     1087             						// Load current values
     1088             						float64x2_t values01 = { currentValues0[j], currentValues1[j] };
     1089             
     1090             						// Load x
     1091             						float64x2_t vx01 = { xv[currentColIndices0[j]], xv[currentColIndices1[j]] };
     1092             
     1093             						// Add contributions
     1094             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1095             					}
     1096             
     1097             					// Remove diagonal contribution and update rows i and i+1
     1098             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1099             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1100             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1101             				} else { // A.chunkSize == 1
     1102             					const double * const currentValues = A.matrixValues[i];
     1103             					const local_int_t * const currentColIndices = A.mtxIndL[i];
     1104             					const double currentDiagonal = matrixDiagonal[i][0];
     1105             					float64x2_t contribs = vdupq_n_f64(0.0);
     1106             
     1107             					local_int_t j = 0;
     1108             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1109             						// Load values
     1110             						float64x2_t values = vld1q_f64(&currentValues[j]);
     1111             
     1112             						// Load x
     1113             						float64x2_t vxv = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
     1114             
     1115             						// Add contribution
     1116             						contribs = vfmaq_f64(contribs, values, vxv);
     1117             					}
     1118             					// Reduce contribution
     1119             					// First for i and i+1
     1120             					double totalContribution;
     1121             					totalContribution = vaddvq_f64(contribs);
     1122             
     1123             					// Substract contributions from RHS
     1124             					double sum = rv[i] - totalContribution;
     1125             
     1126             					// Add contributions from missing elements (if any)
     1127             					if ( j < currentNumberOfNonzeros ) {
     1128             						sum -= currentValues[j] * xv[currentColIndices[j]];
     1129             					}
     1130             
     1131             					// Remove diagonal contribution and update rows i and i+1
     1132             					sum += xv[i] * currentDiagonal;
     1133             					xv[i] = sum / currentDiagonal;
     1134             				}
     1135             			}
     1136             		}
     1137             	}
     1138             
     1139             	firstBlock = A.numberOfBlocks-1;
     1140             	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
     1141             	/*
     1142             	 * BACKWARD
     1143             	 */
     1144             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
     1145             		if ( color < A.numberOfColors-1 ) {
     1146             			firstBlock -= A.numberOfBlocksInColor[color+1];
     1147             			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
     1148             		}
     1149             #ifndef HPCG_NO_OPENMP
     1150             #pragma omp parallel for SCHEDULE(runtime)
     1151             #endif
     1152             		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) { // we skip a whole superblock on each iteration
     1153             			local_int_t firstRow = ((block+1) * A.blockSize) - 1; // this is the last row of the last block (i.e., next block first row - 1)
     1154             			local_int_t firstChunk = firstRow / A.chunkSize; // this is the  chunk of the row above
     1155             			local_int_t lastChunk = (firstRow - A.blockSize*A.chunkSize) / A.chunkSize; 
     1156             
     1157             			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
     1158             				local_int_t first = A.chunkSize * chunk;
     1159             				local_int_t last = first + A.chunkSize;
     1160             
     1161             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
     1162             				if ( A.chunkSize == 4 ) {
     1163             					local_int_t i = last-1-3;
     1164             
     1165             					const double * const currentValues3 = A.matrixValues[i+3];
     1166             					const double * const currentValues2 = A.matrixValues[i+2];
     1167             					const double * const currentValues1 = A.matrixValues[i+1];
     1168             					const double * const currentValues0 = A.matrixValues[i  ];
     1169             
     1170             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
     1171             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
     1172             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1173             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1174             
     1175             					const double currentDiagonal[4] = {\
     1176             							matrixDiagonal[i  ][0],\
     1177             							matrixDiagonal[i+1][0],\
     1178             							matrixDiagonal[i+2][0],\
     1179             							matrixDiagonal[i+3][0]};
     1180             
     1181             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1182             					float64x2_t diagonal23 = vld1q_f64(&currentDiagonal[2]);
     1183             
     1184             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1185             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1186             					float64x2_t contribs2 = vdupq_n_f64(0.0);
     1187             					float64x2_t contribs3 = vdupq_n_f64(0.0);
     1188             
     1189             					float64x2_t vrv23 = vld1q_f64(&rv[i+2]);
     1190             					float64x2_t vrv01 = vld1q_f64(&rv[i  ]);
     1191             
     1192             					float64x2_t vxv23 = vld1q_f64(&xv[i+2]);
     1193             					float64x2_t vxv01 = vld1q_f64(&xv[i  ]);
     1194             
     1195             					local_int_t j = 0;
     1196             					for ( j = currentNumberOfNonzeros-2; j >= 0; j -= 2 ) {
     1197             						// Load values
     1198             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1199             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1200             						float64x2_t values2 = vld1q_f64(&currentValues2[j]);
     1201             						float64x2_t values3 = vld1q_f64(&currentValues3[j]);
     1202             
     1203             						// Load x
     1204             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1205             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1206             						float64x2_t vxv2 = { xv[currentColIndices2[j]], xv[currentColIndices2[j+1]] };
     1207             						float64x2_t vxv3 = { xv[currentColIndices3[j]], xv[currentColIndices3[j+1]] };
     1208             
     1209             						// Add contribution
     1210             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1211             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1212             						contribs2 = vfmaq_f64(contribs2, values2, vxv2);
     1213             						contribs3 = vfmaq_f64(contribs3, values3, vxv3);
     1214             					}
     1215             					// Reduce contribution
     1216             					// First for i and i-1
     1217             					float64x2_t totalContribution01;
     1218             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1219             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1220             
     1221             					// Then for i-2 and i-3
     1222             					float64x2_t totalContribution23;
     1223             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs2), totalContribution23, 0);
     1224             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs3), totalContribution23, 1);
     1225             
     1226             					// Substract contributions from RHS
     1227             					float64x2_t sum23 = vsubq_f64(vrv23, totalContribution23);
     1228             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1229             
     1230             					// Add contributions from missing elements (if any)
     1231             					if ( j == -1 ) {
     1232             						// Load current values
     1233             						float64x2_t values23 = { currentValues2[j+1], currentValues3[j+1] };
     1234             						float64x2_t values01 = { currentValues0[j+1], currentValues1[j+1] };
     1235             
     1236             						// Load x
     1237             						float64x2_t vx23 = { xv[currentColIndices2[j+1]], xv[currentColIndices3[j+1]] };
     1238             						float64x2_t vx01 = { xv[currentColIndices0[j+1]], xv[currentColIndices1[j+1]] };
     1239             
     1240             						// Add contributions
     1241             						sum23 = vfmsq_f64(sum23, values23, vx23);
     1242             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1243             					}
     1244             
     1245             					// Remove diagonal contribution and update rows i-2 and i-3
     1246             					sum23 = vfmaq_f64(sum23, vxv23, diagonal23);
     1247             					xv[i+3] = vgetq_lane_f64(sum23, 1) / currentDiagonal[3];
     1248             					xv[i+2] = vgetq_lane_f64(sum23, 0) / currentDiagonal[2];
     1249             
     1250             					// Remove diagonal contribution and update rows i and i-1
     1251             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1252             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1253             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1254             				} else if ( A.chunkSize == 2 ) {
     1255             					local_int_t i = last-1-1;
     1256             
     1257             					const double * const currentValues1 = A.matrixValues[i+1];
     1258             					const double * const currentValues0 = A.matrixValues[i  ];
     1259             
     1260             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1261             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1262             
     1263             					const double currentDiagonal[2] = {\
     1264             							matrixDiagonal[i  ][0],\
     1265             							matrixDiagonal[i+1][0]};
     1266             
     1267             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1268             
     1269             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1270             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1271             
     1272             					float64x2_t vrv01 = vld1q_f64(&rv[i  ]);
     1273             
     1274             					float64x2_t vxv01 = vld1q_f64(&xv[i  ]);
     1275             
     1276             					local_int_t j = 0;
     1277             					for ( j = currentNumberOfNonzeros-2; j >= 0; j -= 2 ) {
     1278             						// Load values
     1279             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1280             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1281             
     1282             						// Load x
     1283             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1284             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1285             
     1286             						// Add contribution
     1287             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1288             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1289             					}
     1290             					// Reduce contribution
     1291             					// First for i and i-1
     1292             					float64x2_t totalContribution01;
     1293             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1294             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1295             
     1296             					// Substract contributions from RHS
     1297             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1298             
     1299             					// Add contributions from missing elements (if any)
     1300             					if ( j == -1 ) {
     1301             						// Load current values
     1302             						float64x2_t values01 = { currentValues0[j+1], currentValues1[j+1] };
     1303             
     1304             						// Load x
     1305             						float64x2_t vx01 = { xv[currentColIndices0[j+1]], xv[currentColIndices1[j+1]] };
     1306             
     1307             						// Add contributions
     1308             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1309             					}
     1310             
     1311             					// Remove diagonal contribution and update rows i and i-1
     1312             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1313             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1314             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1315             				} else { // A.chunkSize == 1
     1316             					local_int_t i = last - 1; // == first
     1317             					const double * const currentValues = A.matrixValues[i];
     1318             					const local_int_t * const currentColIndices = A.mtxIndL[i];
     1319             					const double currentDiagonal = matrixDiagonal[i][0];
     1320             
     1321             					float64x2_t contribs = vdupq_n_f64(0.0);
     1322             
     1323             					local_int_t j = 0;
     1324             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1325             						// Load values
     1326             						float64x2_t values = vld1q_f64(&currentValues[j]);
     1327             
     1328             						// Load x
     1329             						float64x2_t vxv = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
     1330             
     1331             						// Add contribution
     1332             						contribs = vfmaq_f64(contribs, values, vxv);
     1333             					}
     1334             					// Reduce contribution
     1335             					double totalContribution = vaddvq_f64(contribs);
     1336             
     1337             					// Substract contribution from RHS
     1338             					double sum = rv[i] - totalContribution;
     1339             
     1340             					// Add contributions from missing elements (if any)
     1341             					if ( j < currentNumberOfNonzeros ) {
     1342             						sum -= currentValues[j] * xv[currentColIndices[j]];
     1343             					}
     1344             
     1345             					// Remove diagonal contribution and updated row i
     1346             					sum += xv[i] * currentDiagonal;
     1347             					xv[i] = sum / currentDiagonal;
     1348             				}
     1349             			}
     1350             		}
     1351             	}
     1352             
     1353             	return 0;
     1354             }
     1355             /*
     1356              *
     1357              */
     1358             #endif
     1359             //#else // !HPCG_USE_SVE ! HPCG_USE_NEON
     1360             
     1361             int ComputeFusedSYMGS_SPMV ( const SparseMatrix & A, const Vector & r, Vector & x, Vector & y ) {
     1362             	assert(x.localLength == A.localNumberOfColumns);
     1363             
     1364             #ifndef HPCG_NO_MPI
     1365             	ExchangeHalo(A, x);
     1366             #endif
     1367             
     1368             	const double * const rv = r.values;
     1369             	double * const xv = x.values;
     1370             	double * const yv = y.values;
     1371             	double **matrixDiagonal = A.matrixDiagonal;
     1372             
     1373             	/*
     1374             	 * FORWARD
     1375             	 */
     1376    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1377             #ifndef HPCG_NO_OPENMP
     1378             #pragma omp parallel for SCHEDULE(runtime)
     1379             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1380   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
     1381   p         			local_int_t row = A.tdg[l][i];
     1382   p         			const double * const currentValues = A.matrixValues[row];
     1383   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1384   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1385   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1386   p         			double sum = rv[row];
     1387             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1388   p     8v  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j++ ) {
     1389   p     8v  				local_int_t curCol = currentColIndices[j];
     1390   p     8v  				sum -= currentValues[j] * xv[curCol];
     1391   p     8v  			}
     1392   p         			sum += xv[row] * currentDiagonal;
     1393   p         			xv[row] = sum / currentDiagonal;
     1394   pi        		}
     1395    i        	}
     1396             
     1397             	/*
     1398             	 * BACKWARD (fusing SYMGS and SPMV)
     1399             	 */
     1400    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1401             #ifndef HPCG_NO_OPENMP
     1402             #pragma omp parallel for SCHEDULE(runtime)
     1403             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1404   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
     1405   p         			local_int_t row = A.tdg[l][i];
     1406   p         			const double * const currentValues = A.matrixValues[row];
     1407   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1408   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1409   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1410   p         			double sum = 0.0;
     1411             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1412   p     8v  			for ( local_int_t j = currentNumberOfNonzeros-1; j >= 0; j-- ) {
     1413   p     8v  				local_int_t curCol = currentColIndices[j];
     1414   p     8v  				sum += currentValues[j] * xv[curCol];
     1415   p     8v  			}
     1416   p         			sum -= xv[row] * currentDiagonal;
     1417   p         			xv[row] = (rv[row] - sum) / currentDiagonal;
     1418   p         			sum += xv[row] * currentDiagonal;
     1419   p         			yv[row] = sum;
     1420   p         		}
     1421             	}
     1422             
     1423             	return 0;
     1424             }
     1425             
     1426             int ComputeSYMGS_TDG ( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
     1427             
     1428             	assert( x.localLength == A.localNumberOfColumns);
     1429             
     1430             #ifndef HPCG_NO_MPI
     1431             	ExchangeHalo(A,x);
     1432             #endif
     1433             
     1434             	const double * const rv = r.values;
     1435             	double * const xv = x.values;
     1436             	double **matrixDiagonal = A.matrixDiagonal;
     1437             
     1438             /*#ifndef HPCG_NO_OPENMP
     1439             #pragma omp parallel SCHEDULE(runtime)
     1440             {
     1441             #endif
     1442             */
     1443             #pragma statement scache_isolate_way L2=10
     1444             #pragma statement scache_isolate_assign xv
     1445             
     1446             	/*
     1447             	 * FORWARD
     1448             	 */
     1449    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1450             #ifndef HPCG_NO_OPENMP
     1451             #pragma omp parallel for SCHEDULE(runtime)
     1452             #endif
     1453             		#pragma loop unroll_and_jam(4)
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1454   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
     1455   p         			local_int_t row = A.tdg[l][i];
     1456   p         			const double * const currentValues = A.matrixValues[row];
     1457   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1458   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1459   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1460   p         			double sum = rv[row];
     1461             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1462   p     8v  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j++ ) {
     1463   p     8v  				local_int_t curCol = currentColIndices[j];
     1464   p     8v  				sum -= currentValues[j] * xv[curCol];
     1465   p     8v  			}
     1466   p         			sum += xv[row] * currentDiagonal;
     1467   p         			xv[row] = sum / currentDiagonal;
     1468   pi        		}
     1469    i        	}
     1470             
     1471             	/*
     1472             	 * BACKWARD
     1473             	 */
     1474    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1475             #ifndef HPCG_NO_OPENMP
     1476             #pragma omp parallel for SCHEDULE(runtime)
     1477             #endif
     1478             		#pragma loop unroll_and_jam(4)
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1479   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
     1480   p         			local_int_t row = A.tdg[l][i];
     1481   p         			const double * const currentValues = A.matrixValues[row];
     1482   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1483   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1484   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1485   p         			double sum = rv[row];
     1486             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1487   p     8v  			for ( local_int_t j = currentNumberOfNonzeros-1; j >= 0; j-- ) {
     1488   p     8v  				local_int_t curCol = currentColIndices[j];
     1489   p     8v  				sum -= currentValues[j] * xv[curCol];
     1490   p     8v  			}
     1491   p         			sum += xv[row] * currentDiagonal;
     1492   p         			xv[row] = sum / currentDiagonal;
     1493   p         		}
     1494             	}
     1495             
     1496             	#pragma statement end_scache_isolate_assign
     1497             	#pragma statement end_scache_isolate_way
     1498             /*#ifndef HPCG_NO_OPENMP
     1499             }
     1500             #endif*/
     1501             
     1502             	return 0;
     1503             }
     1504             
     1505             int ComputeSYMGS_BLOCK( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
     1506             
     1507             	assert(x.localLength >= A.localNumberOfColumns);
     1508             	
     1509             #ifndef HPCG_NO_MPI
     1510             	ExchangeHalo(A, x);
     1511             #endif
     1512             
     1513             	const local_int_t nrow = A.localNumberOfRows;
     1514             	double **matrixDiagonal = A.matrixDiagonal;
     1515             	const double * const rv = r.values;
     1516             	double * const xv = x.values;
     1517             
     1518             	local_int_t firstBlock = 0;
     1519    i        	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
     1520             	/*
     1521             	 * FORWARD
     1522             	 */
     1523             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) {
     1524             		if ( color > 0 ) {
     1525    i        			firstBlock += A.numberOfBlocksInColor[color-1];
     1526    i        			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
     1527             		}
     1528             #ifndef HPCG_NO_OPENMP
     1529             #pragma omp parallel for SCHEDULE(runtime)
     1530             #endif
     1531   p         		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) {
     1532   p         			local_int_t firstRow = block * A.blockSize;
     1533   p         			local_int_t firstChunk = firstRow / A.chunkSize;
     1534   p         			local_int_t lastChunk = (firstRow + A.blockSize*A.chunkSize) / A.chunkSize;
     1535             
     1536   p         			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) {
     1537   p         				local_int_t first = A.chunkSize * chunk;
     1538   p         				local_int_t last = first + A.chunkSize;
     1539             
     1540             				//for ( local_int_t i = first; i < last; i+= (A.chunkSize/2)) {
     1541   p         				local_int_t i = first;
     1542   p         				if ( A.chunkSize == 4 ) {
     1543   p         					double sum0 = rv[i+0];
     1544   p         					double sum1 = rv[i+1];
     1545   p         					double sum2 = rv[i+2];
     1546   p         					double sum3 = rv[i+3];
     1547             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1548   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1549   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1550   p      s  						sum1 -= A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
     1551   p      s  						sum2 -= A.matrixValues[i+2][j] * xv[A.mtxIndL[i+2][j]];
     1552   p      s  						sum3 -= A.matrixValues[i+3][j] * xv[A.mtxIndL[i+3][j]];
     1553   pi     s  					}
     1554   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1555   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1556   p         					sum1 += matrixDiagonal[i+1][0] * xv[i+1];
     1557   p         					xv[i+1] = sum1 / matrixDiagonal[i+1][0];
     1558   p         					sum2 += matrixDiagonal[i+2][0] * xv[i+2];
     1559   p         					xv[i+2] = sum2 / matrixDiagonal[i+2][0];
     1560   p         					sum3 += matrixDiagonal[i+3][0] * xv[i+3];
     1561   p         					xv[i+3] = sum3 / matrixDiagonal[i+3][0];
     1562   p         				} else if ( A.chunkSize == 2 ) {
     1563   p         					double sum0 = rv[i+0];
     1564   p         					double sum1 = rv[i+1];
     1565             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1566   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1567   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1568   p      s  						sum1 -= A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
     1569   pi     s  					}
     1570   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1571   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1572   p         					sum1 += matrixDiagonal[i+1][0] * xv[i+1];
     1573   p         					xv[i+1] = sum1 / matrixDiagonal[i+1][0];
     1574   p         				} else { // A.chunkSize == 1
     1575   p         					double sum0 = rv[i+0];
     1576             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1577   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1578   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1579   pi     s  					}
     1580   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1581   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1582   p         				}
     1583   p         			}
     1584   p         		}
     1585             	}
     1586             
     1587             	firstBlock = A.numberOfBlocks-1;
     1588    i        	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
     1589             	/*
     1590             	 * BACKWARD
     1591             	 */
     1592             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
     1593             		if ( color < A.numberOfColors-1 ) {
     1594    i        			firstBlock -= A.numberOfBlocksInColor[color+1];
     1595    i        			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
     1596             		}
     1597             #ifndef HPCG_NO_OPENMP
     1598             #pragma omp parallel for SCHEDULE(runtime)
     1599             #endif
     1600   p         		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) {
     1601   p         			local_int_t firstRow = ((block+1) * A.blockSize) - 1; // this is the last row of the last block
     1602   p         			local_int_t firstChunk = firstRow / A.chunkSize; // this is the  chunk of the row above
     1603   p         			local_int_t lastChunk = (firstRow - A.blockSize*A.chunkSize) / A.chunkSize; 
     1604             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1605   p         			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
     1606   p         				local_int_t first = A.chunkSize * chunk;
     1607   p         				local_int_t last = first + A.chunkSize;
     1608             
     1609             				//for ( local_int_t i = last-1; i >= first; i -= (A.chunkSize/2)) {
     1610   p         				local_int_t i = last-1;
     1611   p         				if ( A.chunkSize == 4 ) {
     1612   p         					double sum3 = rv[i-3];
     1613   p         					double sum2 = rv[i-2];
     1614   p         					double sum1 = rv[i-1];
     1615   p         					double sum0 = rv[i  ];
     1616             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.28, ITR: 32, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1617   pi     v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1618   p      v  						sum3 -= A.matrixValues[i-3][j] * xv[A.mtxIndL[i-3][j]];
     1619   p      v  						sum2 -= A.matrixValues[i-2][j] * xv[A.mtxIndL[i-2][j]];
     1620   p      v  						sum1 -= A.matrixValues[i-1][j] * xv[A.mtxIndL[i-1][j]];
     1621   p      v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1622   p      v  					}
     1623   p         					sum3 += matrixDiagonal[i-3][0] * xv[i-3];
     1624   p         					xv[i-3] = sum3 / matrixDiagonal[i-3][0];
     1625             
     1626   p         					sum2 += matrixDiagonal[i-2][0] * xv[i-2];
     1627   p         					xv[i-2] = sum2 / matrixDiagonal[i-2][0];
     1628             
     1629   p         					sum1 += matrixDiagonal[i-1][0] * xv[i-1];
     1630   p         					xv[i-1] = sum1 / matrixDiagonal[i-1][0];
     1631             
     1632   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1633   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1634   p         				} else if ( A.chunkSize == 2 ) {
     1635   p         					double sum1 = rv[i-1];
     1636   p         					double sum0 = rv[i  ];
     1637             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 96, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1638   pi    4v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1639   p     4v  						sum1 -= A.matrixValues[i-1][j] * xv[A.mtxIndL[i-1][j]];
     1640   p     4v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1641   p     4v  					}
     1642             
     1643   p         					sum1 += matrixDiagonal[i-1][0] * xv[i-1];
     1644   p         					xv[i-1] = sum1 / matrixDiagonal[i-1][0];
     1645             
     1646   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1647   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1648   p         				} else { // A.chunkSize == 1
     1649   p         					double sum0 = rv[i  ];
     1650             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1651   pi    8v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1652   p     8v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1653   p     8v  					}
     1654             
     1655   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1656   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1657   p         				}
     1658   p         			}
     1659   p         		}
     1660             	}
     1661             
     1662             	return 0;
     1663             }
     1664             //#endif
     1665             
     1666             
     1667             
     1668             /*!
     1669               Routine to compute one step of symmetric Gauss-Seidel:
     1670             
     1671               Assumption about the structure of matrix A:
     1672               - Each row 'i' of the matrix has nonzero diagonal value whose address is matrixDiagonal[i]
     1673               - Entries in row 'i' are ordered such that:
     1674                    - lower triangular terms are stored before the diagonal element.
     1675                    - upper triangular terms are stored after the diagonal element.
     1676                    - No other assumptions are made about entry ordering.
     1677             
     1678               Symmetric Gauss-Seidel notes:
     1679               - We use the input vector x as the RHS and start with an initial guess for y of all zeros.
     1680               - We perform one forward sweep.  Since y is initially zero we can ignore the upper triangular terms of A.
     1681               - We then perform one back sweep.
     1682                    - For simplicity we include the diagonal contribution in the for-j loop, then correct the sum after
     1683             
     1684               @param[in] A the known system matrix
     1685               @param[in] r the input vector
     1686               @param[inout] x On entry, x should contain relevant values, on exit x contains the result of one symmetric GS sweep with r as the RHS.
     1687             
     1688               @return returns 0 upon success and non-zero otherwise
     1689             
     1690               @warning Early versions of this kernel (Version 1.1 and earlier) had the r and x arguments in reverse order, and out of sync with other kernels.
     1691             
     1692               @see ComputeSYMGS_ref
     1693             */
     1694             int ComputeSYMGS( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace) {
     1695             
     1696             	// This function is just a stub right now which decides which implementation of the SYMGS will be executed (TDG or block coloring)
     1697             	if ( A.TDG ) {
     1698             #ifdef HPCG_USE_NEON
     1699             		return ComputeSYMGS_TDG_NEON(A, r, x);
     1700             #elif defined HPCG_USE_SVE
     1701             		return ComputeSYMGS_TDG_SVE(A, r, x, trace);
     1702             #else
     1703             		return ComputeSYMGS_TDG(A, r, x, trace);
     1704             #endif
     1705             	}
     1706             #ifdef HPCG_USE_NEON
     1707             	return ComputeSYMGS_BLOCK_NEON(A, r, x);
     1708             #elif defined HPCG_USE_SVE
     1709             	return ComputeSYMGS_BLOCK_SVE(A, r, x, trace);
     1710             #else
     1711             	return ComputeSYMGS_BLOCK(A, r, x, trace);
     1712             #endif
     1713             }
     1714             
     1715             inline void SYMGS_VERSION_1(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     1716             	
     1717             	double **matrixDiagonal = A.matrixDiagonal;
     1718             
     1719             	/*
     1720             	 * FORWARD SWEEP
     1721             	 */
     1722             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1723             		local_int_t tdgLevelSize = A.tdg[l].size();
     1724             		if((tdgLevelSize%2) == 0) {
     1725             #ifndef HPCG_NO_OPENMP
     1726             #pragma omp parallel for SCHEDULE(runtime)
     1727             #endif
     1728             		for ( local_int_t i = 0; i < tdgLevelSize; i+=2 ) {
     1729             			local_int_t row_1 = A.tdg[l][i];
     1730             			local_int_t row_2 = A.tdg[l][i+1];
     1731             			const double * const currentValues_1 = A.matrixValues[row_1];
     1732             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1733             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1734             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1735             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1736             
     1737             			const double * const currentValues_2 = A.matrixValues[row_2];
     1738             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1739             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1740             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1741             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1742             			
     1743             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     1744             
     1745             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1746             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1747             				
     1748             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1749             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1750             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1751             
     1752             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1753             
     1754             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1755             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1756             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1757             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1758             
     1759             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1760             			}
     1761             
     1762             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1763             			double sum_1 = rv[row_1] - totalContribution_1;
     1764             
     1765             			sum_1 += xv[row_1] * currentDiagonal_1;
     1766             			xv[row_1] = sum_1 / currentDiagonal_1;
     1767             
     1768             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1769             			double sum_2 = rv[row_2] - totalContribution_2;
     1770             
     1771             			sum_2 += xv[row_2] * currentDiagonal_2;
     1772             			xv[row_2] = sum_2 / currentDiagonal_2;
     1773             		}
     1774             		}
     1775             		else
     1776             		{
     1777             #ifndef HPCG_NO_OPENMP
     1778             #pragma omp parallel for SCHEDULE(runtime)
     1779             #endif
     1780             		for ( local_int_t i = 0; i < tdgLevelSize; i++ ) {
     1781             			local_int_t row = A.tdg[l][i];
     1782             			const double * const currentValues = A.matrixValues[row];
     1783             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1784             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1785             			const double currentDiagonal = matrixDiagonal[row][0];
     1786             			svfloat64_t contribs = svdup_f64(0.0);
     1787             
     1788             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1789             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1790             				
     1791             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1792             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1793             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1794             
     1795             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1796             			}
     1797             
     1798             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1799             			double sum = rv[row] - totalContribution;
     1800             
     1801             			sum += xv[row] * currentDiagonal;
     1802             			xv[row] = sum / currentDiagonal;
     1803             		}
     1804             		}
     1805             	}
     1806             
     1807             	/*
     1808             	 * BACKWARD SWEEP
     1809             	 */
     1810             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1811             		local_int_t tdgLevelSize = A.tdg[l].size();
     1812             		if((tdgLevelSize%2) == 0) {		
     1813             #ifndef HPCG_NO_OPENMP
     1814             #pragma omp parallel for SCHEDULE(runtime)
     1815             #endif
     1816             		for ( local_int_t i = tdgLevelSize-1; i >= 0; i-= 2 ) {
     1817             			local_int_t row_1 = A.tdg[l][i];
     1818             			local_int_t row_2 = A.tdg[l][i-1];
     1819             			const double * const currentValues_1 = A.matrixValues[row_1];
     1820             			const double * const currentValues_2 = A.matrixValues[row_2];
     1821             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1822             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1823             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1824             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1825             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1826             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1827             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1828             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1829             
     1830             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     1831             							
     1832             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1833             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1834             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1835             				
     1836             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1837             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1838             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1839             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1840             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1841             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1842             
     1843             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1844             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1845             			}
     1846             
     1847             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1848             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1849             			double sum_1 = rv[row_1] - totalContribution_1;
     1850             			double sum_2 = rv[row_2] - totalContribution_2;
     1851             
     1852             			sum_1 += xv[row_1] * currentDiagonal_1;
     1853             			sum_2 += xv[row_2] * currentDiagonal_2;
     1854             			xv[row_1] = sum_1 / currentDiagonal_1;
     1855             			xv[row_2] = sum_2 / currentDiagonal_2;
     1856             		}
     1857             		}
     1858             		else
     1859             		{
     1860             #ifndef HPCG_NO_OPENMP
     1861             #pragma omp parallel for SCHEDULE(runtime)
     1862             #endif
     1863             		for ( local_int_t i = tdgLevelSize-1; i >= 0; i-- ) {
     1864             			local_int_t row = A.tdg[l][i];
     1865             			const double * const currentValues = A.matrixValues[row];
     1866             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1867             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1868             			const double currentDiagonal = matrixDiagonal[row][0];
     1869             			svfloat64_t contribs = svdup_f64(0.0);
     1870             
     1871             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1872             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1873             				
     1874             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1875             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1876             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1877             
     1878             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1879             			}
     1880             
     1881             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1882             			double sum = rv[row] - totalContribution;
     1883             
     1884             			sum += xv[row] * currentDiagonal;
     1885             			xv[row] = sum / currentDiagonal;
     1886             		}
     1887             		}
     1888             	}
     1889             }
     1890             
     1891             inline void SYMGS_VERSION_2(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     1892             	
     1893             	double **matrixDiagonal = A.matrixDiagonal;
     1894             
     1895             	/*
     1896             	 * FORWARD SWEEP
     1897             	 */
     1898             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1899             		local_int_t tdgLevelSize = A.tdg[l].size();
     1900             		local_int_t maxLevelSize = 2*(tdgLevelSize / 2);
     1901             
     1902             #ifndef HPCG_NO_OPENMP
     1903             	#pragma omp parallel
     1904             	{
     1905             	#pragma omp for nowait SCHEDULE(runtime)
     1906             #endif
     1907             		for ( local_int_t i = 0; i < maxLevelSize; i+=2 ) {
     1908             			local_int_t row_1 = A.tdg[l][i];
     1909             			local_int_t row_2 = A.tdg[l][i+1];
     1910             			const double * const currentValues_1 = A.matrixValues[row_1];
     1911             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1912             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1913             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1914             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1915             
     1916             			const double * const currentValues_2 = A.matrixValues[row_2];
     1917             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1918             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1919             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1920             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1921             			
     1922             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     1923             
     1924             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1925             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1926             				
     1927             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1928             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1929             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1930             
     1931             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1932             
     1933             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1934             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1935             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1936             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1937             
     1938             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1939             			}
     1940             
     1941             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1942             			double sum_1 = rv[row_1] - totalContribution_1;
     1943             
     1944             			sum_1 += xv[row_1] * currentDiagonal_1;
     1945             			xv[row_1] = sum_1 / currentDiagonal_1;
     1946             
     1947             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1948             			double sum_2 = rv[row_2] - totalContribution_2;
     1949             
     1950             			sum_2 += xv[row_2] * currentDiagonal_2;
     1951             			xv[row_2] = sum_2 / currentDiagonal_2;
     1952             		}
     1953             
     1954             		#pragma omp single 
     1955             		if (maxLevelSize < tdgLevelSize) {
     1956             			local_int_t i = maxLevelSize;
     1957             
     1958             			local_int_t row = A.tdg[l][i];
     1959             			const double * const currentValues = A.matrixValues[row];
     1960             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1961             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1962             			const double currentDiagonal = matrixDiagonal[row][0];
     1963             			svfloat64_t contribs = svdup_f64(0.0);
     1964             
     1965             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1966             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1967             				
     1968             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1969             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1970             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1971             
     1972             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1973             			}
     1974             
     1975             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1976             			double sum = rv[row] - totalContribution;
     1977             
     1978             			sum += xv[row] * currentDiagonal;
     1979             			xv[row] = sum / currentDiagonal;
     1980             		}
     1981             #ifndef HPCG_NO_OPENMP
     1982             	}
     1983             #endif
     1984             	}
     1985             
     1986             	/*
     1987             	 * BACKWARD SWEEP
     1988             	 */
     1989             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1990             		local_int_t tdgLevelSize = A.tdg[l].size();
     1991             		local_int_t maxLevelSize = 2*(tdgLevelSize / 2);
     1992             
     1993             #ifndef HPCG_NO_OPENMP
     1994             #pragma omp parallel 
     1995             	{
     1996             		#pragma omp single nowait 
     1997             		{
     1998             #endif
     1999             		if (tdgLevelSize > maxLevelSize) {
     2000             			local_int_t i = maxLevelSize-1;
     2001             
     2002             			local_int_t row = A.tdg[l][i];
     2003             			const double * const currentValues = A.matrixValues[row];
     2004             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2005             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2006             			const double currentDiagonal = matrixDiagonal[row][0];
     2007             			svfloat64_t contribs = svdup_f64(0.0);
     2008             
     2009             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2010             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2011             				
     2012             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2013             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2014             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2015             
     2016             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2017             			}
     2018             
     2019             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2020             			double sum = rv[row] - totalContribution;
     2021             
     2022             			sum += xv[row] * currentDiagonal;
     2023             			xv[row] = sum / currentDiagonal;
     2024             		}
     2025             #ifndef HPCG_NO_OPENMP
     2026             		}
     2027             #pragma omp for SCHEDULE(runtime)
     2028             #endif
     2029             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 2 ) {
     2030             			local_int_t row_1 = A.tdg[l][i];
     2031             			local_int_t row_2 = A.tdg[l][i-1];
     2032             			const double * const currentValues_1 = A.matrixValues[row_1];
     2033             			const double * const currentValues_2 = A.matrixValues[row_2];
     2034             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2035             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2036             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2037             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2038             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2039             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2040             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2041             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2042             
     2043             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2044             							
     2045             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2046             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2047             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2048             				
     2049             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2050             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2051             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2052             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2053             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2054             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2055             
     2056             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2057             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2058             			}
     2059             
     2060             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2061             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2062             			double sum_1 = rv[row_1] - totalContribution_1;
     2063             			double sum_2 = rv[row_2] - totalContribution_2;
     2064             
     2065             			sum_1 += xv[row_1] * currentDiagonal_1;
     2066             			sum_2 += xv[row_2] * currentDiagonal_2;
     2067             			xv[row_1] = sum_1 / currentDiagonal_1;
     2068             			xv[row_2] = sum_2 / currentDiagonal_2;
     2069             		}
     2070             #ifndef HPCG_NO_OPENMP
     2071             	}
     2072             #endif
     2073             	}
     2074             }
     2075             
     2076             inline void SYMGS_VERSION_3(const SparseMatrix& A, double * const& prxv, const double * const& rv) {
     2077             	
     2078             	double **matrixDiagonal = A.matrixDiagonal;
     2079             	double *xv = prxv;
     2080             
     2081             //#pragma statement scache_isolate_way L2=10
     2082             //#pragma statement scache_isolate_assign xv
     2083             #ifndef HPCG_NO_OPENMP
     2084             	#pragma omp parallel
     2085             	{
     2086             #endif
     2087             	/*
     2088             	 * FORWARD SWEEP
     2089             	 */
     2090             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     2091             		local_int_t tdgLevelSize = A.tdg[l].size();
     2092             		local_int_t maxLevelSize = 4*(tdgLevelSize / 4);
     2093             
     2094             #ifndef HPCG_NO_OPENMP
     2095             	//#pragma loop nounroll
     2096             	#pragma omp for nowait SCHEDULE(runtime)
     2097             #endif
     2098             		for ( local_int_t i = 0; i < maxLevelSize; i+=4 ) {
     2099             			local_int_t row_1 = A.tdg[l][i];
     2100             			local_int_t row_2 = A.tdg[l][i+1];
     2101             			local_int_t row_3 = A.tdg[l][i+2];
     2102             			local_int_t row_4 = A.tdg[l][i+3];
     2103             			const double * const currentValues_1 = A.matrixValues[row_1];
     2104             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2105             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2106             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2107             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2108             
     2109             			const double * const currentValues_2 = A.matrixValues[row_2];
     2110             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2111             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2112             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2113             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2114             
     2115             			const double * const currentValues_3 = A.matrixValues[row_3];
     2116             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2117             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2118             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2119             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2120             
     2121             			const double * const currentValues_4 = A.matrixValues[row_4];
     2122             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2123             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2124             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2125             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2126             
     2127             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     2128             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);
     2129             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);
     2130             
     2131             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2132             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2133             				
     2134             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2135             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2136             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2137             
     2138             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2139             
     2140             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2141             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2142             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2143             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2144             
     2145             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2146             
     2147             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2148             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2149             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2150             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2151             
     2152             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2153             
     2154             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2155             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2156             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2157             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2158             
     2159             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2160             			}
     2161             
     2162             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2163             			double sum_1 = rv[row_1] - totalContribution_1;
     2164             
     2165             			sum_1 += xv[row_1] * currentDiagonal_1;
     2166             			xv[row_1] = sum_1 / currentDiagonal_1;
     2167             
     2168             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2169             			double sum_2 = rv[row_2] - totalContribution_2;
     2170             
     2171             			sum_2 += xv[row_2] * currentDiagonal_2;
     2172             			xv[row_2] = sum_2 / currentDiagonal_2;
     2173             
     2174             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2175             			double sum_3 = rv[row_3] - totalContribution_3;
     2176             
     2177             			sum_3 += xv[row_3] * currentDiagonal_3;
     2178             			xv[row_3] = sum_3 / currentDiagonal_3;
     2179             
     2180             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2181             			double sum_4 = rv[row_4] - totalContribution_4;
     2182             
     2183             			sum_4 += xv[row_4] * currentDiagonal_4;
     2184             			xv[row_4] = sum_4 / currentDiagonal_4;
     2185             
     2186             		}
     2187             
     2188             //#pragma omp single
     2189             		if (maxLevelSize < tdgLevelSize) {
     2190             /************
     2191             #ifndef HPCG_NO_OPENMP
     2192             //#pragma loop nounroll
     2193             #pragma omp for SCHEDULE(runtime)
     2194             #endif
     2195             		for ( local_int_t i = maxLevelSize; i < tdgLevelSize; i++ ) {
     2196             
     2197             			local_int_t row = A.tdg[l][i];
     2198             			const double * const currentValues = A.matrixValues[row];
     2199             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2200             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2201             			const double currentDiagonal = matrixDiagonal[row][0];
     2202             			svfloat64_t contribs = svdup_f64(0.0);
     2203             
     2204             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2205             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2206             				
     2207             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2208             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2209             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2210             
     2211             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2212             			}
     2213             
     2214             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2215             			double sum = rv[row] - totalContribution;
     2216             
     2217             			sum += xv[row] * currentDiagonal;
     2218             			xv[row] = sum / currentDiagonal;
     2219             		}
     2220             *******/
     2221             		#pragma omp sections nowait
     2222             		{
     2223             			#pragma omp section 
     2224             			{
     2225             				local_int_t i = maxLevelSize;
     2226             				local_int_t row = A.tdg[l][i];
     2227             				const double * const currentValues = A.matrixValues[row];
     2228             				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2229             				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2230             				const double currentDiagonal = matrixDiagonal[row][0];
     2231             				svfloat64_t contribs = svdup_f64(0.0);
     2232             
     2233             				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2234             					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2235             					
     2236             					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2237             					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2238             					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2239             
     2240             					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2241             				}
     2242             
     2243             				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2244             				double sum = rv[row] - totalContribution;
     2245             
     2246             				sum += xv[row] * currentDiagonal;
     2247             				xv[row] = sum / currentDiagonal;
     2248             			}
     2249             			#pragma omp section 
     2250             			{
     2251             				local_int_t i = maxLevelSize + 1;
     2252             				if (i < tdgLevelSize) {
     2253             				local_int_t row = A.tdg[l][i];
     2254             				const double * const currentValues = A.matrixValues[row];
     2255             				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2256             				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2257             				const double currentDiagonal = matrixDiagonal[row][0];
     2258             				svfloat64_t contribs = svdup_f64(0.0);
     2259             
     2260             				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2261             					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2262             					
     2263             					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2264             					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2265             					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2266             
     2267             					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2268             				}
     2269             
     2270             				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2271             				double sum = rv[row] - totalContribution;
     2272             
     2273             				sum += xv[row] * currentDiagonal;
     2274             				xv[row] = sum / currentDiagonal;
     2275             				}
     2276             			}
     2277             			#pragma omp section 
     2278             			{
     2279             				local_int_t i = maxLevelSize + 2;
     2280             				if (i < tdgLevelSize) {
     2281             				local_int_t row = A.tdg[l][i];
     2282             				const double * const currentValues = A.matrixValues[row];
     2283             				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2284             				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2285             				const double currentDiagonal = matrixDiagonal[row][0];
     2286             				svfloat64_t contribs = svdup_f64(0.0);
     2287             
     2288             				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2289             					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2290             					
     2291             					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2292             					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2293             					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2294             
     2295             					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2296             				}
     2297             
     2298             				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2299             				double sum = rv[row] - totalContribution;
     2300             
     2301             				sum += xv[row] * currentDiagonal;
     2302             				xv[row] = sum / currentDiagonal;
     2303             				}
     2304             			}
     2305             		}
     2306             
     2307             /***********/
     2308             #ifndef HPCG_NO_OPENMP
     2309             	}
     2310             	#pragma omp barrier
     2311             #endif
     2312             	}
     2313             
     2314             	/*
     2315             	 * BACKWARD SWEEP
     2316             	 */
     2317             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     2318             		local_int_t tdgLevelSize = A.tdg[l].size();
     2319             		local_int_t maxLevelSize = 4*(tdgLevelSize / 4);
     2320             
     2321             #ifndef HPCG_NO_OPENMP
     2322             		//#pragma omp single nowait 
     2323             		//{
     2324             		//#pragma loop nounroll
     2325             		#pragma omp for nowait SCHEDULE(runtime)
     2326             #endif
     2327             		for ( local_int_t i = tdgLevelSize-1; i >= maxLevelSize; i-- ) {
     2328             
     2329             			local_int_t row = A.tdg[l][i];
     2330             			const double * const currentValues = A.matrixValues[row];
     2331             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2332             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2333             			const double currentDiagonal = matrixDiagonal[row][0];
     2334             			svfloat64_t contribs = svdup_f64(0.0);
     2335             
     2336             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2337             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2338             				
     2339             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2340             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2341             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2342             
     2343             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2344             			}
     2345             
     2346             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2347             			double sum = rv[row] - totalContribution;
     2348             
     2349             			sum += xv[row] * currentDiagonal;
     2350             			xv[row] = sum / currentDiagonal;
     2351             		}
     2352             #ifndef HPCG_NO_OPENMP
     2353             		//}
     2354             //#pragma loop nounroll
     2355             #pragma omp for SCHEDULE(runtime)
     2356             #endif
     2357             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 4 ) {
     2358             			local_int_t row_1 = A.tdg[l][i];
     2359             			local_int_t row_2 = A.tdg[l][i-1];
     2360             			local_int_t row_3 = A.tdg[l][i-2];
     2361             			local_int_t row_4 = A.tdg[l][i-3];
     2362             			const double * const currentValues_1 = A.matrixValues[row_1];
     2363             			const double * const currentValues_2 = A.matrixValues[row_2];
     2364             			const double * const currentValues_3 = A.matrixValues[row_3];
     2365             			const double * const currentValues_4 = A.matrixValues[row_4];
     2366             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2367             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2368             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2369             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2370             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2371             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2372             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2373             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2374             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2375             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2376             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2377             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2378             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2379             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2380             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2381             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2382             
     2383             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2384             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);				
     2385             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);				
     2386             							
     2387             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2388             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2389             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2390             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2391             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2392             				
     2393             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2394             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2395             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2396             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2397             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2398             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2399             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2400             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2401             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2402             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2403             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2404             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2405             
     2406             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2407             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2408             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2409             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2410             			}
     2411             
     2412             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2413             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2414             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2415             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2416             			double sum_1 = rv[row_1] - totalContribution_1;
     2417             			double sum_2 = rv[row_2] - totalContribution_2;
     2418             			double sum_3 = rv[row_3] - totalContribution_3;
     2419             			double sum_4 = rv[row_4] - totalContribution_4;
     2420             
     2421             			sum_1 += xv[row_1] * currentDiagonal_1;
     2422             			sum_2 += xv[row_2] * currentDiagonal_2;
     2423             			sum_3 += xv[row_3] * currentDiagonal_3;
     2424             			sum_4 += xv[row_4] * currentDiagonal_4;
     2425             			xv[row_1] = sum_1 / currentDiagonal_1;
     2426             			xv[row_2] = sum_2 / currentDiagonal_2;
     2427             			xv[row_3] = sum_3 / currentDiagonal_3;
     2428             			xv[row_4] = sum_4 / currentDiagonal_4;
     2429             		}
     2430             #ifndef HPCG_NO_OPENMP
     2431             	}
     2432             #endif
     2433             	}
     2434             
     2435             //#pragma statement end_scache_isolate_assign
     2436             //#pragma statement end_scache_isolate_way	
     2437             }
     2438             /////////////
     2439             inline void SYMGS_VERSION_4(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     2440             	
     2441             	double **matrixDiagonal = A.matrixDiagonal;
     2442             
     2443             	/*
     2444             	 * FORWARD SWEEP
     2445             	 */
     2446             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     2447             		local_int_t tdgLevelSize = A.tdg[l].size();
     2448             		local_int_t maxLevelSize = 6*(tdgLevelSize / 6);
     2449             
     2450             #ifndef HPCG_NO_OPENMP
     2451             	#pragma omp parallel
     2452             	{
     2453             	#pragma omp for nowait SCHEDULE(runtime)
     2454             #endif
     2455             		for ( local_int_t i = 0; i < maxLevelSize; i+=6 ) {
     2456             			local_int_t row_1 = A.tdg[l][i];
     2457             			local_int_t row_2 = A.tdg[l][i+1];
     2458             			local_int_t row_3 = A.tdg[l][i+2];
     2459             			local_int_t row_4 = A.tdg[l][i+3];
     2460             			local_int_t row_5 = A.tdg[l][i+4];
     2461             			local_int_t row_6 = A.tdg[l][i+5];
     2462             			const double * const currentValues_1 = A.matrixValues[row_1];
     2463             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2464             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2465             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2466             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2467             
     2468             			const double * const currentValues_2 = A.matrixValues[row_2];
     2469             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2470             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2471             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2472             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2473             
     2474             			const double * const currentValues_3 = A.matrixValues[row_3];
     2475             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2476             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2477             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2478             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2479             
     2480             			const double * const currentValues_4 = A.matrixValues[row_4];
     2481             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2482             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2483             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2484             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2485             
     2486             			const double * const currentValues_5 = A.matrixValues[row_5];
     2487             			const local_int_t * const currentColIndices_5 = A.mtxIndL[row_5];
     2488             			const int currentNumberOfNonzeros_5 = A.nonzerosInRow[row_5];
     2489             			const double currentDiagonal_5 = matrixDiagonal[row_5][0];
     2490             			svfloat64_t contribs_5 = svdup_f64(0.0);
     2491             
     2492             			const double * const currentValues_6 = A.matrixValues[row_6];
     2493             			const local_int_t * const currentColIndices_6 = A.mtxIndL[row_6];
     2494             			const int currentNumberOfNonzeros_6 = A.nonzerosInRow[row_6];
     2495             			const double currentDiagonal_6 = matrixDiagonal[row_6][0];
     2496             			svfloat64_t contribs_6 = svdup_f64(0.0);
     2497             
     2498             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     2499             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);
     2500             			const int maxNumberOfNonzeros3 = std::max(currentNumberOfNonzeros_5, currentNumberOfNonzeros_6);
     2501             			const int maxNumberOfNonzeros4 = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);
     2502             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros4, maxNumberOfNonzeros3);
     2503             
     2504             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2505             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);		
     2506             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2507             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2508             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2509             
     2510             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2511             
     2512             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2513             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2514             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2515             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2516             
     2517             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2518             
     2519             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2520             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2521             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2522             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2523             
     2524             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2525             
     2526             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2527             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2528             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2529             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2530             
     2531             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2532             
     2533             				svbool_t pg_5 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_5);
     2534             				svfloat64_t mtxValues_5 = svld1_f64(pg_5, &currentValues_5[j]);
     2535             				svuint64_t indices_5 = svld1sw_u64(pg_5, &currentColIndices_5[j]);
     2536             				svfloat64_t xvv_5 = svld1_gather_u64index_f64(pg_5, xv, indices_5);
     2537             
     2538             				contribs_5 = svmla_f64_m(pg_5, contribs_5, xvv_5, mtxValues_5);
     2539             
     2540             				svbool_t pg_6 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_6);
     2541             				svfloat64_t mtxValues_6 = svld1_f64(pg_6, &currentValues_6[j]);
     2542             				svuint64_t indices_6 = svld1sw_u64(pg_6, &currentColIndices_6[j]);
     2543             				svfloat64_t xvv_6 = svld1_gather_u64index_f64(pg_6, xv, indices_6);
     2544             
     2545             				contribs_6 = svmla_f64_m(pg_6, contribs_6, xvv_6, mtxValues_6);
     2546             			}
     2547             
     2548             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2549             			double sum_1 = rv[row_1] - totalContribution_1;
     2550             
     2551             			sum_1 += xv[row_1] * currentDiagonal_1;
     2552             			xv[row_1] = sum_1 / currentDiagonal_1;
     2553             
     2554             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2555             			double sum_2 = rv[row_2] - totalContribution_2;
     2556             
     2557             			sum_2 += xv[row_2] * currentDiagonal_2;
     2558             			xv[row_2] = sum_2 / currentDiagonal_2;
     2559             
     2560             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2561             			double sum_3 = rv[row_3] - totalContribution_3;
     2562             
     2563             			sum_3 += xv[row_3] * currentDiagonal_3;
     2564             			xv[row_3] = sum_3 / currentDiagonal_3;
     2565             
     2566             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2567             			double sum_4 = rv[row_4] - totalContribution_4;
     2568             
     2569             			sum_4 += xv[row_4] * currentDiagonal_4;
     2570             			xv[row_4] = sum_4 / currentDiagonal_4;
     2571             
     2572             			double totalContribution_5 = svaddv_f64(svptrue_b64(), contribs_5);
     2573             			double sum_5 = rv[row_5] - totalContribution_5;
     2574             
     2575             			sum_5 += xv[row_5] * currentDiagonal_5;
     2576             			xv[row_5] = sum_5 / currentDiagonal_5;
     2577             
     2578             			double totalContribution_6 = svaddv_f64(svptrue_b64(), contribs_6);
     2579             			double sum_6 = rv[row_6] - totalContribution_6;
     2580             
     2581             			sum_6 += xv[row_6] * currentDiagonal_6;
     2582             			xv[row_6] = sum_6 / currentDiagonal_6;
     2583             		}
     2584             
     2585             //#pragma omp single
     2586             		if (maxLevelSize < tdgLevelSize) {
     2587             #ifndef HPCG_NO_OPENMP
     2588             #pragma omp for SCHEDULE(runtime)
     2589             #endif
     2590             		for ( local_int_t i = maxLevelSize; i < tdgLevelSize; i++ ) {
     2591             
     2592             			local_int_t row = A.tdg[l][i];
     2593             			const double * const currentValues = A.matrixValues[row];
     2594             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2595             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2596             			const double currentDiagonal = matrixDiagonal[row][0];
     2597             			svfloat64_t contribs = svdup_f64(0.0);
     2598             
     2599             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2600             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2601             				
     2602             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2603             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2604             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2605             
     2606             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2607             			}
     2608             
     2609             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2610             			double sum = rv[row] - totalContribution;
     2611             
     2612             			sum += xv[row] * currentDiagonal;
     2613             			xv[row] = sum / currentDiagonal;
     2614             		}
     2615             		}
     2616             #ifndef HPCG_NO_OPENMP
     2617             	}
     2618             #endif
     2619             	}
     2620             
     2621             	/*
     2622             	 * BACKWARD SWEEP
     2623             	 */
     2624             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     2625             		local_int_t tdgLevelSize = A.tdg[l].size();
     2626             		local_int_t maxLevelSize = 6*(tdgLevelSize / 6);
     2627             
     2628             #ifndef HPCG_NO_OPENMP
     2629             #pragma omp parallel 
     2630             	{
     2631             		//#pragma omp single nowait 
     2632             		//{
     2633             		#pragma omp for nowait SCHEDULE(runtime)
     2634             #endif
     2635             		for ( local_int_t i = tdgLevelSize-1; i >= maxLevelSize; i-- ) {
     2636             
     2637             			local_int_t row = A.tdg[l][i];
     2638             			const double * const currentValues = A.matrixValues[row];
     2639             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2640             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2641             			const double currentDiagonal = matrixDiagonal[row][0];
     2642             			svfloat64_t contribs = svdup_f64(0.0);
     2643             
     2644             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2645             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2646             				
     2647             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2648             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2649             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2650             
     2651             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2652             			}
     2653             
     2654             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2655             			double sum = rv[row] - totalContribution;
     2656             
     2657             			sum += xv[row] * currentDiagonal;
     2658             			xv[row] = sum / currentDiagonal;
     2659             		}
     2660             #ifndef HPCG_NO_OPENMP
     2661             		//}
     2662             #pragma omp for SCHEDULE(runtime)
     2663             #endif
     2664             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 6 ) {
     2665             			local_int_t row_1 = A.tdg[l][i];
     2666             			local_int_t row_2 = A.tdg[l][i-1];
     2667             			local_int_t row_3 = A.tdg[l][i-2];
     2668             			local_int_t row_4 = A.tdg[l][i-3];
     2669             			local_int_t row_5 = A.tdg[l][i-4];
     2670             			local_int_t row_6 = A.tdg[l][i-5];
     2671             			const double * const currentValues_1 = A.matrixValues[row_1];
     2672             			const double * const currentValues_2 = A.matrixValues[row_2];
     2673             			const double * const currentValues_3 = A.matrixValues[row_3];
     2674             			const double * const currentValues_4 = A.matrixValues[row_4];
     2675             			const double * const currentValues_5 = A.matrixValues[row_5];
     2676             			const double * const currentValues_6 = A.matrixValues[row_6];
     2677             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2678             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2679             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2680             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2681             			const local_int_t * const currentColIndices_5 = A.mtxIndL[row_5];
     2682             			const local_int_t * const currentColIndices_6 = A.mtxIndL[row_6];
     2683             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2684             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2685             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2686             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2687             			const int currentNumberOfNonzeros_5 = A.nonzerosInRow[row_5];
     2688             			const int currentNumberOfNonzeros_6 = A.nonzerosInRow[row_6];
     2689             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2690             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2691             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2692             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2693             			const double currentDiagonal_5 = matrixDiagonal[row_5][0];
     2694             			const double currentDiagonal_6 = matrixDiagonal[row_6][0];
     2695             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2696             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2697             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2698             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2699             			svfloat64_t contribs_5 = svdup_f64(0.0);
     2700             			svfloat64_t contribs_6 = svdup_f64(0.0);
     2701             
     2702             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2703             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);				
     2704             			const int maxNumberOfNonzeros3 = std::max(currentNumberOfNonzeros_5, currentNumberOfNonzeros_6);				
     2705             			const int maxNumberOfNonzeros4 = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);				
     2706             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros3, maxNumberOfNonzeros4);				
     2707             							
     2708             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2709             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2710             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2711             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2712             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2713             				svbool_t pg_5 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_5);
     2714             				svbool_t pg_6 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_6);
     2715             				
     2716             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2717             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2718             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2719             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2720             				svfloat64_t mtxValues_5 = svld1_f64(pg_5, &currentValues_5[j]);
     2721             				svfloat64_t mtxValues_6 = svld1_f64(pg_6, &currentValues_6[j]);
     2722             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2723             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2724             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2725             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2726             				svuint64_t indices_5 = svld1sw_u64(pg_5, &currentColIndices_5[j]);
     2727             				svuint64_t indices_6 = svld1sw_u64(pg_6, &currentColIndices_6[j]);
     2728             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2729             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2730             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2731             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2732             				svfloat64_t xvv_5 = svld1_gather_u64index_f64(pg_5, xv, indices_5);
     2733             				svfloat64_t xvv_6 = svld1_gather_u64index_f64(pg_6, xv, indices_6);
     2734             
     2735             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2736             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2737             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2738             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2739             				contribs_5 = svmla_f64_m(pg_5, contribs_5, xvv_5, mtxValues_5);
     2740             				contribs_6 = svmla_f64_m(pg_6, contribs_6, xvv_6, mtxValues_6);
     2741             			}
     2742             
     2743             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2744             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2745             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2746             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2747             			double totalContribution_5 = svaddv_f64(svptrue_b64(), contribs_5);
     2748             			double totalContribution_6 = svaddv_f64(svptrue_b64(), contribs_6);
     2749             			double sum_1 = rv[row_1] - totalContribution_1;
     2750             			double sum_2 = rv[row_2] - totalContribution_2;
     2751             			double sum_3 = rv[row_3] - totalContribution_3;
     2752             			double sum_4 = rv[row_4] - totalContribution_4;
     2753             			double sum_5 = rv[row_5] - totalContribution_5;
     2754             			double sum_6 = rv[row_6] - totalContribution_6;
     2755             
     2756             			sum_1 += xv[row_1] * currentDiagonal_1;
     2757             			sum_2 += xv[row_2] * currentDiagonal_2;
     2758             			sum_3 += xv[row_3] * currentDiagonal_3;
     2759             			sum_4 += xv[row_4] * currentDiagonal_4;
     2760             			sum_5 += xv[row_5] * currentDiagonal_5;
     2761             			sum_6 += xv[row_6] * currentDiagonal_6;
     2762             			xv[row_1] = sum_1 / currentDiagonal_1;
     2763             			xv[row_2] = sum_2 / currentDiagonal_2;
     2764             			xv[row_3] = sum_3 / currentDiagonal_3;
     2765             			xv[row_4] = sum_4 / currentDiagonal_4;
     2766             			xv[row_5] = sum_5 / currentDiagonal_5;
     2767             			xv[row_6] = sum_6 / currentDiagonal_6;
     2768             		}
     2769             #ifndef HPCG_NO_OPENMP
     2770             	}
     2771             #endif
     2772             	}
     2773             }
Total prefetch num: 0
Optimization messages
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1376: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1380: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1380: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1381: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1381: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1388: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1388: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1388: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1390: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1394: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1394: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1395: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1400: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1404: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1404: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1405: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1405: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1412: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1412: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1412: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1414: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1449: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1454: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1454: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1455: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1455: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1462: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1462: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1462: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1464: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1468: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1468: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1469: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1474: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1479: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1479: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1480: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1480: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1487: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1487: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1487: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1489: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1519: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1525: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1526: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1548: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1548: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1548: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1553: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1566: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1566: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1566: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1569: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1577: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1577: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1577: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1579: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1588: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1594: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1595: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1617: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1617: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1617: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1617: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 32.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1638: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1638: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1638: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1638: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 96.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1639: Method of calculating sum or product is changed.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1640: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1651: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1651: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1651: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1651: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1652: Method of calculating sum or product is changed.
Statistics information
  Option information
    Command line options : -c -DHPCG_CONTIGUOUS_ARRAYS -DHPCG_NO_MPI -DENABLE_MG_COUNTERS -Nfjomplib -DENABLE_MG_COUNTERS -DHPCG_MAN_OPT_SPMV_UNROLL -DTEST_SPMV_AS_TDG -I./src -I./src/OOKAMI_OMP_FJ -Kfast -KSVE -Kopenmp -Koptmsg=2 -Nlst=t -Kocl -I../src -o src/ComputeSYMGS.o
    Effective options    : -g0 -mt -Qy -std=gnu++14 -x- -x=quick -O3 -Knoalias_const
                           -Kalign_loops -Knoarray_declaration_opt -Kassume=noshortloop
                           -Kassume=nomemory_bandwidth -Kassume=notime_saving_compilation
                           -Kcmodel=small -Keval -Keval_noconcurrent
                           -Knoextract_stride_store -Kfast_matmul -Knofenv_access
                           -Kfp_contract -Kfp_relaxed -Kfsimple -Kfz -Khpctag
                           -Kilfunc=procedure -Klargepage -Klib -Kloop_blocking
                           -Kloop_fission -Kloop_nofission_stripmining
                           -Kloop_fission_threshold=50 -Kloop_fusion -Kloop_interchange
                           -Kloop_part_simd -Kloop_perfect_nest -Kloop_noversioning
                           -Klooptype=f -Knomemalias -Kmfunc=1 -Kocl -Komitfp -Kopenmp
                           -Kopenmp_noassume_norecurrence
                           -Kopenmp_nocollapse_except_innermost
                           -Kopenmp_loop_variable=private -Kopenmp_noordered_reduction
                           -Knoopenmp_simd -Knooptlib_string -Koptmsg=2
                           -Knopc_relative_literal_loads -Knoparallel
                           -Kparallel_nofp_precision -Knopreex -Kprefetch_cache_level=all
                           -Kprefetch_noconditional -Kprefetch_noindirect -Kprefetch_noinfer
                           -Kprefetch_sequential=auto -Kprefetch_nostride -Kprefetch_strong
                           -Kprefetch_strong_L2 -Knopreload -Krdconv=1
                           -Kremove_inlinefunction -Knorestp -Ksch_post_ra -Ksch_pre_ra
                           -Ksibling_calls -Ksimd=auto -Ksimd_packed_promotion
                           -Ksimd_reduction_product -Ksimd_reg_size=512
                           -Ksimd_nouncounted_loop -Ksimd_use_multiple_structures
                           -Knostrict_aliasing -Knostriping -KA64FX -KARMV8_3_A -KSVE -Kswp
                           -Kswp_freg_rate=100 -Kswp_ireg_rate=100 -Kswp_preg_rate=100
                           -Kswp_policy=auto -Kunroll -Knounroll_and_jam -Knozfill
                           -Ncancel_overtime_compilation -Nnocoverage -Nexceptions -Nnofjcex
                           -Nfjprof -Nnohook_func -Nnohook_time -Nfjomplib -Nline -Nlst=p
                           -Nlst=t -Nquickdbg=noheapchk -Nquickdbg=nosubchk -NRnotrap
                           -Nnoreordered_variable_stack -Nrt_notune -Nsetvalue=noheap
                           -Nsetvalue=nostack -Nsetvalue=noscalar -Nsetvalue=noarray
                           -Nsetvalue=nostruct -Nsrc -Nsta
