Fujitsu C/C++ Version 4.7.0   Wed Nov  9 04:26:37 2022
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj2
  Source file       : ../src/ComputeSPMV.cpp
(line-no.)(optimize)
        1             /*
        2              *
        3              *  SPDX-License-Identifier: Apache-2.0
        4              *
        5              *  Copyright (C) 2019, Arm Limited and contributors
        6              *
        7              *  Licensed under the Apache License, Version 2.0 (the "License");
        8              *  you may not use this file except in compliance with the License.
        9              *  You may obtain a copy of the License at
       10              *
       11              *      http://www.apache.org/licenses/LICENSE-2.0
       12              *
       13              *  Unless required by applicable law or agreed to in writing, software
       14              *  distributed under the License is distributed on an "AS IS" BASIS,
       15              *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       16              *  See the License for the specific language governing permissions and
       17              *  limitations under the License.
       18              *
       19              */
       20             
       21             
       22             //@HEADER
       23             // ***************************************************
       24             //
       25             // HPCG: High Performance Conjugate Gradient Benchmark
       26             //
       27             // Contact:
       28             // Michael A. Heroux ( maherou@sandia.gov)
       29             // Jack Dongarra     (dongarra@eecs.utk.edu)
       30             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       31             //
       32             // ***************************************************
       33             //@HEADER
       34             
       35             /*!
       36               @file ComputeSPMV.cpp
       37             
       38               HPCG routine
       39               */
       40             
       41             #include "ComputeSPMV.hpp"
       42             #include "ComputeSPMV_ref.hpp"
       43             #include <cassert>
       44             #ifndef HPCG_NO_MPI
       45             #include "ExchangeHalo.hpp"
       46             #endif
       47             #ifdef HPCG_USE_NEON
       48             #include "arm_neon.h"
       49             #endif
       50             #ifdef HPCG_USE_SVE
       51             #include "arm_sve.h"
       52             #endif
       53             #ifdef HPCG_USE_ARMPL_SPMV
       54             #include "armpl_sparse.h"
       55             #endif
       56             
       57             /*!
       58               Routine to compute sparse matrix vector product y = Ax where:
       59             Precondition: First call exchange_externals to get off-processor values of x
       60             
       61             This routine calls the reference SpMV implementation by default, but
       62             can be replaced by a custom, optimized routine suited for
       63             the target system.
       64             
       65             @param[in]  A the known system matrix
       66             @param[in]  x the known vector
       67             @param[out] y the On exit contains the result: Ax.
       68             
       69             @return returns 0 upon success and non-zero otherwise
       70             
       71             @see ComputeSPMV_ref
       72             */
       73             
       74             #ifdef HPCG_MAN_OPT_SCHEDULE_ON
       75             	#define SCHEDULE_CONF	schedule(runtime)
       76             #else
       77             	#define SCHEDULE_CONF
       78             #endif
       79             
       80             int ComputeSPMV( const SparseMatrix & A, Vector & x, Vector & y) {
       81             
       82             	assert(x.localLength >= A.localNumberOfColumns);
       83             	assert(y.localLength >= A.localNumberOfRows);
       84             
       85             #ifndef HPCG_NO_MPI
       86             	ExchangeHalo(A,x);
       87             #endif
       88             	const double * const xv = x.values;
       89             	double * const yv = y.values;
       90             	const local_int_t nrow = A.localNumberOfRows;
       91             
       92             #if defined(HPCG_USE_NEON) && !defined(HPCG_USE_ARMPL_SPMV)
       93             #ifndef HPCG_NO_OPENMP
       94             #pragma omp parallel for SCHEDULE_CONF
       95             #endif
       96             	for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
       97             		float64x2_t sum0 = vdupq_n_f64(0.0);
       98             		float64x2_t sum1 = vdupq_n_f64(0.0);
       99             		if ( A.nonzerosInRow[i] == A.nonzerosInRow[i+1] ) {
      100             			local_int_t j = 0;
      101             			for ( j = 0; j < A.nonzerosInRow[i]-1; j+=2 ) {
      102             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      103             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      104             
      105             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      106             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      107             
      108             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      109             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      110             
      111             			}
      112             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      113             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      114             
      115             			if ( j < A.nonzerosInRow[i] ) {
      116             				s0 += A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
      117             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      118             			}
      119             			yv[i  ] = s0;
      120             			yv[i+1] = s1;
      121             		} else if ( A.nonzerosInRow[i] > A.nonzerosInRow[i+1] ) {
      122             			local_int_t j = 0;
      123             			for ( j = 0; j < A.nonzerosInRow[i+1]-1; j+=2 ) {
      124             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      125             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      126             
      127             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      128             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      129             
      130             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      131             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      132             
      133             			}
      134             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      135             			if ( j < A.nonzerosInRow[i+1] ) {
      136             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      137             			}
      138             
      139             			for ( ; j < A.nonzerosInRow[i]-1; j+=2 ) {
      140             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      141             
      142             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      143             
      144             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      145             			}
      146             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      147             			if ( j < A.nonzerosInRow[i] ) {
      148             				s0 += A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
      149             			}
      150             			yv[i  ] = s0;
      151             			yv[i+1] = s1;
      152             		} else { // A.nonzerosInRow[i] < A.nonzerosInRow[i+1]
      153             			local_int_t j = 0;
      154             			for ( j = 0; j < A.nonzerosInRow[i]-1; j+=2 ) {
      155             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      156             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      157             
      158             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      159             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      160             
      161             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      162             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      163             
      164             			}
      165             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      166             			if ( j < A.nonzerosInRow[i] ) {
      167             				s0 += A.matrixValues[i][j] * xv[A.mtxIndL[i][j]];
      168             			}
      169             
      170             			for ( ; j < A.nonzerosInRow[i+1]-1; j+=2 ) {
      171             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      172             
      173             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      174             
      175             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      176             			}
      177             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      178             			if ( j < A.nonzerosInRow[i+1] ) {
      179             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      180             			}
      181             			yv[i  ] = s0;
      182             			yv[i+1] = s1;
      183             		}
      184             	}
      185             #elif defined(HPCG_USE_SVE) && !defined(HPCG_USE_ARMPL_SPMV)
      186             
      187             	if ( nrow % 4 == 0 ) {
      188             #ifndef HPCG_NO_OPENMP
      189             #pragma omp parallel for SCHEDULE_CONF
      190             #endif
      191             		for ( local_int_t i = 0; i < nrow-3; i+=4 ) {
      192             			local_int_t maxnnz01 = A.nonzerosInRow[i  ] > A.nonzerosInRow[i+1] ? A.nonzerosInRow[i  ] : A.nonzerosInRow[i+1];
      193             			local_int_t maxnnz23 = A.nonzerosInRow[i+2] > A.nonzerosInRow[i+3] ? A.nonzerosInRow[i+2] : A.nonzerosInRow[i+3];
      194             			local_int_t maxnnz = maxnnz01 > maxnnz23 ? maxnnz01 : maxnnz23;
      195             			svfloat64_t svsum0 = svdup_f64(0.0);
      196             			svfloat64_t svsum1 = svdup_f64(0.0);
      197             			svfloat64_t svsum2 = svdup_f64(0.0);
      198             			svfloat64_t svsum3 = svdup_f64(0.0);
      199             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      200             				svbool_t pg0 = svwhilelt_b64(j, A.nonzerosInRow[i  ]);
      201             				svbool_t pg1 = svwhilelt_b64(j, A.nonzerosInRow[i+1]);
      202             				svbool_t pg2 = svwhilelt_b64(j, A.nonzerosInRow[i+2]);
      203             				svbool_t pg3 = svwhilelt_b64(j, A.nonzerosInRow[i+3]);
      204             				svfloat64_t values0 = svld1_f64(pg0, &A.matrixValues[i  ][j]);
      205             				svfloat64_t values1 = svld1_f64(pg1, &A.matrixValues[i+1][j]);
      206             				svfloat64_t values2 = svld1_f64(pg2, &A.matrixValues[i+2][j]);
      207             				svfloat64_t values3 = svld1_f64(pg3, &A.matrixValues[i+3][j]);
      208             				svuint64_t indices0 = svld1sw_u64(pg0, &A.mtxIndL[i  ][j]);
      209             				svuint64_t indices1 = svld1sw_u64(pg1, &A.mtxIndL[i+1][j]);
      210             				svuint64_t indices2 = svld1sw_u64(pg2, &A.mtxIndL[i+2][j]);
      211             				svuint64_t indices3 = svld1sw_u64(pg3, &A.mtxIndL[i+3][j]);
      212             
      213             				svfloat64_t svxv0 = svld1_gather_u64index_f64(pg0, xv, indices0);
      214             				svfloat64_t svxv1 = svld1_gather_u64index_f64(pg1, xv, indices1);
      215             				svfloat64_t svxv2 = svld1_gather_u64index_f64(pg2, xv, indices2);
      216             				svfloat64_t svxv3 = svld1_gather_u64index_f64(pg3, xv, indices3);
      217             				svsum0 = svmla_f64_m(pg0, svsum0, values0, svxv0);
      218             				svsum1 = svmla_f64_m(pg1, svsum1, values1, svxv1);
      219             				svsum2 = svmla_f64_m(pg2, svsum2, values2, svxv2);
      220             				svsum3 = svmla_f64_m(pg3, svsum3, values3, svxv3);
      221             			}
      222             			yv[i  ] = svaddv(svptrue_b64(), svsum0);
      223             			yv[i+1] = svaddv(svptrue_b64(), svsum1);
      224             			yv[i+2] = svaddv(svptrue_b64(), svsum2);
      225             			yv[i+3] = svaddv(svptrue_b64(), svsum3);
      226             		}
      227             	} else if ( nrow % 2 == 0 ) {
      228             #ifndef HPCG_NO_OPENMP
      229             #pragma omp parallel for SCHEDULE_CONF
      230             #endif
      231             		for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
      232             			local_int_t maxnnz = A.nonzerosInRow[i] > A.nonzerosInRow[i+1] ? A.nonzerosInRow[i] : A.nonzerosInRow[i+1];
      233             			svfloat64_t svsum0 = svdup_f64(0.0);
      234             			svfloat64_t svsum1 = svdup_f64(0.0);
      235             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      236             				svbool_t pg0 = svwhilelt_b64(j, A.nonzerosInRow[i]);
      237             				svbool_t pg1 = svwhilelt_b64(j, A.nonzerosInRow[i+1]);
      238             				svfloat64_t values0 = svld1_f64(pg0, &A.matrixValues[i][j]);
      239             				svfloat64_t values1 = svld1_f64(pg1, &A.matrixValues[i+1][j]);
      240             				svuint64_t indices0 = svld1sw_u64(pg0, &A.mtxIndL[i][j]);
      241             				svuint64_t indices1 = svld1sw_u64(pg1, &A.mtxIndL[i+1][j]);
      242             
      243             				svfloat64_t svxv0 = svld1_gather_u64index_f64(pg0, xv, indices0);
      244             				svfloat64_t svxv1 = svld1_gather_u64index_f64(pg1, xv, indices1);
      245             				svsum0 = svmla_f64_m(pg0, svsum0, values0, svxv0);
      246             				svsum1 = svmla_f64_m(pg1, svsum1, values1, svxv1);
      247             			}
      248             			yv[i] = svaddv(svptrue_b64(), svsum0);
      249             			yv[i+1] = svaddv(svptrue_b64(), svsum1);
      250             		}
      251             	} else {
      252             #ifndef HPCG_NO_OPENMP
      253             #pragma omp parallel for SCHEDULE_CONF
      254             #endif
      255             		for ( local_int_t i = 0; i < nrow; i++ ) {
      256             			local_int_t maxnnz = A.nonzerosInRow[i];
      257             			svfloat64_t svsum = svdup_f64(0.0);
      258             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      259             				svbool_t pg = svwhilelt_b64(j, maxnnz);
      260             				svfloat64_t values = svld1_f64(pg, &A.matrixValues[i][j]);
      261             				svuint64_t indices = svld1sw_u64(pg, &A.mtxIndL[i][j]);
      262             
      263             				svfloat64_t svxv = svld1_gather_u64index_f64(pg, xv, indices);
      264             				svsum = svmla_f64_m(pg, svsum, values, svxv);
      265             			}
      266             			yv[i] = svaddv(svptrue_b64(), svsum);
      267             		}
      268             	}
      269             #elif defined(HPCG_USE_ARMPL_SPMV)
      270             	double alpha = 1.0;
      271             	double beta = 0.0;
      272             
      273             	armpl_spmv_exec_d(ARMPL_SPARSE_OPERATION_NOTRANS, alpha, A.armpl_mat, xv, beta, yv);
      274             
      275             #elif defined(HPCG_MAN_OPT_SPMV_UNROLL)
      276             
      277             #ifndef HPCG_NO_OPENMP
      278             #pragma omp parallel for SCHEDULE_CONF
      279             #endif
      280             	for ( local_int_t i = 0; i < nrow-1; ++i ) {
      281             		double sum0 = 0.0;
      282             		double sum1 = 0.0;
      283             		if (A.nonzerosInRow[i] == A.nonzerosInRow[i+1]) {
      284             			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      285             				local_int_t curCol0 = A.mtxIndL[i][j];
      286             				local_int_t curCol1 = A.mtxIndL[i+1][j];
      287             
      288             				sum0 += A.matrixValues[i][j] * xv[curCol0];
      289             				sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      290             			}
      291             			yv[i] = sum0;
      292             			yv[i+1] = sum1;
      293             			++i;
      294             		}
      295             		else {
      296             			double sum = 0.0;
      297             			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      298             				local_int_t curCol = A.mtxIndL[i][j];
      299             				sum += A.matrixValues[i][j] * xv[curCol];
      300             			}
      301             			yv[i] = sum;
      302             			++i;
      303             			//if (i < nrow) {
      304             				sum = 0.0;
      305             				for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      306             					local_int_t curCol = A.mtxIndL[i][j];
      307             					sum += A.matrixValues[i][j] * xv[curCol];
      308             				}
      309             				yv[i] = sum;
      310             			//}
      311             		}
      312             	}
      313             #else
      314             #ifndef HPCG_NO_OPENMP
      315             #pragma omp parallel for SCHEDULE_CONF
      316             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      317   p         	for ( local_int_t i = 0; i < nrow; i++ ) {
      318   p         		double sum = 0.0;
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      319   p     8v  		for ( local_int_t j = 0; j < A.nonzerosInRow[i]; j++ ) {
      320   p     8v  			local_int_t curCol = A.mtxIndL[i][j];
      321   p     8v  			sum += A.matrixValues[i][j] * xv[curCol];
      322   p     8v  		}
      323   p         		yv[i] = sum;
      324   p         	}
      325             #endif
      326             
      327             	return 0;
      328             }
Total prefetch num: 0
Optimization messages
  jwd6004s-i  "../src/ComputeSPMV.cpp", line 319: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSPMV.cpp", line 319: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSPMV.cpp", line 319: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSPMV.cpp", line 321: Method of calculating sum or product is changed.
Statistics information
  Option information
    Command line options : -c -DHPCG_CONTIGUOUS_ARRAYS -DHPCG_NO_MPI -DHPCG_MAN_OPT_DDOT -DDDOT_INTRINSICS -DDDOT_4_UNROLL -I./src -I./src/OOKAMI_OMP_FJ -Kfast -KSVE -Kopenmp -ffast-math -funroll-loops -std=c++11 -ffp-contract=fast -march=armv8.2-a+sve -Koptmsg=2 -Nlst=t -I../src -o src/ComputeSPMV.o
    Effective options    : -g0 -mt -Qy -std=gnu++11 -x- -x=quick -O3 -Knoalias_const
                           -Kalign_loops -Knoarray_declaration_opt -Kassume=noshortloop
                           -Kassume=nomemory_bandwidth -Kassume=notime_saving_compilation
                           -Kcmodel=small -Keval -Keval_noconcurrent
                           -Knoextract_stride_store -Kfast_matmul -Knofenv_access
                           -Kfp_contract -Kfp_relaxed -Kfsimple -Kfz -Khpctag
                           -Kilfunc=procedure -Klargepage -Klib -Kloop_blocking
                           -Kloop_fission -Kloop_nofission_stripmining
                           -Kloop_fission_threshold=50 -Kloop_fusion -Kloop_interchange
                           -Kloop_part_simd -Kloop_perfect_nest -Kloop_noversioning
                           -Klooptype=f -Knomemalias -Kmfunc=1 -Knoocl -Komitfp -Kopenmp
                           -Kopenmp_noassume_norecurrence
                           -Kopenmp_nocollapse_except_innermost
                           -Kopenmp_loop_variable=private -Kopenmp_noordered_reduction
                           -Knoopenmp_simd -Knooptlib_string -Koptmsg=2
                           -Knopc_relative_literal_loads -Knoparallel
                           -Kparallel_nofp_precision -Knopreex -Kprefetch_cache_level=all
                           -Kprefetch_noconditional -Kprefetch_noindirect -Kprefetch_noinfer
                           -Kprefetch_sequential=auto -Kprefetch_nostride -Kprefetch_strong
                           -Kprefetch_strong_L2 -Knopreload -Krdconv=1
                           -Kremove_inlinefunction -Knorestp -Ksch_post_ra -Ksch_pre_ra
                           -Ksibling_calls -Ksimd=auto -Ksimd_packed_promotion
                           -Ksimd_reduction_product -Ksimd_reg_size=512
                           -Ksimd_nouncounted_loop -Ksimd_use_multiple_structures
                           -Knostrict_aliasing -Knostriping -KA64FX -KARMV8_2_A -KSVE -Kswp
                           -Kswp_freg_rate=100 -Kswp_ireg_rate=100 -Kswp_preg_rate=100
                           -Kswp_policy=auto -Kunroll -Knounroll_and_jam -Knozfill
                           -Ncancel_overtime_compilation -Nnocoverage -Nexceptions -Nnofjcex
                           -Nfjprof -Nnohook_func -Nnohook_time -Nlibomp -Nline -Nlst=p
                           -Nlst=t -Nquickdbg=noheapchk -Nquickdbg=nosubchk -NRnotrap
                           -Nnoreordered_variable_stack -Nrt_notune -Nsetvalue=noheap
                           -Nsetvalue=nostack -Nsetvalue=noscalar -Nsetvalue=noarray
                           -Nsetvalue=nostruct -Nsrc -Nsta
