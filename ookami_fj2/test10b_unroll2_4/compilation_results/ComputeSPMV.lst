Fujitsu C/C++ Version 4.7.0   Thu Nov 17 06:52:33 2022
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj2
  Source file       : ../src/ComputeSPMV.cpp
(line-no.)(optimize)
        1             /*
        2              *
        3              *  SPDX-License-Identifier: Apache-2.0
        4              *
        5              *  Copyright (C) 2019, Arm Limited and contributors
        6              *
        7              *  Licensed under the Apache License, Version 2.0 (the "License");
        8              *  you may not use this file except in compliance with the License.
        9              *  You may obtain a copy of the License at
       10              *
       11              *      http://www.apache.org/licenses/LICENSE-2.0
       12              *
       13              *  Unless required by applicable law or agreed to in writing, software
       14              *  distributed under the License is distributed on an "AS IS" BASIS,
       15              *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       16              *  See the License for the specific language governing permissions and
       17              *  limitations under the License.
       18              *
       19              */
       20             
       21             
       22             //@HEADER
       23             // ***************************************************
       24             //
       25             // HPCG: High Performance Conjugate Gradient Benchmark
       26             //
       27             // Contact:
       28             // Michael A. Heroux ( maherou@sandia.gov)
       29             // Jack Dongarra     (dongarra@eecs.utk.edu)
       30             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       31             //
       32             // ***************************************************
       33             //@HEADER
       34             
       35             /*!
       36               @file ComputeSPMV.cpp
       37             
       38               HPCG routine
       39               */
       40             
       41             #include "ComputeSPMV.hpp"
       42             #include "ComputeSPMV_ref.hpp"
       43             #include <cassert>
       44             #ifndef HPCG_NO_MPI
       45             #include "ExchangeHalo.hpp"
       46             #endif
       47             #ifdef HPCG_USE_NEON
       48             #include "arm_neon.h"
       49             #endif
       50             #ifdef HPCG_USE_SVE
       51             #include "arm_sve.h"
       52             #endif
       53             #ifdef HPCG_USE_ARMPL_SPMV
       54             #include "armpl_sparse.h"
       55             #endif
       56             
       57             /*!
       58               Routine to compute sparse matrix vector product y = Ax where:
       59             Precondition: First call exchange_externals to get off-processor values of x
       60             
       61             This routine calls the reference SpMV implementation by default, but
       62             can be replaced by a custom, optimized routine suited for
       63             the target system.
       64             
       65             @param[in]  A the known system matrix
       66             @param[in]  x the known vector
       67             @param[out] y the On exit contains the result: Ax.
       68             
       69             @return returns 0 upon success and non-zero otherwise
       70             
       71             @see ComputeSPMV_ref
       72             */
       73             
       74             #ifdef HPCG_MAN_OPT_SCHEDULE_ON
       75             	#define SCHEDULE(T)	schedule(T)
       76             #else
       77             	#define SCHEDULE(T)
       78             #endif
       79             
       80             #ifdef SPMV_2_UNROLL
       81             #define ComputeSPMV_unroll2 ComputeSPMV_manual
       82             #elif defined SPMV_4_UNROLL
       83             #define ComputeSPMV_unroll4 ComputeSPMV_manual
       84             #endif
       85             inline void ComputeSPMV_manual(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow);
       86             
       87             int ComputeSPMV( const SparseMatrix & A, Vector & x, Vector & y) {
       88             
       89             	assert(x.localLength >= A.localNumberOfColumns);
       90             	assert(y.localLength >= A.localNumberOfRows);
       91             
       92             #ifndef HPCG_NO_MPI
       93             	ExchangeHalo(A,x);
       94             #endif
       95             	const double * const xv = x.values;
       96             	double * const yv = y.values;
       97             	const local_int_t nrow = A.localNumberOfRows;
       98             
       99             #if defined(HPCG_USE_NEON) && !defined(HPCG_USE_ARMPL_SPMV)
      100             #ifndef HPCG_NO_OPENMP
      101             #pragma omp parallel for SCHEDULE(runtime)
      102             #endif
      103             	for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
      104             		float64x2_t sum0 = vdupq_n_f64(0.0);
      105             		float64x2_t sum1 = vdupq_n_f64(0.0);
      106             		if ( A.nonzerosInRow[i] == A.nonzerosInRow[i+1] ) {
      107             			local_int_t j = 0;
      108             			for ( j = 0; j < A.nonzerosInRow[i]-1; j+=2 ) {
      109             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      110             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      111             
      112             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      113             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      114             
      115             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      116             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      117             
      118             			}
      119             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      120             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      121             
      122             			if ( j < A.nonzerosInRow[i] ) {
      123             				s0 += A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
      124             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      125             			}
      126             			yv[i  ] = s0;
      127             			yv[i+1] = s1;
      128             		} else if ( A.nonzerosInRow[i] > A.nonzerosInRow[i+1] ) {
      129             			local_int_t j = 0;
      130             			for ( j = 0; j < A.nonzerosInRow[i+1]-1; j+=2 ) {
      131             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      132             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      133             
      134             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      135             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      136             
      137             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      138             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      139             
      140             			}
      141             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      142             			if ( j < A.nonzerosInRow[i+1] ) {
      143             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      144             			}
      145             
      146             			for ( ; j < A.nonzerosInRow[i]-1; j+=2 ) {
      147             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      148             
      149             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      150             
      151             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      152             			}
      153             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      154             			if ( j < A.nonzerosInRow[i] ) {
      155             				s0 += A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
      156             			}
      157             			yv[i  ] = s0;
      158             			yv[i+1] = s1;
      159             		} else { // A.nonzerosInRow[i] < A.nonzerosInRow[i+1]
      160             			local_int_t j = 0;
      161             			for ( j = 0; j < A.nonzerosInRow[i]-1; j+=2 ) {
      162             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      163             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      164             
      165             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      166             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      167             
      168             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      169             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      170             
      171             			}
      172             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      173             			if ( j < A.nonzerosInRow[i] ) {
      174             				s0 += A.matrixValues[i][j] * xv[A.mtxIndL[i][j]];
      175             			}
      176             
      177             			for ( ; j < A.nonzerosInRow[i+1]-1; j+=2 ) {
      178             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      179             
      180             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      181             
      182             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      183             			}
      184             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      185             			if ( j < A.nonzerosInRow[i+1] ) {
      186             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      187             			}
      188             			yv[i  ] = s0;
      189             			yv[i+1] = s1;
      190             		}
      191             	}
      192             #elif defined(HPCG_USE_SVE) && !defined(HPCG_USE_ARMPL_SPMV)
      193             
      194             	if ( nrow % 4 == 0 ) {
      195             #ifndef HPCG_NO_OPENMP
      196             #pragma omp parallel for SCHEDULE(runtime)
      197             #endif
      198             		for ( local_int_t i = 0; i < nrow-3; i+=4 ) {
      199             			local_int_t maxnnz01 = A.nonzerosInRow[i  ] > A.nonzerosInRow[i+1] ? A.nonzerosInRow[i  ] : A.nonzerosInRow[i+1];
      200             			local_int_t maxnnz23 = A.nonzerosInRow[i+2] > A.nonzerosInRow[i+3] ? A.nonzerosInRow[i+2] : A.nonzerosInRow[i+3];
      201             			local_int_t maxnnz = maxnnz01 > maxnnz23 ? maxnnz01 : maxnnz23;
      202             			svfloat64_t svsum0 = svdup_f64(0.0);
      203             			svfloat64_t svsum1 = svdup_f64(0.0);
      204             			svfloat64_t svsum2 = svdup_f64(0.0);
      205             			svfloat64_t svsum3 = svdup_f64(0.0);
      206             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      207             				svbool_t pg0 = svwhilelt_b64(j, A.nonzerosInRow[i  ]);
      208             				svbool_t pg1 = svwhilelt_b64(j, A.nonzerosInRow[i+1]);
      209             				svbool_t pg2 = svwhilelt_b64(j, A.nonzerosInRow[i+2]);
      210             				svbool_t pg3 = svwhilelt_b64(j, A.nonzerosInRow[i+3]);
      211             				svfloat64_t values0 = svld1_f64(pg0, &A.matrixValues[i  ][j]);
      212             				svfloat64_t values1 = svld1_f64(pg1, &A.matrixValues[i+1][j]);
      213             				svfloat64_t values2 = svld1_f64(pg2, &A.matrixValues[i+2][j]);
      214             				svfloat64_t values3 = svld1_f64(pg3, &A.matrixValues[i+3][j]);
      215             				svuint64_t indices0 = svld1sw_u64(pg0, &A.mtxIndL[i  ][j]);
      216             				svuint64_t indices1 = svld1sw_u64(pg1, &A.mtxIndL[i+1][j]);
      217             				svuint64_t indices2 = svld1sw_u64(pg2, &A.mtxIndL[i+2][j]);
      218             				svuint64_t indices3 = svld1sw_u64(pg3, &A.mtxIndL[i+3][j]);
      219             
      220             				svfloat64_t svxv0 = svld1_gather_u64index_f64(pg0, xv, indices0);
      221             				svfloat64_t svxv1 = svld1_gather_u64index_f64(pg1, xv, indices1);
      222             				svfloat64_t svxv2 = svld1_gather_u64index_f64(pg2, xv, indices2);
      223             				svfloat64_t svxv3 = svld1_gather_u64index_f64(pg3, xv, indices3);
      224             				svsum0 = svmla_f64_m(pg0, svsum0, values0, svxv0);
      225             				svsum1 = svmla_f64_m(pg1, svsum1, values1, svxv1);
      226             				svsum2 = svmla_f64_m(pg2, svsum2, values2, svxv2);
      227             				svsum3 = svmla_f64_m(pg3, svsum3, values3, svxv3);
      228             			}
      229             			yv[i  ] = svaddv(svptrue_b64(), svsum0);
      230             			yv[i+1] = svaddv(svptrue_b64(), svsum1);
      231             			yv[i+2] = svaddv(svptrue_b64(), svsum2);
      232             			yv[i+3] = svaddv(svptrue_b64(), svsum3);
      233             		}
      234             	} else if ( nrow % 2 == 0 ) {
      235             #ifndef HPCG_NO_OPENMP
      236             #pragma omp parallel for SCHEDULE(runtime)
      237             #endif
      238             		for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
      239             			local_int_t maxnnz = A.nonzerosInRow[i] > A.nonzerosInRow[i+1] ? A.nonzerosInRow[i] : A.nonzerosInRow[i+1];
      240             			svfloat64_t svsum0 = svdup_f64(0.0);
      241             			svfloat64_t svsum1 = svdup_f64(0.0);
      242             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      243             				svbool_t pg0 = svwhilelt_b64(j, A.nonzerosInRow[i]);
      244             				svbool_t pg1 = svwhilelt_b64(j, A.nonzerosInRow[i+1]);
      245             				svfloat64_t values0 = svld1_f64(pg0, &A.matrixValues[i][j]);
      246             				svfloat64_t values1 = svld1_f64(pg1, &A.matrixValues[i+1][j]);
      247             				svuint64_t indices0 = svld1sw_u64(pg0, &A.mtxIndL[i][j]);
      248             				svuint64_t indices1 = svld1sw_u64(pg1, &A.mtxIndL[i+1][j]);
      249             
      250             				svfloat64_t svxv0 = svld1_gather_u64index_f64(pg0, xv, indices0);
      251             				svfloat64_t svxv1 = svld1_gather_u64index_f64(pg1, xv, indices1);
      252             				svsum0 = svmla_f64_m(pg0, svsum0, values0, svxv0);
      253             				svsum1 = svmla_f64_m(pg1, svsum1, values1, svxv1);
      254             			}
      255             			yv[i] = svaddv(svptrue_b64(), svsum0);
      256             			yv[i+1] = svaddv(svptrue_b64(), svsum1);
      257             		}
      258             	} else {
      259             #ifndef HPCG_NO_OPENMP
      260             #pragma omp parallel for SCHEDULE(runtime)
      261             #endif
      262             		for ( local_int_t i = 0; i < nrow; i++ ) {
      263             			local_int_t maxnnz = A.nonzerosInRow[i];
      264             			svfloat64_t svsum = svdup_f64(0.0);
      265             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      266             				svbool_t pg = svwhilelt_b64(j, maxnnz);
      267             				svfloat64_t values = svld1_f64(pg, &A.matrixValues[i][j]);
      268             				svuint64_t indices = svld1sw_u64(pg, &A.mtxIndL[i][j]);
      269             
      270             				svfloat64_t svxv = svld1_gather_u64index_f64(pg, xv, indices);
      271             				svsum = svmla_f64_m(pg, svsum, values, svxv);
      272             			}
      273             			yv[i] = svaddv(svptrue_b64(), svsum);
      274             		}
      275             	}
      276             #elif defined(HPCG_USE_ARMPL_SPMV)
      277             	double alpha = 1.0;
      278             	double beta = 0.0;
      279             
      280             	armpl_spmv_exec_d(ARMPL_SPARSE_OPERATION_NOTRANS, alpha, A.armpl_mat, xv, beta, yv);
      281             
      282             #elif defined(HPCG_MAN_OPT_SPMV_UNROLL)
      283             	ComputeSPMV_manual(A, xv, yv, nrow);
      284             /*#ifndef HPCG_NO_OPENMP
      285             #pragma omp parallel for SCHEDULE(runtime)
      286             #endif
      287             	for ( local_int_t i = 0; i < nrow-1; ++i ) {
      288             		double sum0 = 0.0;
      289             		double sum1 = 0.0;
      290             		if (A.nonzerosInRow[i] == A.nonzerosInRow[i+1]) {
      291             			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      292             				local_int_t curCol0 = A.mtxIndL[i][j];
      293             				local_int_t curCol1 = A.mtxIndL[i+1][j];
      294             
      295             				sum0 += A.matrixValues[i][j] * xv[curCol0];
      296             				sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      297             			}
      298             			yv[i] = sum0;
      299             			yv[i+1] = sum1;
      300             			++i;
      301             		}
      302             		else {
      303             			double sum = 0.0;
      304             			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      305             				local_int_t curCol = A.mtxIndL[i][j];
      306             				sum += A.matrixValues[i][j] * xv[curCol];
      307             			}
      308             			yv[i] = sum;
      309             			++i;
      310             			//if (i < nrow) {
      311             				sum = 0.0;
      312             				for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      313             					local_int_t curCol = A.mtxIndL[i][j];
      314             					sum += A.matrixValues[i][j] * xv[curCol];
      315             				}
      316             				yv[i] = sum;
      317             			//}
      318             		}
      319             	}*/
      320             #else
      321             #ifndef HPCG_NO_OPENMP
      322             #pragma omp parallel for SCHEDULE(runtime)
      323             #endif
      324             	for ( local_int_t i = 0; i < nrow; i++ ) {
      325             		double sum = 0.0;
      326             		for ( local_int_t j = 0; j < A.nonzerosInRow[i]; j++ ) {
      327             			local_int_t curCol = A.mtxIndL[i][j];
      328             			sum += A.matrixValues[i][j] * xv[curCol];
      329             		}
      330             		yv[i] = sum;
      331             	}
      332             #endif
      333             
      334             	return 0;
      335             }
      336             
      337             #ifdef HPCG_MAN_OPT_SPMV_UNROLL
      338             inline void ComputeSPMV_unroll2(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow) {
      339             #pragma fj loop zfill
      340             #pragma statement scache_isolate_way L2=10
      341             #pragma statement scache_isolate_assign xv
      342             
      343             #ifndef HPCG_NO_OPENMP
      344             #pragma omp parallel for SCHEDULE(runtime)
      345             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      346   p         	for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
      347   p         		double sum0 = 0.0;
      348   p         		double sum1 = 0.0;
      349   p         		if (A.nonzerosInRow[i] == A.nonzerosInRow[i+1]) {
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.22, ITR: 96, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      350   p     4v  			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      351   p     4v  				local_int_t curCol0 = A.mtxIndL[i][j];
      352   p     4v  				local_int_t curCol1 = A.mtxIndL[i+1][j];
      353             
      354   p     4v  				sum0 += A.matrixValues[i][j] * xv[curCol0];
      355   p     4v  				sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      356   p     4v  			}
      357   p         		}
      358   p         		else {
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      359   p     8v  			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      360   p     8v  				local_int_t curCol = A.mtxIndL[i][j];
      361   p     8v  				sum0 += A.matrixValues[i][j] * xv[curCol];
      362   p     8v  			}
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      363   p     8v  			for ( local_int_t j = 0; j < A.nonzerosInRow[i+1]; ++j ) {
      364   p     8v  				local_int_t curCol = A.mtxIndL[i+1][j];
      365   p     8v  				sum1 += A.matrixValues[i+1][j] * xv[curCol];
      366   p     8v  			}
      367   p         		}
      368   p         		yv[i] = sum0;
      369   p         		yv[i+1] = sum1;
      370   p         	}
      371             	#pragma statement end_scache_isolate_assign
      372             	#pragma statement end_scache_isolate_way
      373             }
      374             
      375             inline void ComputeSPMV_unroll4(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow) {
      376             	#pragma fj loop zfill
      377             	#pragma statement scache_isolate_way L2=10
      378             	#pragma statement scache_isolate_assign xv
      379             
      380             #ifndef HPCG_NO_OPENMP
      381             #pragma omp parallel for SCHEDULE(runtime)
      382             #endif
      383             	for ( local_int_t i = 0; i < nrow-3; i+=4) {
      384             		double sum0 = 0.0, sum1 = 0.0, sum2 = 0.0, sum3 = 0.0;
      385             
      386             		if ((A.nonzerosInRow[i] == A.nonzerosInRow[i+1]) && (A.nonzerosInRow[i] == A.nonzerosInRow[i+2]) && (A.nonzerosInRow[i] == A.nonzerosInRow[i+3]) ){
      387             			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      388             				local_int_t curCol0 = A.mtxIndL[i][j];
      389             				local_int_t curCol1 = A.mtxIndL[i+1][j];
      390             				local_int_t curCol2 = A.mtxIndL[i+2][j];
      391             				local_int_t curCol3 = A.mtxIndL[i+3][j];
      392             
      393             				sum0 += A.matrixValues[i][j] * xv[curCol0];
      394             				sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      395             				sum2 += A.matrixValues[i+2][j] * xv[curCol2];
      396             				sum3 += A.matrixValues[i+3][j] * xv[curCol3];
      397             			}
      398             
      399             			yv[i] = sum0;
      400             			yv[i+1] = sum1;
      401             			yv[i+2] = sum2;
      402             			yv[i+3] = sum3;
      403             		}
      404             		else {
      405             			for ( local_int_t ix = 0; ix < 4; ++ix) {
      406             				double sum = 0.0;
      407             				for ( local_int_t j = 0; j < A.nonzerosInRow[i+ix]; ++j ) {
      408             					local_int_t curCol = A.mtxIndL[i+ix][j];
      409             					sum += A.matrixValues[i+ix][j] * xv[curCol];
      410             				}
      411             				yv[i+ix] = sum;
      412             			}
      413             		}
      414             /*
      415             		local_int_t max1 = A.nonzerosInRow[i  ];
      416             		local_int_t min1 = A.nonzerosInRow[i+1];
      417             		local_int_t tmp = max1;
      418             		if (max1 < min1) { max1 = min1; min1 = tmp; } 
      419             
      420             		local_int_t max2 = A.nonzerosInRow[i+2];
      421             		local_int_t min2 = A.nonzerosInRow[i+3];
      422             		tmp = max2;
      423             		if (max2 < min2) { max2 = min2; min2 = tmp; }
      424             
      425             		local_int_t max = max2 > max1 ? max2 : max1;
      426             		local_int_t min = min1 < min2 ? min1 : min2;
      427             
      428             		for ( local_int_t j = 0; j < min; ++j ) {
      429             			local_int_t curCol0 = A.mtxIndL[i][j];
      430             			local_int_t curCol1 = A.mtxIndL[i+1][j];
      431             			local_int_t curCol2 = A.mtxIndL[i+2][j];
      432             			local_int_t curCol3 = A.mtxIndL[i+3][j];
      433             
      434             			sum0 += A.matrixValues[i][j] * xv[curCol0];
      435             			sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      436             			sum2 += A.matrixValues[i+2][j] * xv[curCol2];
      437             			sum3 += A.matrixValues[i+3][j] * xv[curCol3];
      438             		}
      439             		for ( local_int_t j=min; j < max; ++j ) {
      440             			local_int_t curCol0 = (j<A.nonzerosInRow[i  ]) ? A.mtxIndL[i][j] : 0;
      441             			local_int_t curCol1 = (j<A.nonzerosInRow[i+1]) ? A.mtxIndL[i+1][j] : 0;
      442             			local_int_t curCol2 = (j<A.nonzerosInRow[i+2]) ? A.mtxIndL[i+2][j] : 0;
      443             			local_int_t curCol3 = (j<A.nonzerosInRow[i+3]) ? A.mtxIndL[i+3][j] : 0;
      444             
      445             			sum0 += A.matrixValues[i][j] * xv[curCol0];
      446             			sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      447             			sum2 += A.matrixValues[i+2][j] * xv[curCol2];
      448             			sum3 += A.matrixValues[i+3][j] * xv[curCol3];
      449             		}
      450             
      451             		yv[i] = sum0;
      452             		yv[i+1] = sum1;
      453             		yv[i+2] = sum2;
      454             		yv[i+3] = sum3;*/
      455             	}
      456                 #pragma statement end_scache_isolate_assign
      457                 #pragma statement end_scache_isolate_way		
      458             }
      459             #endif //HPCG_MAN_OPT_SPMV_UNROLL
Total prefetch num: 0
Optimization messages
  jwd6004s-i  "../src/ComputeSPMV.cpp", line 350: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSPMV.cpp", line 350: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSPMV.cpp", line 350: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 96.
  jwd8208o-i  "../src/ComputeSPMV.cpp", line 354: Method of calculating sum or product is changed.
  jwd8208o-i  "../src/ComputeSPMV.cpp", line 355: Method of calculating sum or product is changed.
  jwd6004s-i  "../src/ComputeSPMV.cpp", line 359: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSPMV.cpp", line 359: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSPMV.cpp", line 359: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSPMV.cpp", line 361: Method of calculating sum or product is changed.
  jwd6004s-i  "../src/ComputeSPMV.cpp", line 363: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSPMV.cpp", line 363: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSPMV.cpp", line 363: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSPMV.cpp", line 365: Method of calculating sum or product is changed.
Statistics information
  Option information
    Command line options : -c -DHPCG_CONTIGUOUS_ARRAYS -DHPCG_NO_MPI -DHPCG_MAN_OPT_SPMV_UNROLL -DSPMV_2_UNROLL -Khpctag -DHPCG_MAN_OPT_SPMV_UNROLL -I./src -I./src/OOKAMI_OMP_FJ -I/lustre/software/arm/22.1/armpl-22.1.0_AArch64_RHEL-8_arm-linux-compiler_aarch64-linux/include -Kfast -KSVE -Kopenmp -ffast-math -funroll-loops -std=c++11 -ffp-contract=fast -march=armv8.2-a+sve -Koptmsg=2 -Nlst=t -I../src -o src/ComputeSPMV.o
    Effective options    : -g0 -mt -Qy -std=gnu++11 -x- -x=quick -O3 -Knoalias_const
                           -Kalign_loops -Knoarray_declaration_opt -Kassume=noshortloop
                           -Kassume=nomemory_bandwidth -Kassume=notime_saving_compilation
                           -Kcmodel=small -Keval -Keval_noconcurrent
                           -Knoextract_stride_store -Kfast_matmul -Knofenv_access
                           -Kfp_contract -Kfp_relaxed -Kfsimple -Kfz -Khpctag
                           -Kilfunc=procedure -Klargepage -Klib -Kloop_blocking
                           -Kloop_fission -Kloop_nofission_stripmining
                           -Kloop_fission_threshold=50 -Kloop_fusion -Kloop_interchange
                           -Kloop_part_simd -Kloop_perfect_nest -Kloop_noversioning
                           -Klooptype=f -Knomemalias -Kmfunc=1 -Knoocl -Komitfp -Kopenmp
                           -Kopenmp_noassume_norecurrence
                           -Kopenmp_nocollapse_except_innermost
                           -Kopenmp_loop_variable=private -Kopenmp_noordered_reduction
                           -Knoopenmp_simd -Knooptlib_string -Koptmsg=2
                           -Knopc_relative_literal_loads -Knoparallel
                           -Kparallel_nofp_precision -Knopreex -Kprefetch_cache_level=all
                           -Kprefetch_noconditional -Kprefetch_noindirect -Kprefetch_noinfer
                           -Kprefetch_sequential=auto -Kprefetch_nostride -Kprefetch_strong
                           -Kprefetch_strong_L2 -Knopreload -Krdconv=1
                           -Kremove_inlinefunction -Knorestp -Ksch_post_ra -Ksch_pre_ra
                           -Ksibling_calls -Ksimd=auto -Ksimd_packed_promotion
                           -Ksimd_reduction_product -Ksimd_reg_size=512
                           -Ksimd_nouncounted_loop -Ksimd_use_multiple_structures
                           -Knostrict_aliasing -Knostriping -KA64FX -KARMV8_2_A -KSVE -Kswp
                           -Kswp_freg_rate=100 -Kswp_ireg_rate=100 -Kswp_preg_rate=100
                           -Kswp_policy=auto -Kunroll -Knounroll_and_jam -Knozfill
                           -Ncancel_overtime_compilation -Nnocoverage -Nexceptions -Nnofjcex
                           -Nfjprof -Nnohook_func -Nnohook_time -Nlibomp -Nline -Nlst=p
                           -Nlst=t -Nquickdbg=noheapchk -Nquickdbg=nosubchk -NRnotrap
                           -Nnoreordered_variable_stack -Nrt_notune -Nsetvalue=noheap
                           -Nsetvalue=nostack -Nsetvalue=noscalar -Nsetvalue=noarray
                           -Nsetvalue=nostruct -Nsrc -Nsta
