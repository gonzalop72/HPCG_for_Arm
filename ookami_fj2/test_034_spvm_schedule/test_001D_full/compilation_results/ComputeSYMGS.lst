Fujitsu C/C++ Version 4.7.0   Mon Jul 17 14:57:24 2023
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj2
  Source file       : ../src/ComputeSYMGS.cpp
(line-no.)(optimize)
        1             /*
        2              *
        3              *  SPDX-License-Identifier: Apache-2.0
        4              *
        5              *  Copyright (C) 2019, Arm Limited and contributors
        6              *
        7              *  Licensed under the Apache License, Version 2.0 (the "License");
        8              *  you may not use this file except in compliance with the License.
        9              *  You may obtain a copy of the License at
       10              *
       11              *      http://www.apache.org/licenses/LICENSE-2.0
       12              *
       13              *  Unless required by applicable law or agreed to in writing, software
       14              *  distributed under the License is distributed on an "AS IS" BASIS,
       15              *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       16              *  See the License for the specific language governing permissions and
       17              *  limitations under the License.
       18              *
       19              */
       20             
       21             //@HEADER
       22             // ***************************************************
       23             //
       24             // HPCG: High Performance Conjugate Gradient Benchmark
       25             //
       26             // Contact:
       27             // Michael A. Heroux ( maherou@sandia.gov)
       28             // Jack Dongarra     (dongarra@eecs.utk.edu)
       29             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       30             //
       31             // ***************************************************
       32             //@HEADER
       33             
       34             /*!
       35              @file ComputeSYMGS.cpp
       36             
       37              HPCG routine
       38              */
       39             
       40             #include "ComputeSYMGS.hpp"
       41             #include "ComputeSYMGS_ref.hpp"
       42             #ifndef HPCG_NO_MPI
       43             #include "ExchangeHalo.hpp"
       44             #endif
       45             
       46             #include "likwid_instrumentation.hpp"
       47             
       48             #ifdef HPCG_MAN_OPT_SCHEDULE_ON
       49             	#define SCHEDULE(T)	schedule(T)
       50             #else
       51             	#define SCHEDULE(T)
       52             #endif
       53             
       54             #define xxxxMANUAL_TASK_DISTRIBUTION
       55             #ifndef HPCG_NO_OPENMP
       56             	#include "omp.h"
       57             #endif
       58             
       59             /**************************************************************************************************/
       60             /**************************************************************************************************/
       61             /**************************************************************************************************/
       62             /* SVE IMPLEMENTATIONS                                                                            */
       63             /**************************************************************************************************/
       64             /**************************************************************************************************/
       65             /**************************************************************************************************/
       66             
       67             #include "arm_sve.h"
       68             #ifdef HPCG_USE_SVE
       69             #include "arm_sve.h"
       70             
       71             #ifdef UNROLLING_4_A
       72             #elif defined(UNROLLING_4_B)
       73             	#define MANUAL_TASK_DISTRIBUTION
       74             #elif defined(UNROLLING_6_A)
       75             #elif defined(UNROLLING_6_B)
       76             #elif defined(UNROLLING_6_C)
       77             	#define MANUAL_TASK_DISTRIBUTION
       78             #elif defined(REF_UNROLLING_4)
       79             #else
       80             #endif
       81             
       82             
       83             inline void SYMGS_VERSION_1(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-2
       84             inline void SYMGS_VERSION_2(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-2 V2
       85             inline void SYMGS_VERSION_3(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-4 - OPTIMUM
       86             inline void SYMGS_VERSION_4(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-6
       87             #include "ComputeSYMGS_OPT.cpp"
       88             #include "ComputeSYMGS_OPT2.cpp" 
       89             
       90             /*
       91              * TDG VERSION
       92              */
       93             int ComputeSYMGS_TDG_SVE(const SparseMatrix & A, const Vector & r, Vector & x, TraceData &trace) {
       94             	assert(x.localLength == A.localNumberOfColumns);
       95             
       96             #ifndef HPCG_NO_MPI
       97             	ExchangeHalo(A, x);
       98             #endif
       99             
      100             	const double * const rv = r.values;
      101             	double * const xv = x.values;
      102             	double **matrixDiagonal = A.matrixDiagonal;
      103             
      104             LIKWID_START(trace.enabled, "symgs_tdg");
      105             
      106             #ifdef UNROLLING_4_A
      107             	SYMGS_VERSION_5(A, xv, rv); 
      108             #elif defined(UNROLLING_4_B)
      109             	SYMGS_VERSION_5(A, xv, rv); 
      110             #elif defined(UNROLLING_6_A)
      111             	SYMGS_VERSION_4(A, xv, rv); 
      112             #elif defined(UNROLLING_6_B)
      113             	SYMGS_VERSION_6(A, xv, rv); 
      114             #elif defined(UNROLLING_6_C)
      115             	SYMGS_VERSION_6(A, xv, rv); 
      116             #elif defined(REF_UNROLLING_4)
      117             	SYMGS_VERSION_3(A, xv, rv); 
      118             #else
      119             
      120             //#pragma statement scache_isolate_way L2=10
      121             //#pragma statement scache_isolate_assign xv
      122             	/*
      123             	 * FORWARD SWEEP
      124             	 */
      125    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      126             
      127             		local_int_t totalSize = A.tdg[l].size();
      128             		local_int_t size1 = 2*(totalSize/2);
      129             		//#pragma loop nounroll
      130             		//#pragma loop nounroll_and_jam
      131             		//if((A.tdg[l].size()%2) == 0) {
      132             #ifndef HPCG_NO_OPENMP
      133             #pragma omp parallel
      134             {
      135             #pragma omp for nowait SCHEDULE(runtime)
      136             #endif
      137   p         		for ( local_int_t i = 0; i < size1; i+=2 ) {
      138   p         			local_int_t row_1 = A.tdg[l][i];
      139   p         			local_int_t row_2 = A.tdg[l][i+1];
      140   p         			const double * const currentValues_1 = A.matrixValues[row_1];
      141   p         			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
      142   p         			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
      143   p         			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
      144   p         			svfloat64_t contribs_1 = svdup_f64(0.0);
      145             
      146   p         			const double * const currentValues_2 = A.matrixValues[row_2];
      147   p         			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
      148   p         			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
      149   p         			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
      150   p         			svfloat64_t contribs_2 = svdup_f64(0.0);
      151             			
      152   p         			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
      153             
      154   p      s  			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
      155   p      s  				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
      156             				
      157   p      s  				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
      158   p      s  				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
      159   p      s  				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
      160             
      161   p      s  				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
      162             
      163   p      s  				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
      164   p      s  				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
      165   p      s  				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
      166   p      s  				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
      167             
      168   p      s  				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
      169   p      s  			}
      170             
      171   p         			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
      172   p         			double sum_1 = rv[row_1] - totalContribution_1;
      173             
      174   p         			sum_1 += xv[row_1] * currentDiagonal_1;
      175   p         			xv[row_1] = sum_1 / currentDiagonal_1;
      176             
      177   p         			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
      178   p         			double sum_2 = rv[row_2] - totalContribution_2;
      179             
      180   p         			sum_2 += xv[row_2] * currentDiagonal_2;
      181   p         			xv[row_2] = sum_2 / currentDiagonal_2;
      182   p         		}
      183             		//}
      184             		//else
      185             		//{
      186             #ifndef HPCG_NO_OPENMP
      187             //#pragma omp parallel for SCHEDULE(runtime)
      188             #pragma omp single 
      189   s         {
      190             #endif
      191   s         		if (size1 < totalSize) {
      192   s         			local_int_t i = size1;
      193             		//for ( local_int_t i = size1; i < totalSize; i++ ) {
      194   s         			local_int_t row = A.tdg[l][i];
      195   s         			const double * const currentValues = A.matrixValues[row];
      196   s         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      197   s         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      198   s         			const double currentDiagonal = matrixDiagonal[row][0];
      199   s         			svfloat64_t contribs = svdup_f64(0.0);
      200             
      201   s      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      202   s      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      203             				
      204   s      s  				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      205   s      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      206   s      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      207             
      208   s      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      209   s      s  			}
      210             
      211   s         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      212   s         			double sum = rv[row] - totalContribution;
      213             
      214   s         			sum += xv[row] * currentDiagonal;
      215   s         			xv[row] = sum / currentDiagonal;
      216             		//}
      217   s         		}
      218             #ifndef HPCG_NO_OPENMP
      219   s         }
      220             }
      221             #endif
      222    i        	}
      223             
      224             	/*
      225             	 * BACKWARD SWEEP
      226             	 */
      227    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      228             #ifndef HPCG_NO_OPENMP
      229             #pragma omp parallel for SCHEDULE(runtime)
      230             #endif
      231   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      232   p         			local_int_t row = A.tdg[l][i];
      233   p         			const double * const currentValues = A.matrixValues[row];
      234   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      235   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      236   p         			const double currentDiagonal = matrixDiagonal[row][0];
      237   p         			svfloat64_t contribs = svdup_f64(0.0);
      238             
      239   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      240   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      241             				
      242   p      s  				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      243   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      244   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      245             
      246   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      247   p      s  			}
      248             
      249   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      250   p         			double sum = rv[row] - totalContribution;
      251             
      252   p         			sum += xv[row] * currentDiagonal;
      253   p         			xv[row] = sum / currentDiagonal;
      254   p         		}
      255             
      256             /*#ifndef HPCG_NO_OPENMP
      257             #pragma omp parallel for SCHEDULE(runtime)
      258             #endif
      259             		for ( local_int_t i = size1-1; i >= 0; i-= 2 ) {
      260             			local_int_t row_1 = A.tdg[l][i];
      261             			local_int_t row_2 = A.tdg[l][i-1];
      262             			const double * const currentValues_1 = A.matrixValues[row_1];
      263             			const double * const currentValues_2 = A.matrixValues[row_2];
      264             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
      265             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
      266             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
      267             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
      268             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
      269             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
      270             			svfloat64_t contribs_1 = svdup_f64(0.0);
      271             			svfloat64_t contribs_2 = svdup_f64(0.0);
      272             
      273             			//#pragma loop nounroll
      274             			//#pragma loop nounroll_and_jam
      275             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      276             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      277             				
      278             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      279             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      280             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      281             
      282             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      283             			}
      284             
      285             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      286             			double sum = rv[row] - totalContribution;
      287             
      288             			sum += xv[row] * currentDiagonal;
      289             			xv[row] = sum / currentDiagonal;
      290             		}*/
      291             	}
      292             //#pragma statement end_scache_isolate_assign
      293             //#pragma statement end_scache_isolate_way
      294             
      295             #endif //TEST_XX
      296             
      297             LIKWID_STOP(trace.enabled, "symgs_tdg");
      298             
      299             	return 0;
      300             }
      301             /*
      302              * END OF TDG VERSION
      303              */
      304             
      305             /*
      306              * TDG FUSED SYMGS-SPMV VERSION
      307              */
      308             int ComputeFusedSYMGS_SPMV_SVE(const SparseMatrix & A, const Vector & r, Vector & x, Vector & y, TraceData& trace) {
      309             	assert(x.localLength == A.localNumberOfColumns);
      310             
      311             #ifndef HPCG_NO_MPI
      312             	ExchangeHalo(A, x);
      313             #endif
      314             
      315             	const double * const rv = r.values;
      316             	double * const xv = x.values;
      317             	double **matrixDiagonal = A.matrixDiagonal;
      318             	double * const yv = y.values;
      319             
      320             	/*
      321             	 * FORWARD SWEEP
      322             	 */
      323    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      324             #ifndef HPCG_NO_OPENMP
      325             #pragma omp parallel for SCHEDULE(runtime)
      326             #endif
      327   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      328   p         			local_int_t row = A.tdg[l][i];
      329   p         			const double * const currentValues = A.matrixValues[row];
      330   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      331   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      332   p         			const double currentDiagonal = matrixDiagonal[row][0];
      333   p         			svfloat64_t contribs = svdup_f64(0.0);
      334             
      335   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      336   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      337             				
      338   p      s  				svfloat64_t mtxValues = svld1(pg, &currentValues[j]);
      339   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      340   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      341             
      342   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      343   p      s  			}
      344             
      345   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      346   p         			double sum = rv[row] - totalContribution;
      347             
      348   p         			sum += xv[row] * currentDiagonal;
      349   p         			xv[row] = sum / currentDiagonal;
      350   pi        		}
      351    i        	}
      352             
      353             	/*
      354             	 * BACKWARD SWEEP
      355             	 */
      356    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      357             #ifndef HPCG_NO_OPENMP
      358             #pragma omp parallel for SCHEDULE(runtime)
      359             #endif
      360   pi        		for ( local_int_t i = A.tdg[l].size(); i >= 0; i-- ) {
      361   p         			local_int_t row = A.tdg[l][i];
      362   p         			const double * const currentValues = A.matrixValues[row];
      363   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      364   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      365   p         			const double currentDiagonal = matrixDiagonal[row][0];
      366   p         			svfloat64_t contribs = svdup_f64(0.0);
      367             
      368   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      369   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      370             				
      371   p      s  				svfloat64_t mtxValues = svld1(pg, &currentValues[j]);
      372   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      373   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      374             
      375   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      376   p      s  			}
      377             
      378   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      379   p         			totalContribution -= xv[row] * currentDiagonal; // remove diagonal contribution
      380   p         			double sum = rv[row] - totalContribution; // substract contributions from RHS
      381   p         			xv[row] = sum / currentDiagonal; // update row
      382             
      383             			// SPMV part
      384   p         			totalContribution += xv[row] * currentDiagonal; // add updated diagonal contribution
      385   p         			yv[row] = totalContribution; // update SPMV output vector
      386             			
      387   p         		}
      388             	}
      389             
      390             	return 0;
      391             }
      392             /*
      393              * END OF TDG FUSED SYMGS-SPMV VERSION
      394              */
      395             
      396             /*
      397              * BLOCK COLORED VERSION
      398              */
      399             int ComputeSYMGS_BLOCK_SVE(const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
      400             	assert(x.localLength >= A.localNumberOfColumns);
      401             
      402             #ifndef HPCG_NO_MPI
      403             	ExchangeHalo(A, x);
      404             #endif
      405             
      406             	double **matrixDiagonal = A.matrixDiagonal;
      407             	const double * const rv = r.values;
      408             	double * const xv = x.values;
      409             	local_int_t firstBlock = 0;
      410    i        	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
      411             
      412             LIKWID_START(trace.enabled, "symgs_bc");		
      413             
      414             	/*
      415             	 * FORWARD SWEEP
      416             	 */
      417             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) { // for each color
      418             		if ( color > 0 ) {
      419    i        			firstBlock += A.numberOfBlocksInColor[color-1];
      420    i        			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
      421             		}
      422             #ifndef HPCG_NO_OPENMP
      423             #pragma omp parallel for SCHEDULE(runtime)
      424             #endif
      425   p         		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) { // for each superblock with the same color
      426   p         			local_int_t firstRow = block * A.blockSize;
      427   p         			local_int_t firstChunk = firstRow / A.chunkSize;
      428   p         			local_int_t lastChunk = (firstRow + A.blockSize * A.chunkSize) / A.chunkSize;
      429             
      430   p         			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) { // for each chunk of this super block
      431   p         				local_int_t first = A.chunkSize * chunk;
      432   p         				local_int_t last = first + A.chunkSize;
      433   p         				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      434   p         				local_int_t i = first;
      435   p         				if ( A.chunkSize == 4 ) {
      436   p         					const double * const currentValues0 = A.matrixValues[i  ];
      437   p         					const double * const currentValues1 = A.matrixValues[i+1];
      438   p         					const double * const currentValues2 = A.matrixValues[i+2];
      439   p         					const double * const currentValues3 = A.matrixValues[i+3];
      440             
      441   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      442   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      443   p         					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      444   p         					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      445             
      446   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      447   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      448   p         					const double currentDiagonal2 = matrixDiagonal[i+2][0];
      449   p         					const double currentDiagonal3 = matrixDiagonal[i+3][0];
      450             
      451   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      452   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      453   p         					svfloat64_t contribs2 = svdup_f64(0.0);
      454   p         					svfloat64_t contribs3 = svdup_f64(0.0);
      455             
      456   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      457   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      458             
      459   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      460   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      461   p      s  						svfloat64_t values2 = svld1_f64(pg, &currentValues2[j]);
      462   p      s  						svfloat64_t values3 = svld1_f64(pg, &currentValues3[j]);
      463             
      464   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      465   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      466   p      s  						svuint64_t indices2 = svld1sw_u64(pg, &currentColIndices2[j]);
      467   p      s  						svuint64_t indices3 = svld1sw_u64(pg, &currentColIndices3[j]);
      468             
      469   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      470   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      471   p      s  						svfloat64_t xvv2 = svld1_gather_u64index_f64(pg, xv, indices2);
      472   p      s  						svfloat64_t xvv3 = svld1_gather_u64index_f64(pg, xv, indices3);
      473             
      474   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      475   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1);
      476   p      s  						contribs2 = svmla_f64_m(pg, contribs2, xvv2, values2);
      477   p      s  						contribs3 = svmla_f64_m(pg, contribs3, xvv3, values3);
      478   p      s  					}
      479             
      480   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      481   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      482   p         					double totalContribution2 = svaddv_f64(svptrue_b64(), contribs2);
      483   p         					double totalContribution3 = svaddv_f64(svptrue_b64(), contribs3);
      484             
      485   p         					double sum0 = rv[i  ] - totalContribution0;
      486   p         					double sum1 = rv[i+1] - totalContribution1;
      487   p         					double sum2 = rv[i+2] - totalContribution2;
      488   p         					double sum3 = rv[i+3] - totalContribution3;
      489             
      490   p         					sum0 += xv[i  ] * currentDiagonal0;
      491   p         					sum1 += xv[i+1] * currentDiagonal1;
      492   p         					sum2 += xv[i+2] * currentDiagonal2;
      493   p         					sum3 += xv[i+3] * currentDiagonal3;
      494             
      495   p         					xv[i  ] = sum0 / currentDiagonal0;
      496   p         					xv[i+1] = sum1 / currentDiagonal1;
      497   p         					xv[i+2] = sum2 / currentDiagonal2;
      498   p         					xv[i+3] = sum3 / currentDiagonal3;
      499   p         				} else if ( A.chunkSize == 2 ) {
      500   p         					const double * const currentValues0 = A.matrixValues[i  ];
      501   p         					const double * const currentValues1 = A.matrixValues[i+1];
      502             
      503   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      504   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      505             
      506   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      507   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      508             
      509   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      510   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      511             
      512   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      513   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      514             
      515   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      516   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      517             
      518   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      519   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      520             
      521   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      522   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      523             
      524   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      525   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1);
      526   p      s  					}
      527             
      528   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      529   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      530             
      531   p         					double sum0 = rv[i  ] - totalContribution0;
      532   p         					double sum1 = rv[i+1] - totalContribution1;
      533             
      534   p         					sum0 += xv[i  ] * currentDiagonal0;
      535   p         					sum1 += xv[i+1] * currentDiagonal1;
      536             
      537   p         					xv[i  ] = sum0 / currentDiagonal0;
      538   p         					xv[i+1] = sum1 / currentDiagonal1;
      539   p         				} else { //A.chunkSize == 1
      540   p         					const double * const currentValues0 = A.matrixValues[i  ];
      541             
      542   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      543             
      544   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      545             
      546   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      547             
      548   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      549   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      550             
      551   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      552             
      553   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      554             
      555   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      556             
      557   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      558   p      s  					}
      559             
      560   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      561             
      562   p         					double sum0 = rv[i  ] - totalContribution0;
      563             
      564   p         					sum0 += xv[i  ] * currentDiagonal0;
      565             
      566   p         					xv[i  ] = sum0 / currentDiagonal0;
      567   p         				}
      568   p         			}
      569   p         		}
      570             	}
      571             
      572             	firstBlock = A.numberOfBlocks-1;
      573    i        	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
      574             	/*
      575             	 * BACKWARD SWEEP
      576             	 */
      577             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
      578             		if ( color < A.numberOfColors-1 ) {
      579    i        			firstBlock -= A.numberOfBlocksInColor[color+1];
      580    i        			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
      581             		}
      582             #ifndef HPCG_NO_OPENMP
      583             #pragma omp parallel for SCHEDULE(runtime)
      584             #endif
      585   p         		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) {
      586   p         			local_int_t firstRow = ((block+1) * A.blockSize) - 1;
      587   p         			local_int_t firstChunk = firstRow / A.chunkSize;
      588   p         			local_int_t lastChunk = (firstRow - A.blockSize * A.chunkSize) / A.chunkSize;
      589             
      590   p         			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
      591   p         				local_int_t first = A.chunkSize * chunk;
      592   p         				local_int_t last = first + A.chunkSize;
      593             
      594   p         				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      595   p         				local_int_t i = first;
      596   p         				if ( A.chunkSize == 4 ) {
      597   p         					const double * const currentValues3 = A.matrixValues[i+3];
      598   p         					const double * const currentValues2 = A.matrixValues[i+2];
      599   p         					const double * const currentValues1 = A.matrixValues[i+1];
      600   p         					const double * const currentValues0 = A.matrixValues[i  ];
      601             
      602   p         					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      603   p         					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      604   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      605   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      606             
      607   p         					const double currentDiagonal3 = matrixDiagonal[i+3][0];
      608   p         					const double currentDiagonal2 = matrixDiagonal[i+2][0];
      609   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      610   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      611             
      612   p         					svfloat64_t contribs3 = svdup_f64(0.0);
      613   p         					svfloat64_t contribs2 = svdup_f64(0.0);
      614   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      615   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      616             
      617   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      618   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      619             
      620   p      s  						svfloat64_t values3 = svld1_f64(pg, &currentValues3[j]);
      621   p      s  						svfloat64_t values2 = svld1_f64(pg, &currentValues2[j]);
      622   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      623   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      624             
      625   p      s  						svuint64_t indices3 = svld1sw_u64(pg, &currentColIndices3[j]);
      626   p      s  						svuint64_t indices2 = svld1sw_u64(pg, &currentColIndices2[j]);
      627   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      628   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      629             
      630   p      s  						svfloat64_t xvv3 = svld1_gather_u64index_f64(pg, xv, indices3);
      631   p      s  						svfloat64_t xvv2 = svld1_gather_u64index_f64(pg, xv, indices2);
      632   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      633   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      634             
      635   p      s  						contribs3 = svmla_f64_m(pg, contribs3, xvv3, values3 );
      636   p      s  						contribs2 = svmla_f64_m(pg, contribs2, xvv2, values2 );
      637   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1 );
      638   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      639   p      s  					}
      640             
      641   p         					double totalContribution3 = svaddv_f64(svptrue_b64(), contribs3);
      642   p         					double totalContribution2 = svaddv_f64(svptrue_b64(), contribs2);
      643   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      644   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      645             
      646   p         					double sum3 = rv[i+3] - totalContribution3;
      647   p         					double sum2 = rv[i+2] - totalContribution2;
      648   p         					double sum1 = rv[i+1] - totalContribution1;
      649   p         					double sum0 = rv[i  ] - totalContribution0;
      650             
      651   p         					sum3 += xv[i+3] * currentDiagonal3;
      652   p         					sum2 += xv[i+2] * currentDiagonal2;
      653   p         					sum1 += xv[i+1] * currentDiagonal1;
      654   p         					sum0 += xv[i  ] * currentDiagonal0;
      655             					
      656   p         					xv[i+3] = sum3 / currentDiagonal3;
      657   p         					xv[i+2] = sum2 / currentDiagonal2;
      658   p         					xv[i+1] = sum1 / currentDiagonal1;
      659   p         					xv[i  ] = sum0 / currentDiagonal0;
      660   p         				} else if ( A.chunkSize == 2 ) {
      661   p         					const double * const currentValues1 = A.matrixValues[i+1];
      662   p         					const double * const currentValues0 = A.matrixValues[i  ];
      663             
      664   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      665   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      666             
      667   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      668   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      669             
      670   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      671   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      672             
      673   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      674   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      675             
      676   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      677   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      678             
      679   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      680   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      681             
      682   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      683   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      684             
      685   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1 );
      686   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      687   p      s  					}
      688             
      689   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      690   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      691             
      692   p         					double sum1 = rv[i+1] - totalContribution1;
      693   p         					double sum0 = rv[i  ] - totalContribution0;
      694             
      695   p         					sum1 += xv[i+1] * currentDiagonal1;
      696   p         					sum0 += xv[i  ] * currentDiagonal0;
      697             					
      698   p         					xv[i+1] = sum1 / currentDiagonal1;
      699   p         					xv[i  ] = sum0 / currentDiagonal0;
      700   p         				} else { // A.chunkSize == 1
      701   p         					const double * const currentValues0 = A.matrixValues[i  ];
      702             
      703   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      704             
      705   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      706             
      707   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      708             
      709   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      710   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      711             
      712   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      713             
      714   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      715             
      716   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      717             
      718   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      719   p      s  					}
      720             
      721   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      722             
      723   p         					double sum0 = rv[i  ] - totalContribution0;
      724             
      725   p         					sum0 += xv[i  ] * currentDiagonal0;
      726             					
      727   p         				}
      728   p         			}
      729   p         		}
      730             	}
      731             LIKWID_STOP(trace.enabled, "symgs_bc");			
      732             
      733             	return 0;
      734             }
      735             /*
      736              * END OF BLOCK COLORED VERSION
      737              */
      738             #elif defined(HPCG_USE_NEON)
      739             
      740             /**************************************************************************************************/
      741             /**************************************************************************************************/
      742             /**************************************************************************************************/
      743             /* NEON IMPLEMENTATIONS                                                                           */
      744             /**************************************************************************************************/
      745             /**************************************************************************************************/
      746             /**************************************************************************************************/
      747             
      748             #include "arm_neon.h"
      749             
      750             /*
      751              * TDG VERSION
      752              */
      753             int ComputeSYMGS_TDG_NEON(const SparseMatrix & A, const Vector & r, Vector & x) {
      754             	assert(x.localLength == A.localNumberOfColumns);
      755             
      756             #ifndef HPCG_NO_MPI
      757             	ExchangeHalo(A, x);
      758             #endif
      759             
      760             	const double * const rv = r.values;
      761             	double * const xv = x.values;
      762             	double **matrixDiagonal = A.matrixDiagonal;
      763             
      764             	/*
      765             	 * FORWARD
      766             	 */
      767             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      768             #ifndef HPCG_NO_OPENMP
      769             #pragma omp parallel for SCHEDULE(runtime)
      770             #endif
      771             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      772             			local_int_t row = A.tdg[l][i];
      773             			const double * const currentValues = A.matrixValues[row];
      774             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      775             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      776             			const double currentDiagonal = matrixDiagonal[row][0];
      777             			float64x2_t contribs = vdupq_n_f64(0.0);
      778             
      779             			local_int_t j = 0;
      780             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      781             				// Load the needed j values
      782             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      783             				// Load the needed x values
      784             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      785             				float64x2_t xvv = vld1q_f64(aux);
      786             				// Add the contribution
      787             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      788             			}
      789             			// reduce contributions
      790             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      791             			double sum = rv[row] - totalContribution;
      792             			// Add missing values from last loop
      793             			if ( j < currentNumberOfNonzeros ) {
      794             				sum -= currentValues[j] * xv[currentColIndices[j]];
      795             			}
      796             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      797             			xv[row] = sum / currentDiagonal; // update row
      798             		}
      799             	}
      800             
      801             	/*
      802             	 * BACKWARD
      803             	 */
      804             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      805             #ifndef HPCG_NO_OPENMP
      806             #pragma omp parallel for SCHEDULE(runtime)
      807             #endif
      808             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      809             			local_int_t row = A.tdg[l][i];
      810             			const double * const currentValues = A.matrixValues[row];
      811             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      812             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      813             			const double currentDiagonal = matrixDiagonal[row][0];
      814             			float64x2_t contribs = vdupq_n_f64(0.0);
      815             
      816             			local_int_t j = 0;
      817             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      818             				// Load the needed j values
      819             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      820             				// Load the needed x values
      821             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      822             				float64x2_t xvv = vld1q_f64(aux);
      823             				// Add the contribution
      824             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      825             			}
      826             			// reduce contributions
      827             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      828             			double sum = rv[row] - totalContribution;
      829             			// Add missing values from last loop
      830             			if ( j < currentNumberOfNonzeros ) {
      831             				sum -= currentValues[j] * xv[currentColIndices[j]];
      832             			}
      833             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      834             			xv[row] = sum / currentDiagonal; // update row
      835             		}
      836             	}
      837             
      838             	return 0;
      839             }
      840             /*
      841              *
      842              */
      843             ////////////////////////////////////////////////////////////////////////////////
      844             ////////////////////////////////////////////////////////////////////////////////
      845             ////////////////////////////////////////////////////////////////////////////////
      846             /*
      847              * TDG FUSED VERSION
      848              */
      849             int ComputeFusedSYMGS_SPMV_NEON(const SparseMatrix & A, const Vector & r, Vector & x, Vector & y) {
      850             	assert(x.localLength == A.localNumberOfColumns);
      851             
      852             #ifndef HPCG_NO_MPI
      853             	ExchangeHalo(A, x);
      854             #endif
      855             
      856             	const double * const rv = r.values;
      857             	double * const xv = x.values;
      858             	double * const yv = y.values;
      859             	double **matrixDiagonal = A.matrixDiagonal;
      860             
      861             	/*
      862             	 * FORWARD
      863             	 */
      864             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      865             #ifndef HPCG_NO_OPENMP
      866             #pragma omp parallel for SCHEDULE(runtime)
      867             #endif
      868             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      869             			local_int_t row = A.tdg[l][i];
      870             			const double * const currentValues = A.matrixValues[row];
      871             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      872             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      873             			const double currentDiagonal = matrixDiagonal[row][0];
      874             			float64x2_t contribs = vdupq_n_f64(0.0);
      875             
      876             			local_int_t j = 0;
      877             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      878             				// Load the needed j values
      879             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      880             				// Load the needed x values
      881             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      882             				float64x2_t xvv = vld1q_f64(aux);
      883             				// Add the contribution
      884             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      885             			}
      886             			// reduce contributions
      887             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      888             			double sum = rv[row] - totalContribution;
      889             			// Add missing values from last loop
      890             			if ( j < currentNumberOfNonzeros ) {
      891             				sum -= currentValues[j] * xv[currentColIndices[j]];
      892             			}
      893             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      894             			xv[row] = sum / currentDiagonal; // update row
      895             		}
      896             	}
      897             
      898             	/*
      899             	 * BACKWARD (fusing SYMGS and SPMV)
      900             	 */
      901             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      902             #ifndef HPCG_NO_OPENMP
      903             #pragma omp parallel for SCHEDULE(runtime)
      904             #endif
      905             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      906             			local_int_t row = A.tdg[l][i];
      907             			const double * const currentValues = A.matrixValues[row];
      908             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      909             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      910             			const double currentDiagonal = matrixDiagonal[row][0];
      911             			float64x2_t contribs = vdupq_n_f64(0.0);
      912             
      913             			local_int_t j = 0;
      914             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      915             				// Load the needed j values
      916             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      917             				// Load the needed x values
      918             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      919             				float64x2_t xvv = vld1q_f64(aux);
      920             				// Add the contribution
      921             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      922             			}
      923             			// reduce contributions
      924             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      925             			// Add missing values from last loop
      926             			if ( j < currentNumberOfNonzeros ) {
      927             				totalContribution += currentValues[j] * xv[currentColIndices[j]];
      928             			}
      929             			totalContribution -= xv[row] * currentDiagonal; // remove diagonal contribution
      930             			double sum = rv[row] - totalContribution; // substract contributions from RHS
      931             			xv[row] = sum / currentDiagonal; // update row
      932             			// Fusion part
      933             			totalContribution += xv[row] * currentDiagonal; // add updated diagonal contribution
      934             			yv[row] = totalContribution; // update SPMV output vector
      935             		}
      936             	}
      937             
      938             	return 0;
      939             }
      940             /*
      941              *
      942              */
      943             ////////////////////////////////////////////////////////////////////////////////
      944             ////////////////////////////////////////////////////////////////////////////////
      945             ////////////////////////////////////////////////////////////////////////////////
      946             /*
      947              * BLOCK COLORED VERSION
      948              */
      949             int ComputeSYMGS_BLOCK_NEON(const SparseMatrix & A, const Vector & r, Vector & x) {
      950             
      951             	assert(x.localLength >= A.localNumberOfColumns);
      952             	
      953             #ifndef HPCG_NO_MPI
      954             	ExchangeHalo(A, x);
      955             #endif
      956             
      957             	double **matrixDiagonal = A.matrixDiagonal;
      958             	const double * const rv = r.values;
      959             	double * const xv = x.values;
      960             
      961             	local_int_t firstBlock = 0;
      962             	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
      963             	/*
      964             	 * FORWARD
      965             	 */
      966             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) { // for each color
      967             		if ( color > 0 ) {
      968             			firstBlock += A.numberOfBlocksInColor[color-1];
      969             			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
      970             		}
      971             #ifndef HPCG_NO_OPENMP
      972             #pragma omp parallel for SCHEDULE(runtime)
      973             #endif
      974             		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) { // for each super block with the same color
      975             			local_int_t firstRow = block * A.blockSize;
      976             			local_int_t firstChunk = firstRow / A.chunkSize;
      977             			local_int_t lastChunk = (firstRow + A.blockSize*A.chunkSize) / A.chunkSize;
      978             
      979             			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) { // for each chunk of this super block
      980             				local_int_t first = A.chunkSize * chunk;
      981             				local_int_t last = first + A.chunkSize;
      982             
      983             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      984             				local_int_t i = first;
      985             				if ( A.chunkSize == 4 ) {
      986             					const double * const currentValues0 = A.matrixValues[i  ];
      987             					const double * const currentValues1 = A.matrixValues[i+1];
      988             					const double * const currentValues2 = A.matrixValues[i+2];
      989             					const double * const currentValues3 = A.matrixValues[i+3];
      990             
      991             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      992             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      993             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      994             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      995             
      996             					const double currentDiagonal[4] = { matrixDiagonal[i  ][0],\
      997             														matrixDiagonal[i+1][0],\
      998             														matrixDiagonal[i+2][0],\
      999             														matrixDiagonal[i+3][0]};
     1000             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1001             					float64x2_t diagonal23 = vld1q_f64(&currentDiagonal[2]);
     1002             
     1003             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1004             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1005             					float64x2_t contribs2 = vdupq_n_f64(0.0);
     1006             					float64x2_t contribs3 = vdupq_n_f64(0.0);
     1007             
     1008             					float64x2_t vrv01 = vld1q_f64(&rv[i]);
     1009             					float64x2_t vrv23 = vld1q_f64(&rv[i+2]);
     1010             
     1011             					float64x2_t vxv01 = vld1q_f64(&xv[i]);
     1012             					float64x2_t vxv23 = vld1q_f64(&xv[i+2]);
     1013             
     1014             					local_int_t j = 0;
     1015             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1016             						// Load values
     1017             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1018             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1019             						float64x2_t values2 = vld1q_f64(&currentValues2[j]);
     1020             						float64x2_t values3 = vld1q_f64(&currentValues3[j]);
     1021             
     1022             						// Load x
     1023             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1024             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1025             						float64x2_t vxv2 = { xv[currentColIndices2[j]], xv[currentColIndices2[j+1]] };
     1026             						float64x2_t vxv3 = { xv[currentColIndices3[j]], xv[currentColIndices3[j+1]] };
     1027             
     1028             						// Add contribution
     1029             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1030             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1031             						contribs2 = vfmaq_f64(contribs2, values2, vxv2);
     1032             						contribs3 = vfmaq_f64(contribs3, values3, vxv3);
     1033             					}
     1034             					// Reduce contribution
     1035             					// First for i and i+1
     1036             					float64x2_t totalContribution01;
     1037             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1038             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1039             
     1040             					// Then for i+2 and i+3
     1041             					float64x2_t totalContribution23;
     1042             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs2), totalContribution23, 0);
     1043             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs3), totalContribution23, 1);
     1044             
     1045             					// Substract contributions from RHS
     1046             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1047             					float64x2_t sum23 = vsubq_f64(vrv23, totalContribution23);
     1048             
     1049             					// Add contributions from missing elements (if any)
     1050             					if ( j < currentNumberOfNonzeros ) {
     1051             						// Load current values
     1052             						float64x2_t values01 = { currentValues0[j], currentValues1[j] };
     1053             						float64x2_t values23 = { currentValues2[j], currentValues3[j] };
     1054             
     1055             						// Load x
     1056             						float64x2_t vx01 = { xv[currentColIndices0[j]], xv[currentColIndices1[j]] };
     1057             						float64x2_t vx23 = { xv[currentColIndices2[j]], xv[currentColIndices3[j]] };
     1058             
     1059             						// Add contributions
     1060             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1061             						sum23 = vfmsq_f64(sum23, values23, vx23);
     1062             					}
     1063             
     1064             					// Remove diagonal contribution and update rows i and i+1
     1065             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1066             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1067             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1068             
     1069             					// Remove diagonal contribution and update rows i+2 and i+3
     1070             					sum23 = vfmaq_f64(sum23, vxv23, diagonal23);
     1071             					xv[i+2] = vgetq_lane_f64(sum23, 0) / currentDiagonal[2];
     1072             					xv[i+3] = vgetq_lane_f64(sum23, 1) / currentDiagonal[3];
     1073             				} else if ( A.chunkSize == 2 ) {
     1074             					const double * const currentValues0 = A.matrixValues[i  ];
     1075             					const double * const currentValues1 = A.matrixValues[i+1];
     1076             
     1077             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1078             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1079             
     1080             					const double currentDiagonal[2] = { matrixDiagonal[i  ][0],\
     1081             														matrixDiagonal[i+1][0]};
     1082             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1083             
     1084             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1085             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1086             
     1087             					float64x2_t vrv01 = vld1q_f64(&rv[i]);
     1088             
     1089             					float64x2_t vxv01 = vld1q_f64(&xv[i]);
     1090             
     1091             					local_int_t j = 0;
     1092             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1093             						// Load values
     1094             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1095             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1096             
     1097             						// Load x
     1098             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1099             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1100             
     1101             						// Add contribution
     1102             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1103             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1104             					}
     1105             					// Reduce contribution
     1106             					// First for i and i+1
     1107             					float64x2_t totalContribution01;
     1108             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1109             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1110             
     1111             					// Substract contributions from RHS
     1112             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1113             
     1114             					// Add contributions from missing elements (if any)
     1115             					if ( j < currentNumberOfNonzeros ) {
     1116             						// Load current values
     1117             						float64x2_t values01 = { currentValues0[j], currentValues1[j] };
     1118             
     1119             						// Load x
     1120             						float64x2_t vx01 = { xv[currentColIndices0[j]], xv[currentColIndices1[j]] };
     1121             
     1122             						// Add contributions
     1123             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1124             					}
     1125             
     1126             					// Remove diagonal contribution and update rows i and i+1
     1127             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1128             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1129             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1130             				} else { // A.chunkSize == 1
     1131             					const double * const currentValues = A.matrixValues[i];
     1132             					const local_int_t * const currentColIndices = A.mtxIndL[i];
     1133             					const double currentDiagonal = matrixDiagonal[i][0];
     1134             					float64x2_t contribs = vdupq_n_f64(0.0);
     1135             
     1136             					local_int_t j = 0;
     1137             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1138             						// Load values
     1139             						float64x2_t values = vld1q_f64(&currentValues[j]);
     1140             
     1141             						// Load x
     1142             						float64x2_t vxv = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
     1143             
     1144             						// Add contribution
     1145             						contribs = vfmaq_f64(contribs, values, vxv);
     1146             					}
     1147             					// Reduce contribution
     1148             					// First for i and i+1
     1149             					double totalContribution;
     1150             					totalContribution = vaddvq_f64(contribs);
     1151             
     1152             					// Substract contributions from RHS
     1153             					double sum = rv[i] - totalContribution;
     1154             
     1155             					// Add contributions from missing elements (if any)
     1156             					if ( j < currentNumberOfNonzeros ) {
     1157             						sum -= currentValues[j] * xv[currentColIndices[j]];
     1158             					}
     1159             
     1160             					// Remove diagonal contribution and update rows i and i+1
     1161             					sum += xv[i] * currentDiagonal;
     1162             					xv[i] = sum / currentDiagonal;
     1163             				}
     1164             			}
     1165             		}
     1166             	}
     1167             
     1168             	firstBlock = A.numberOfBlocks-1;
     1169             	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
     1170             	/*
     1171             	 * BACKWARD
     1172             	 */
     1173             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
     1174             		if ( color < A.numberOfColors-1 ) {
     1175             			firstBlock -= A.numberOfBlocksInColor[color+1];
     1176             			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
     1177             		}
     1178             #ifndef HPCG_NO_OPENMP
     1179             #pragma omp parallel for SCHEDULE(runtime)
     1180             #endif
     1181             		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) { // we skip a whole superblock on each iteration
     1182             			local_int_t firstRow = ((block+1) * A.blockSize) - 1; // this is the last row of the last block (i.e., next block first row - 1)
     1183             			local_int_t firstChunk = firstRow / A.chunkSize; // this is the  chunk of the row above
     1184             			local_int_t lastChunk = (firstRow - A.blockSize*A.chunkSize) / A.chunkSize; 
     1185             
     1186             			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
     1187             				local_int_t first = A.chunkSize * chunk;
     1188             				local_int_t last = first + A.chunkSize;
     1189             
     1190             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
     1191             				if ( A.chunkSize == 4 ) {
     1192             					local_int_t i = last-1-3;
     1193             
     1194             					const double * const currentValues3 = A.matrixValues[i+3];
     1195             					const double * const currentValues2 = A.matrixValues[i+2];
     1196             					const double * const currentValues1 = A.matrixValues[i+1];
     1197             					const double * const currentValues0 = A.matrixValues[i  ];
     1198             
     1199             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
     1200             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
     1201             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1202             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1203             
     1204             					const double currentDiagonal[4] = {\
     1205             							matrixDiagonal[i  ][0],\
     1206             							matrixDiagonal[i+1][0],\
     1207             							matrixDiagonal[i+2][0],\
     1208             							matrixDiagonal[i+3][0]};
     1209             
     1210             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1211             					float64x2_t diagonal23 = vld1q_f64(&currentDiagonal[2]);
     1212             
     1213             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1214             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1215             					float64x2_t contribs2 = vdupq_n_f64(0.0);
     1216             					float64x2_t contribs3 = vdupq_n_f64(0.0);
     1217             
     1218             					float64x2_t vrv23 = vld1q_f64(&rv[i+2]);
     1219             					float64x2_t vrv01 = vld1q_f64(&rv[i  ]);
     1220             
     1221             					float64x2_t vxv23 = vld1q_f64(&xv[i+2]);
     1222             					float64x2_t vxv01 = vld1q_f64(&xv[i  ]);
     1223             
     1224             					local_int_t j = 0;
     1225             					for ( j = currentNumberOfNonzeros-2; j >= 0; j -= 2 ) {
     1226             						// Load values
     1227             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1228             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1229             						float64x2_t values2 = vld1q_f64(&currentValues2[j]);
     1230             						float64x2_t values3 = vld1q_f64(&currentValues3[j]);
     1231             
     1232             						// Load x
     1233             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1234             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1235             						float64x2_t vxv2 = { xv[currentColIndices2[j]], xv[currentColIndices2[j+1]] };
     1236             						float64x2_t vxv3 = { xv[currentColIndices3[j]], xv[currentColIndices3[j+1]] };
     1237             
     1238             						// Add contribution
     1239             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1240             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1241             						contribs2 = vfmaq_f64(contribs2, values2, vxv2);
     1242             						contribs3 = vfmaq_f64(contribs3, values3, vxv3);
     1243             					}
     1244             					// Reduce contribution
     1245             					// First for i and i-1
     1246             					float64x2_t totalContribution01;
     1247             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1248             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1249             
     1250             					// Then for i-2 and i-3
     1251             					float64x2_t totalContribution23;
     1252             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs2), totalContribution23, 0);
     1253             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs3), totalContribution23, 1);
     1254             
     1255             					// Substract contributions from RHS
     1256             					float64x2_t sum23 = vsubq_f64(vrv23, totalContribution23);
     1257             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1258             
     1259             					// Add contributions from missing elements (if any)
     1260             					if ( j == -1 ) {
     1261             						// Load current values
     1262             						float64x2_t values23 = { currentValues2[j+1], currentValues3[j+1] };
     1263             						float64x2_t values01 = { currentValues0[j+1], currentValues1[j+1] };
     1264             
     1265             						// Load x
     1266             						float64x2_t vx23 = { xv[currentColIndices2[j+1]], xv[currentColIndices3[j+1]] };
     1267             						float64x2_t vx01 = { xv[currentColIndices0[j+1]], xv[currentColIndices1[j+1]] };
     1268             
     1269             						// Add contributions
     1270             						sum23 = vfmsq_f64(sum23, values23, vx23);
     1271             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1272             					}
     1273             
     1274             					// Remove diagonal contribution and update rows i-2 and i-3
     1275             					sum23 = vfmaq_f64(sum23, vxv23, diagonal23);
     1276             					xv[i+3] = vgetq_lane_f64(sum23, 1) / currentDiagonal[3];
     1277             					xv[i+2] = vgetq_lane_f64(sum23, 0) / currentDiagonal[2];
     1278             
     1279             					// Remove diagonal contribution and update rows i and i-1
     1280             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1281             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1282             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1283             				} else if ( A.chunkSize == 2 ) {
     1284             					local_int_t i = last-1-1;
     1285             
     1286             					const double * const currentValues1 = A.matrixValues[i+1];
     1287             					const double * const currentValues0 = A.matrixValues[i  ];
     1288             
     1289             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1290             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1291             
     1292             					const double currentDiagonal[2] = {\
     1293             							matrixDiagonal[i  ][0],\
     1294             							matrixDiagonal[i+1][0]};
     1295             
     1296             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1297             
     1298             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1299             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1300             
     1301             					float64x2_t vrv01 = vld1q_f64(&rv[i  ]);
     1302             
     1303             					float64x2_t vxv01 = vld1q_f64(&xv[i  ]);
     1304             
     1305             					local_int_t j = 0;
     1306             					for ( j = currentNumberOfNonzeros-2; j >= 0; j -= 2 ) {
     1307             						// Load values
     1308             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1309             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1310             
     1311             						// Load x
     1312             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1313             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1314             
     1315             						// Add contribution
     1316             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1317             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1318             					}
     1319             					// Reduce contribution
     1320             					// First for i and i-1
     1321             					float64x2_t totalContribution01;
     1322             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1323             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1324             
     1325             					// Substract contributions from RHS
     1326             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1327             
     1328             					// Add contributions from missing elements (if any)
     1329             					if ( j == -1 ) {
     1330             						// Load current values
     1331             						float64x2_t values01 = { currentValues0[j+1], currentValues1[j+1] };
     1332             
     1333             						// Load x
     1334             						float64x2_t vx01 = { xv[currentColIndices0[j+1]], xv[currentColIndices1[j+1]] };
     1335             
     1336             						// Add contributions
     1337             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1338             					}
     1339             
     1340             					// Remove diagonal contribution and update rows i and i-1
     1341             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1342             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1343             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1344             				} else { // A.chunkSize == 1
     1345             					local_int_t i = last - 1; // == first
     1346             					const double * const currentValues = A.matrixValues[i];
     1347             					const local_int_t * const currentColIndices = A.mtxIndL[i];
     1348             					const double currentDiagonal = matrixDiagonal[i][0];
     1349             
     1350             					float64x2_t contribs = vdupq_n_f64(0.0);
     1351             
     1352             					local_int_t j = 0;
     1353             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1354             						// Load values
     1355             						float64x2_t values = vld1q_f64(&currentValues[j]);
     1356             
     1357             						// Load x
     1358             						float64x2_t vxv = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
     1359             
     1360             						// Add contribution
     1361             						contribs = vfmaq_f64(contribs, values, vxv);
     1362             					}
     1363             					// Reduce contribution
     1364             					double totalContribution = vaddvq_f64(contribs);
     1365             
     1366             					// Substract contribution from RHS
     1367             					double sum = rv[i] - totalContribution;
     1368             
     1369             					// Add contributions from missing elements (if any)
     1370             					if ( j < currentNumberOfNonzeros ) {
     1371             						sum -= currentValues[j] * xv[currentColIndices[j]];
     1372             					}
     1373             
     1374             					// Remove diagonal contribution and updated row i
     1375             					sum += xv[i] * currentDiagonal;
     1376             					xv[i] = sum / currentDiagonal;
     1377             				}
     1378             			}
     1379             		}
     1380             	}
     1381             
     1382             	return 0;
     1383             }
     1384             /*
     1385              *
     1386              */
     1387             #endif
     1388             //#else // !HPCG_USE_SVE ! HPCG_USE_NEON
     1389             
     1390             int ComputeFusedSYMGS_SPMV ( const SparseMatrix & A, const Vector & r, Vector & x, Vector & y ) {
     1391             	assert(x.localLength == A.localNumberOfColumns);
     1392             
     1393             #ifndef HPCG_NO_MPI
     1394             	ExchangeHalo(A, x);
     1395             #endif
     1396             
     1397             	const double * const rv = r.values;
     1398             	double * const xv = x.values;
     1399             	double * const yv = y.values;
     1400             	double **matrixDiagonal = A.matrixDiagonal;
     1401             
     1402             	/*
     1403             	 * FORWARD
     1404             	 */
     1405    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1406             #ifndef HPCG_NO_OPENMP
     1407             #pragma omp parallel for SCHEDULE(runtime)
     1408             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1409   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
     1410   p         			local_int_t row = A.tdg[l][i];
     1411   p         			const double * const currentValues = A.matrixValues[row];
     1412   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1413   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1414   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1415   p         			double sum = rv[row];
     1416             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1417   p     8v  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j++ ) {
     1418   p     8v  				local_int_t curCol = currentColIndices[j];
     1419   p     8v  				sum -= currentValues[j] * xv[curCol];
     1420   p     8v  			}
     1421   p         			sum += xv[row] * currentDiagonal;
     1422   p         			xv[row] = sum / currentDiagonal;
     1423   pi        		}
     1424    i        	}
     1425             
     1426             	/*
     1427             	 * BACKWARD (fusing SYMGS and SPMV)
     1428             	 */
     1429    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1430             #ifndef HPCG_NO_OPENMP
     1431             #pragma omp parallel for SCHEDULE(runtime)
     1432             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1433   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
     1434   p         			local_int_t row = A.tdg[l][i];
     1435   p         			const double * const currentValues = A.matrixValues[row];
     1436   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1437   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1438   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1439   p         			double sum = 0.0;
     1440             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1441   p     8v  			for ( local_int_t j = currentNumberOfNonzeros-1; j >= 0; j-- ) {
     1442   p     8v  				local_int_t curCol = currentColIndices[j];
     1443   p     8v  				sum += currentValues[j] * xv[curCol];
     1444   p     8v  			}
     1445   p         			sum -= xv[row] * currentDiagonal;
     1446   p         			xv[row] = (rv[row] - sum) / currentDiagonal;
     1447   p         			sum += xv[row] * currentDiagonal;
     1448   p         			yv[row] = sum;
     1449   p         		}
     1450             	}
     1451             
     1452             	return 0;
     1453             }
     1454             
     1455             int ComputeSYMGS_TDG ( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
     1456             
     1457             	assert( x.localLength == A.localNumberOfColumns);
     1458             
     1459             #ifndef HPCG_NO_MPI
     1460             	ExchangeHalo(A,x);
     1461             #endif
     1462             
     1463             	const double * const rv = r.values;
     1464             	double * const xv = x.values;
     1465             	double **matrixDiagonal = A.matrixDiagonal;
     1466             
     1467             /*#ifndef HPCG_NO_OPENMP
     1468             #pragma omp parallel SCHEDULE(runtime)
     1469             {
     1470             #endif
     1471             */
     1472             #pragma statement scache_isolate_way L2=10
     1473             #pragma statement scache_isolate_assign xv
     1474             
     1475             	/*
     1476             	 * FORWARD
     1477             	 */
     1478    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1479             #ifndef HPCG_NO_OPENMP
     1480             #pragma omp parallel for SCHEDULE(runtime)
     1481             #endif
     1482             		#pragma loop unroll_and_jam(4)
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1483   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
     1484   p         			local_int_t row = A.tdg[l][i];
     1485   p         			const double * const currentValues = A.matrixValues[row];
     1486   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1487   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1488   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1489   p         			double sum = rv[row];
     1490             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1491   p     8v  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j++ ) {
     1492   p     8v  				local_int_t curCol = currentColIndices[j];
     1493   p     8v  				sum -= currentValues[j] * xv[curCol];
     1494   p     8v  			}
     1495   p         			sum += xv[row] * currentDiagonal;
     1496   p         			xv[row] = sum / currentDiagonal;
     1497   pi        		}
     1498    i        	}
     1499             
     1500             	/*
     1501             	 * BACKWARD
     1502             	 */
     1503    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1504             #ifndef HPCG_NO_OPENMP
     1505             #pragma omp parallel for SCHEDULE(runtime)
     1506             #endif
     1507             		#pragma loop unroll_and_jam(4)
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1508   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
     1509   p         			local_int_t row = A.tdg[l][i];
     1510   p         			const double * const currentValues = A.matrixValues[row];
     1511   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1512   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1513   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1514   p         			double sum = rv[row];
     1515             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1516   p     8v  			for ( local_int_t j = currentNumberOfNonzeros-1; j >= 0; j-- ) {
     1517   p     8v  				local_int_t curCol = currentColIndices[j];
     1518   p     8v  				sum -= currentValues[j] * xv[curCol];
     1519   p     8v  			}
     1520   p         			sum += xv[row] * currentDiagonal;
     1521   p         			xv[row] = sum / currentDiagonal;
     1522   p         		}
     1523             	}
     1524             
     1525             	#pragma statement end_scache_isolate_assign
     1526             	#pragma statement end_scache_isolate_way
     1527             /*#ifndef HPCG_NO_OPENMP
     1528             }
     1529             #endif*/
     1530             
     1531             	return 0;
     1532             }
     1533             
     1534             int ComputeSYMGS_BLOCK( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
     1535             
     1536             	assert(x.localLength >= A.localNumberOfColumns);
     1537             	
     1538             #ifndef HPCG_NO_MPI
     1539             	ExchangeHalo(A, x);
     1540             #endif
     1541             
     1542             	const local_int_t nrow = A.localNumberOfRows;
     1543             	double **matrixDiagonal = A.matrixDiagonal;
     1544             	const double * const rv = r.values;
     1545             	double * const xv = x.values;
     1546             
     1547             	local_int_t firstBlock = 0;
     1548    i        	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
     1549             	/*
     1550             	 * FORWARD
     1551             	 */
     1552             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) {
     1553             		if ( color > 0 ) {
     1554    i        			firstBlock += A.numberOfBlocksInColor[color-1];
     1555    i        			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
     1556             		}
     1557             #ifndef HPCG_NO_OPENMP
     1558             #pragma omp parallel for SCHEDULE(runtime)
     1559             #endif
     1560   p         		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) {
     1561   p         			local_int_t firstRow = block * A.blockSize;
     1562   p         			local_int_t firstChunk = firstRow / A.chunkSize;
     1563   p         			local_int_t lastChunk = (firstRow + A.blockSize*A.chunkSize) / A.chunkSize;
     1564             
     1565   p         			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) {
     1566   p         				local_int_t first = A.chunkSize * chunk;
     1567   p         				local_int_t last = first + A.chunkSize;
     1568             
     1569             				//for ( local_int_t i = first; i < last; i+= (A.chunkSize/2)) {
     1570   p         				local_int_t i = first;
     1571   p         				if ( A.chunkSize == 4 ) {
     1572   p         					double sum0 = rv[i+0];
     1573   p         					double sum1 = rv[i+1];
     1574   p         					double sum2 = rv[i+2];
     1575   p         					double sum3 = rv[i+3];
     1576             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1577   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1578   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1579   p      s  						sum1 -= A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
     1580   p      s  						sum2 -= A.matrixValues[i+2][j] * xv[A.mtxIndL[i+2][j]];
     1581   p      s  						sum3 -= A.matrixValues[i+3][j] * xv[A.mtxIndL[i+3][j]];
     1582   pi     s  					}
     1583   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1584   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1585   p         					sum1 += matrixDiagonal[i+1][0] * xv[i+1];
     1586   p         					xv[i+1] = sum1 / matrixDiagonal[i+1][0];
     1587   p         					sum2 += matrixDiagonal[i+2][0] * xv[i+2];
     1588   p         					xv[i+2] = sum2 / matrixDiagonal[i+2][0];
     1589   p         					sum3 += matrixDiagonal[i+3][0] * xv[i+3];
     1590   p         					xv[i+3] = sum3 / matrixDiagonal[i+3][0];
     1591   p         				} else if ( A.chunkSize == 2 ) {
     1592   p         					double sum0 = rv[i+0];
     1593   p         					double sum1 = rv[i+1];
     1594             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1595   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1596   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1597   p      s  						sum1 -= A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
     1598   pi     s  					}
     1599   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1600   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1601   p         					sum1 += matrixDiagonal[i+1][0] * xv[i+1];
     1602   p         					xv[i+1] = sum1 / matrixDiagonal[i+1][0];
     1603   p         				} else { // A.chunkSize == 1
     1604   p         					double sum0 = rv[i+0];
     1605             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1606   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1607   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1608   pi     s  					}
     1609   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1610   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1611   p         				}
     1612   p         			}
     1613   p         		}
     1614             	}
     1615             
     1616             	firstBlock = A.numberOfBlocks-1;
     1617    i        	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
     1618             	/*
     1619             	 * BACKWARD
     1620             	 */
     1621             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
     1622             		if ( color < A.numberOfColors-1 ) {
     1623    i        			firstBlock -= A.numberOfBlocksInColor[color+1];
     1624    i        			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
     1625             		}
     1626             #ifndef HPCG_NO_OPENMP
     1627             #pragma omp parallel for SCHEDULE(runtime)
     1628             #endif
     1629   p         		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) {
     1630   p         			local_int_t firstRow = ((block+1) * A.blockSize) - 1; // this is the last row of the last block
     1631   p         			local_int_t firstChunk = firstRow / A.chunkSize; // this is the  chunk of the row above
     1632   p         			local_int_t lastChunk = (firstRow - A.blockSize*A.chunkSize) / A.chunkSize; 
     1633             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1634   p         			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
     1635   p         				local_int_t first = A.chunkSize * chunk;
     1636   p         				local_int_t last = first + A.chunkSize;
     1637             
     1638             				//for ( local_int_t i = last-1; i >= first; i -= (A.chunkSize/2)) {
     1639   p         				local_int_t i = last-1;
     1640   p         				if ( A.chunkSize == 4 ) {
     1641   p         					double sum3 = rv[i-3];
     1642   p         					double sum2 = rv[i-2];
     1643   p         					double sum1 = rv[i-1];
     1644   p         					double sum0 = rv[i  ];
     1645             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.28, ITR: 32, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1646   pi     v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1647   p      v  						sum3 -= A.matrixValues[i-3][j] * xv[A.mtxIndL[i-3][j]];
     1648   p      v  						sum2 -= A.matrixValues[i-2][j] * xv[A.mtxIndL[i-2][j]];
     1649   p      v  						sum1 -= A.matrixValues[i-1][j] * xv[A.mtxIndL[i-1][j]];
     1650   p      v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1651   p      v  					}
     1652   p         					sum3 += matrixDiagonal[i-3][0] * xv[i-3];
     1653   p         					xv[i-3] = sum3 / matrixDiagonal[i-3][0];
     1654             
     1655   p         					sum2 += matrixDiagonal[i-2][0] * xv[i-2];
     1656   p         					xv[i-2] = sum2 / matrixDiagonal[i-2][0];
     1657             
     1658   p         					sum1 += matrixDiagonal[i-1][0] * xv[i-1];
     1659   p         					xv[i-1] = sum1 / matrixDiagonal[i-1][0];
     1660             
     1661   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1662   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1663   p         				} else if ( A.chunkSize == 2 ) {
     1664   p         					double sum1 = rv[i-1];
     1665   p         					double sum0 = rv[i  ];
     1666             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 96, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1667   pi    4v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1668   p     4v  						sum1 -= A.matrixValues[i-1][j] * xv[A.mtxIndL[i-1][j]];
     1669   p     4v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1670   p     4v  					}
     1671             
     1672   p         					sum1 += matrixDiagonal[i-1][0] * xv[i-1];
     1673   p         					xv[i-1] = sum1 / matrixDiagonal[i-1][0];
     1674             
     1675   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1676   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1677   p         				} else { // A.chunkSize == 1
     1678   p         					double sum0 = rv[i  ];
     1679             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1680   pi    8v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1681   p     8v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1682   p     8v  					}
     1683             
     1684   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1685   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1686   p         				}
     1687   p         			}
     1688   p         		}
     1689             	}
     1690             
     1691             	return 0;
     1692             }
     1693             //#endif
     1694             
     1695             
     1696             
     1697             /*!
     1698               Routine to compute one step of symmetric Gauss-Seidel:
     1699             
     1700               Assumption about the structure of matrix A:
     1701               - Each row 'i' of the matrix has nonzero diagonal value whose address is matrixDiagonal[i]
     1702               - Entries in row 'i' are ordered such that:
     1703                    - lower triangular terms are stored before the diagonal element.
     1704                    - upper triangular terms are stored after the diagonal element.
     1705                    - No other assumptions are made about entry ordering.
     1706             
     1707               Symmetric Gauss-Seidel notes:
     1708               - We use the input vector x as the RHS and start with an initial guess for y of all zeros.
     1709               - We perform one forward sweep.  Since y is initially zero we can ignore the upper triangular terms of A.
     1710               - We then perform one back sweep.
     1711                    - For simplicity we include the diagonal contribution in the for-j loop, then correct the sum after
     1712             
     1713               @param[in] A the known system matrix
     1714               @param[in] r the input vector
     1715               @param[inout] x On entry, x should contain relevant values, on exit x contains the result of one symmetric GS sweep with r as the RHS.
     1716             
     1717               @return returns 0 upon success and non-zero otherwise
     1718             
     1719               @warning Early versions of this kernel (Version 1.1 and earlier) had the r and x arguments in reverse order, and out of sync with other kernels.
     1720             
     1721               @see ComputeSYMGS_ref
     1722             */
     1723             int ComputeSYMGS( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace) {
     1724             
     1725             	// This function is just a stub right now which decides which implementation of the SYMGS will be executed (TDG or block coloring)
     1726             	if ( A.TDG ) {
     1727             #ifdef HPCG_USE_NEON
     1728             		return ComputeSYMGS_TDG_NEON(A, r, x);
     1729             #elif defined HPCG_USE_SVE
     1730             		return ComputeSYMGS_TDG_SVE(A, r, x, trace);
     1731             #else
     1732             		return ComputeSYMGS_TDG(A, r, x, trace);
     1733             #endif
     1734             	}
     1735             #ifdef HPCG_USE_NEON
     1736             	return ComputeSYMGS_BLOCK_NEON(A, r, x);
     1737             #elif defined HPCG_USE_SVE
     1738             	return ComputeSYMGS_BLOCK_SVE(A, r, x, trace);
     1739             #else
     1740             	return ComputeSYMGS_BLOCK(A, r, x, trace);
     1741             #endif
     1742             }
     1743             
     1744             inline void SYMGS_VERSION_1(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     1745             	
     1746             	double **matrixDiagonal = A.matrixDiagonal;
     1747             
     1748             	/*
     1749             	 * FORWARD SWEEP
     1750             	 */
     1751             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1752             		local_int_t tdgLevelSize = A.tdg[l].size();
     1753             		if((tdgLevelSize%2) == 0) {
     1754             #ifndef HPCG_NO_OPENMP
     1755             #pragma omp parallel for SCHEDULE(runtime)
     1756             #endif
     1757             		for ( local_int_t i = 0; i < tdgLevelSize; i+=2 ) {
     1758             			local_int_t row_1 = A.tdg[l][i];
     1759             			local_int_t row_2 = A.tdg[l][i+1];
     1760             			const double * const currentValues_1 = A.matrixValues[row_1];
     1761             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1762             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1763             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1764             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1765             
     1766             			const double * const currentValues_2 = A.matrixValues[row_2];
     1767             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1768             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1769             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1770             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1771             			
     1772             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     1773             
     1774             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1775             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1776             				
     1777             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1778             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1779             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1780             
     1781             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1782             
     1783             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1784             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1785             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1786             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1787             
     1788             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1789             			}
     1790             
     1791             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1792             			double sum_1 = rv[row_1] - totalContribution_1;
     1793             
     1794             			sum_1 += xv[row_1] * currentDiagonal_1;
     1795             			xv[row_1] = sum_1 / currentDiagonal_1;
     1796             
     1797             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1798             			double sum_2 = rv[row_2] - totalContribution_2;
     1799             
     1800             			sum_2 += xv[row_2] * currentDiagonal_2;
     1801             			xv[row_2] = sum_2 / currentDiagonal_2;
     1802             		}
     1803             		}
     1804             		else
     1805             		{
     1806             #ifndef HPCG_NO_OPENMP
     1807             #pragma omp parallel for SCHEDULE(runtime)
     1808             #endif
     1809             		for ( local_int_t i = 0; i < tdgLevelSize; i++ ) {
     1810             			local_int_t row = A.tdg[l][i];
     1811             			const double * const currentValues = A.matrixValues[row];
     1812             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1813             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1814             			const double currentDiagonal = matrixDiagonal[row][0];
     1815             			svfloat64_t contribs = svdup_f64(0.0);
     1816             
     1817             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1818             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1819             				
     1820             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1821             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1822             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1823             
     1824             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1825             			}
     1826             
     1827             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1828             			double sum = rv[row] - totalContribution;
     1829             
     1830             			sum += xv[row] * currentDiagonal;
     1831             			xv[row] = sum / currentDiagonal;
     1832             		}
     1833             		}
     1834             	}
     1835             
     1836             	/*
     1837             	 * BACKWARD SWEEP
     1838             	 */
     1839             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1840             		local_int_t tdgLevelSize = A.tdg[l].size();
     1841             		if((tdgLevelSize%2) == 0) {		
     1842             #ifndef HPCG_NO_OPENMP
     1843             #pragma omp parallel for SCHEDULE(runtime)
     1844             #endif
     1845             		for ( local_int_t i = tdgLevelSize-1; i >= 0; i-= 2 ) {
     1846             			local_int_t row_1 = A.tdg[l][i];
     1847             			local_int_t row_2 = A.tdg[l][i-1];
     1848             			const double * const currentValues_1 = A.matrixValues[row_1];
     1849             			const double * const currentValues_2 = A.matrixValues[row_2];
     1850             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1851             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1852             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1853             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1854             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1855             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1856             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1857             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1858             
     1859             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     1860             							
     1861             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1862             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1863             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1864             				
     1865             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1866             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1867             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1868             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1869             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1870             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1871             
     1872             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1873             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1874             			}
     1875             
     1876             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1877             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1878             			double sum_1 = rv[row_1] - totalContribution_1;
     1879             			double sum_2 = rv[row_2] - totalContribution_2;
     1880             
     1881             			sum_1 += xv[row_1] * currentDiagonal_1;
     1882             			sum_2 += xv[row_2] * currentDiagonal_2;
     1883             			xv[row_1] = sum_1 / currentDiagonal_1;
     1884             			xv[row_2] = sum_2 / currentDiagonal_2;
     1885             		}
     1886             		}
     1887             		else
     1888             		{
     1889             #ifndef HPCG_NO_OPENMP
     1890             #pragma omp parallel for SCHEDULE(runtime)
     1891             #endif
     1892             		for ( local_int_t i = tdgLevelSize-1; i >= 0; i-- ) {
     1893             			local_int_t row = A.tdg[l][i];
     1894             			const double * const currentValues = A.matrixValues[row];
     1895             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1896             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1897             			const double currentDiagonal = matrixDiagonal[row][0];
     1898             			svfloat64_t contribs = svdup_f64(0.0);
     1899             
     1900             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1901             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1902             				
     1903             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1904             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1905             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1906             
     1907             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1908             			}
     1909             
     1910             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1911             			double sum = rv[row] - totalContribution;
     1912             
     1913             			sum += xv[row] * currentDiagonal;
     1914             			xv[row] = sum / currentDiagonal;
     1915             		}
     1916             		}
     1917             	}
     1918             }
     1919             
     1920             inline void SYMGS_VERSION_2(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     1921             	
     1922             	double **matrixDiagonal = A.matrixDiagonal;
     1923             
     1924             	/*
     1925             	 * FORWARD SWEEP
     1926             	 */
     1927             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1928             		local_int_t tdgLevelSize = A.tdg[l].size();
     1929             		local_int_t maxLevelSize = 2*(tdgLevelSize / 2);
     1930             
     1931             #ifndef HPCG_NO_OPENMP
     1932             	#pragma omp parallel
     1933             	{
     1934             	#pragma omp for nowait SCHEDULE(runtime)
     1935             #endif
     1936             		for ( local_int_t i = 0; i < maxLevelSize; i+=2 ) {
     1937             			local_int_t row_1 = A.tdg[l][i];
     1938             			local_int_t row_2 = A.tdg[l][i+1];
     1939             			const double * const currentValues_1 = A.matrixValues[row_1];
     1940             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1941             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1942             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1943             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1944             
     1945             			const double * const currentValues_2 = A.matrixValues[row_2];
     1946             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1947             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1948             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1949             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1950             			
     1951             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     1952             
     1953             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1954             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1955             				
     1956             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1957             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1958             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1959             
     1960             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1961             
     1962             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1963             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1964             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1965             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1966             
     1967             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1968             			}
     1969             
     1970             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1971             			double sum_1 = rv[row_1] - totalContribution_1;
     1972             
     1973             			sum_1 += xv[row_1] * currentDiagonal_1;
     1974             			xv[row_1] = sum_1 / currentDiagonal_1;
     1975             
     1976             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1977             			double sum_2 = rv[row_2] - totalContribution_2;
     1978             
     1979             			sum_2 += xv[row_2] * currentDiagonal_2;
     1980             			xv[row_2] = sum_2 / currentDiagonal_2;
     1981             		}
     1982             
     1983             		#pragma omp single 
     1984             		if (maxLevelSize < tdgLevelSize) {
     1985             			local_int_t i = maxLevelSize;
     1986             
     1987             			local_int_t row = A.tdg[l][i];
     1988             			const double * const currentValues = A.matrixValues[row];
     1989             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1990             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1991             			const double currentDiagonal = matrixDiagonal[row][0];
     1992             			svfloat64_t contribs = svdup_f64(0.0);
     1993             
     1994             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1995             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1996             				
     1997             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1998             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1999             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2000             
     2001             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2002             			}
     2003             
     2004             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2005             			double sum = rv[row] - totalContribution;
     2006             
     2007             			sum += xv[row] * currentDiagonal;
     2008             			xv[row] = sum / currentDiagonal;
     2009             		}
     2010             #ifndef HPCG_NO_OPENMP
     2011             	}
     2012             #endif
     2013             	}
     2014             
     2015             	/*
     2016             	 * BACKWARD SWEEP
     2017             	 */
     2018             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     2019             		local_int_t tdgLevelSize = A.tdg[l].size();
     2020             		local_int_t maxLevelSize = 2*(tdgLevelSize / 2);
     2021             
     2022             #ifndef HPCG_NO_OPENMP
     2023             #pragma omp parallel 
     2024             	{
     2025             		#pragma omp single nowait 
     2026             		{
     2027             #endif
     2028             		if (tdgLevelSize > maxLevelSize) {
     2029             			local_int_t i = maxLevelSize-1;
     2030             
     2031             			local_int_t row = A.tdg[l][i];
     2032             			const double * const currentValues = A.matrixValues[row];
     2033             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2034             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2035             			const double currentDiagonal = matrixDiagonal[row][0];
     2036             			svfloat64_t contribs = svdup_f64(0.0);
     2037             
     2038             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2039             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2040             				
     2041             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2042             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2043             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2044             
     2045             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2046             			}
     2047             
     2048             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2049             			double sum = rv[row] - totalContribution;
     2050             
     2051             			sum += xv[row] * currentDiagonal;
     2052             			xv[row] = sum / currentDiagonal;
     2053             		}
     2054             #ifndef HPCG_NO_OPENMP
     2055             		}
     2056             #pragma omp for SCHEDULE(runtime)
     2057             #endif
     2058             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 2 ) {
     2059             			local_int_t row_1 = A.tdg[l][i];
     2060             			local_int_t row_2 = A.tdg[l][i-1];
     2061             			const double * const currentValues_1 = A.matrixValues[row_1];
     2062             			const double * const currentValues_2 = A.matrixValues[row_2];
     2063             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2064             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2065             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2066             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2067             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2068             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2069             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2070             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2071             
     2072             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2073             							
     2074             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2075             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2076             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2077             				
     2078             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2079             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2080             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2081             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2082             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2083             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2084             
     2085             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2086             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2087             			}
     2088             
     2089             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2090             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2091             			double sum_1 = rv[row_1] - totalContribution_1;
     2092             			double sum_2 = rv[row_2] - totalContribution_2;
     2093             
     2094             			sum_1 += xv[row_1] * currentDiagonal_1;
     2095             			sum_2 += xv[row_2] * currentDiagonal_2;
     2096             			xv[row_1] = sum_1 / currentDiagonal_1;
     2097             			xv[row_2] = sum_2 / currentDiagonal_2;
     2098             		}
     2099             #ifndef HPCG_NO_OPENMP
     2100             	}
     2101             #endif
     2102             	}
     2103             }
     2104             
     2105             inline void SYMGS_VERSION_3(const SparseMatrix& A, double * const& prxv, const double * const& rv) {
     2106             	
     2107             	double **matrixDiagonal = A.matrixDiagonal;
     2108             	double *xv = prxv;
     2109             
     2110             //#pragma statement scache_isolate_way L2=10
     2111             //#pragma statement scache_isolate_assign xv
     2112             #ifndef HPCG_NO_OPENMP
     2113             	#pragma omp parallel
     2114             	{
     2115             		local_int_t numThreads = omp_get_num_threads();
     2116             #endif
     2117             	/*
     2118             	 * FORWARD SWEEP
     2119             	 */
     2120             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     2121             		local_int_t tdgLevelSize = A.tdg[l].size();
     2122             		local_int_t maxLevelSize = 4*(tdgLevelSize / 4);
     2123             
     2124             #ifdef MANUAL_TASK_DISTRIBUTION
     2125             		//at least 8 tasks per thread 
     2126             		local_int_t maxTasksPerThread = std::max((tdgLevelSize+numThreads-1)/numThreads, 8);
     2127             		local_int_t groupedTasksPerThread = ((maxTasksPerThread+7)/8)*8;
     2128             		local_int_t threadId = omp_get_thread_num();
     2129             		local_int_t minValue = groupedTasksPerThread*threadId;
     2130             		local_int_t maxValue = minValue+groupedTasksPerThread;
     2131             		maxLevelSize = std::min(maxValue, maxLevelSize);
     2132             		tdgLevelSize = std::min(maxValue, tdgLevelSize);
     2133             
     2134             		#pragma fj loop zfill			
     2135             		#pragma loop nounroll
     2136             		for ( local_int_t i = minValue; i < maxLevelSize; i+=4 ) {
     2137             #else
     2138             #ifndef HPCG_NO_OPENMP
     2139             	#pragma fj loop zfill			
     2140             	#pragma loop nounroll
     2141             	#pragma omp for nowait SCHEDULE(runtime)
     2142             #endif
     2143             		for ( local_int_t i = 0; i < maxLevelSize; i+=4 ) {
     2144             #endif
     2145             			local_int_t row_1 = A.tdg[l][i];
     2146             			local_int_t row_2 = A.tdg[l][i+1];
     2147             			local_int_t row_3 = A.tdg[l][i+2];
     2148             			local_int_t row_4 = A.tdg[l][i+3];
     2149             			const double * const currentValues_1 = A.matrixValues[row_1];
     2150             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2151             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2152             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2153             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2154             
     2155             			const double * const currentValues_2 = A.matrixValues[row_2];
     2156             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2157             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2158             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2159             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2160             
     2161             			const double * const currentValues_3 = A.matrixValues[row_3];
     2162             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2163             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2164             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2165             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2166             
     2167             			const double * const currentValues_4 = A.matrixValues[row_4];
     2168             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2169             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2170             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2171             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2172             
     2173             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     2174             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);
     2175             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);
     2176             
     2177             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2178             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2179             				
     2180             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2181             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2182             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2183             
     2184             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2185             
     2186             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2187             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2188             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2189             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2190             
     2191             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2192             
     2193             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2194             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2195             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2196             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2197             
     2198             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2199             
     2200             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2201             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2202             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2203             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2204             
     2205             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2206             			}
     2207             
     2208             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2209             			double sum_1 = rv[row_1] - totalContribution_1;
     2210             
     2211             			sum_1 += xv[row_1] * currentDiagonal_1;
     2212             			xv[row_1] = sum_1 / currentDiagonal_1;
     2213             
     2214             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2215             			double sum_2 = rv[row_2] - totalContribution_2;
     2216             
     2217             			sum_2 += xv[row_2] * currentDiagonal_2;
     2218             			xv[row_2] = sum_2 / currentDiagonal_2;
     2219             
     2220             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2221             			double sum_3 = rv[row_3] - totalContribution_3;
     2222             
     2223             			sum_3 += xv[row_3] * currentDiagonal_3;
     2224             			xv[row_3] = sum_3 / currentDiagonal_3;
     2225             
     2226             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2227             			double sum_4 = rv[row_4] - totalContribution_4;
     2228             
     2229             			sum_4 += xv[row_4] * currentDiagonal_4;
     2230             			xv[row_4] = sum_4 / currentDiagonal_4;
     2231             		}
     2232             
     2233             //#pragma omp single
     2234             		if (maxLevelSize < tdgLevelSize) {
     2235             /************
     2236             #ifndef HPCG_NO_OPENMP
     2237             //#pragma loop nounroll
     2238             #pragma omp for SCHEDULE(runtime)
     2239             #endif
     2240             		for ( local_int_t i = maxLevelSize; i < tdgLevelSize; i++ ) {
     2241             
     2242             			local_int_t row = A.tdg[l][i];
     2243             			const double * const currentValues = A.matrixValues[row];
     2244             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2245             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2246             			const double currentDiagonal = matrixDiagonal[row][0];
     2247             			svfloat64_t contribs = svdup_f64(0.0);
     2248             
     2249             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2250             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2251             				
     2252             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2253             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2254             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2255             
     2256             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2257             			}
     2258             
     2259             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2260             			double sum = rv[row] - totalContribution;
     2261             
     2262             			sum += xv[row] * currentDiagonal;
     2263             			xv[row] = sum / currentDiagonal;
     2264             		}
     2265             *******/
     2266             		#pragma omp sections nowait
     2267             		{
     2268             			#pragma fj loop zfill			
     2269             			#pragma omp section 
     2270             			{
     2271             				local_int_t i = maxLevelSize;
     2272             				local_int_t row = A.tdg[l][i];
     2273             				const double * const currentValues = A.matrixValues[row];
     2274             				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2275             				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2276             				const double currentDiagonal = matrixDiagonal[row][0];
     2277             				svfloat64_t contribs = svdup_f64(0.0);
     2278             
     2279             				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2280             					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2281             					
     2282             					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2283             					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2284             					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2285             
     2286             					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2287             				}
     2288             
     2289             				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2290             				double sum = rv[row] - totalContribution;
     2291             
     2292             				sum += xv[row] * currentDiagonal;
     2293             				xv[row] = sum / currentDiagonal;
     2294             			}
     2295             			#pragma fj loop zfill			
     2296             			#pragma omp section 
     2297             			{
     2298             				local_int_t i = maxLevelSize + 1;
     2299             				if (i < tdgLevelSize) {
     2300             				local_int_t row = A.tdg[l][i];
     2301             				const double * const currentValues = A.matrixValues[row];
     2302             				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2303             				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2304             				const double currentDiagonal = matrixDiagonal[row][0];
     2305             				svfloat64_t contribs = svdup_f64(0.0);
     2306             
     2307             				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2308             					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2309             					
     2310             					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2311             					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2312             					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2313             
     2314             					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2315             				}
     2316             
     2317             				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2318             				double sum = rv[row] - totalContribution;
     2319             
     2320             				sum += xv[row] * currentDiagonal;
     2321             				xv[row] = sum / currentDiagonal;
     2322             				}
     2323             			}
     2324             			#pragma fj loop zfill			
     2325             			#pragma omp section 
     2326             			{
     2327             				local_int_t i = maxLevelSize + 2;
     2328             				if (i < tdgLevelSize) {
     2329             				local_int_t row = A.tdg[l][i];
     2330             				const double * const currentValues = A.matrixValues[row];
     2331             				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2332             				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2333             				const double currentDiagonal = matrixDiagonal[row][0];
     2334             				svfloat64_t contribs = svdup_f64(0.0);
     2335             
     2336             				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2337             					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2338             					
     2339             					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2340             					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2341             					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2342             
     2343             					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2344             				}
     2345             
     2346             				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2347             				double sum = rv[row] - totalContribution;
     2348             
     2349             				sum += xv[row] * currentDiagonal;
     2350             				xv[row] = sum / currentDiagonal;
     2351             				}
     2352             			}
     2353             		}
     2354             
     2355             /***********/
     2356             #ifndef HPCG_NO_OPENMP
     2357             	}
     2358             	#pragma omp barrier
     2359             #endif
     2360             	}
     2361             
     2362             	/*
     2363             	 * BACKWARD SWEEP
     2364             	 */
     2365             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     2366             		local_int_t tdgLevelSize = A.tdg[l].size();
     2367             		local_int_t maxLevelSize = 4*(tdgLevelSize / 4);
     2368             
     2369             #ifndef HPCG_NO_OPENMP
     2370             		//#pragma omp single nowait 
     2371             		//{
     2372             		#pragma fj loop zfill			
     2373             		#pragma loop nounroll
     2374             		#pragma omp for nowait SCHEDULE(runtime)
     2375             #endif
     2376             		for ( local_int_t i = tdgLevelSize-1; i >= maxLevelSize; i-- ) {
     2377             			local_int_t row = A.tdg[l][i];
     2378             			const double * const currentValues = A.matrixValues[row];
     2379             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2380             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2381             			const double currentDiagonal = matrixDiagonal[row][0];
     2382             			svfloat64_t contribs = svdup_f64(0.0);
     2383             
     2384             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2385             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2386             				
     2387             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2388             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2389             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2390             
     2391             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2392             			}
     2393             
     2394             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2395             			double sum = rv[row] - totalContribution;
     2396             
     2397             			sum += xv[row] * currentDiagonal;
     2398             			xv[row] = sum / currentDiagonal;
     2399             		}
     2400             
     2401             #ifndef HPCG_NO_OPENMP
     2402             		//}
     2403             #pragma fj loop zfill			
     2404             #pragma loop nounroll
     2405             #pragma omp for SCHEDULE(runtime)
     2406             #endif
     2407             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 4 ) {
     2408             			local_int_t row_1 = A.tdg[l][i];
     2409             			local_int_t row_2 = A.tdg[l][i-1];
     2410             			local_int_t row_3 = A.tdg[l][i-2];
     2411             			local_int_t row_4 = A.tdg[l][i-3];
     2412             			const double * const currentValues_1 = A.matrixValues[row_1];
     2413             			const double * const currentValues_2 = A.matrixValues[row_2];
     2414             			const double * const currentValues_3 = A.matrixValues[row_3];
     2415             			const double * const currentValues_4 = A.matrixValues[row_4];
     2416             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2417             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2418             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2419             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2420             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2421             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2422             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2423             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2424             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2425             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2426             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2427             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2428             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2429             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2430             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2431             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2432             
     2433             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2434             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);				
     2435             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);				
     2436             							
     2437             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2438             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2439             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2440             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2441             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2442             				
     2443             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2444             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2445             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2446             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2447             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2448             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2449             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2450             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2451             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2452             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2453             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2454             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2455             
     2456             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2457             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2458             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2459             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2460             			}
     2461             
     2462             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2463             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2464             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2465             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2466             			double sum_1 = rv[row_1] - totalContribution_1;
     2467             			double sum_2 = rv[row_2] - totalContribution_2;
     2468             			double sum_3 = rv[row_3] - totalContribution_3;
     2469             			double sum_4 = rv[row_4] - totalContribution_4;
     2470             
     2471             			sum_1 += xv[row_1] * currentDiagonal_1;
     2472             			sum_2 += xv[row_2] * currentDiagonal_2;
     2473             			sum_3 += xv[row_3] * currentDiagonal_3;
     2474             			sum_4 += xv[row_4] * currentDiagonal_4;
     2475             			xv[row_1] = sum_1 / currentDiagonal_1;
     2476             			xv[row_2] = sum_2 / currentDiagonal_2;
     2477             			xv[row_3] = sum_3 / currentDiagonal_3;
     2478             			xv[row_4] = sum_4 / currentDiagonal_4;
     2479             		}
     2480             #ifndef HPCG_NO_OPENMP
     2481             	}
     2482             #endif
     2483             	}
     2484             
     2485             //#pragma statement end_scache_isolate_assign
     2486             //#pragma statement end_scache_isolate_way	
     2487             }
     2488             /////////////
     2489             inline void SYMGS_VERSION_4(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     2490             	
     2491             	double **matrixDiagonal = A.matrixDiagonal;
     2492             
     2493             	/*
     2494             	 * FORWARD SWEEP
     2495             	 */
     2496             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     2497             		local_int_t tdgLevelSize = A.tdg[l].size();
     2498             		local_int_t maxLevelSize = 6*(tdgLevelSize / 6);
     2499             
     2500             #ifndef HPCG_NO_OPENMP
     2501             	#pragma omp parallel
     2502             	{
     2503             	#pragma omp for nowait SCHEDULE(runtime)
     2504             #endif
     2505             		for ( local_int_t i = 0; i < maxLevelSize; i+=6 ) {
     2506             			local_int_t row_1 = A.tdg[l][i];
     2507             			local_int_t row_2 = A.tdg[l][i+1];
     2508             			local_int_t row_3 = A.tdg[l][i+2];
     2509             			local_int_t row_4 = A.tdg[l][i+3];
     2510             			local_int_t row_5 = A.tdg[l][i+4];
     2511             			local_int_t row_6 = A.tdg[l][i+5];
     2512             			const double * const currentValues_1 = A.matrixValues[row_1];
     2513             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2514             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2515             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2516             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2517             
     2518             			const double * const currentValues_2 = A.matrixValues[row_2];
     2519             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2520             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2521             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2522             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2523             
     2524             			const double * const currentValues_3 = A.matrixValues[row_3];
     2525             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2526             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2527             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2528             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2529             
     2530             			const double * const currentValues_4 = A.matrixValues[row_4];
     2531             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2532             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2533             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2534             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2535             
     2536             			const double * const currentValues_5 = A.matrixValues[row_5];
     2537             			const local_int_t * const currentColIndices_5 = A.mtxIndL[row_5];
     2538             			const int currentNumberOfNonzeros_5 = A.nonzerosInRow[row_5];
     2539             			const double currentDiagonal_5 = matrixDiagonal[row_5][0];
     2540             			svfloat64_t contribs_5 = svdup_f64(0.0);
     2541             
     2542             			const double * const currentValues_6 = A.matrixValues[row_6];
     2543             			const local_int_t * const currentColIndices_6 = A.mtxIndL[row_6];
     2544             			const int currentNumberOfNonzeros_6 = A.nonzerosInRow[row_6];
     2545             			const double currentDiagonal_6 = matrixDiagonal[row_6][0];
     2546             			svfloat64_t contribs_6 = svdup_f64(0.0);
     2547             
     2548             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     2549             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);
     2550             			const int maxNumberOfNonzeros3 = std::max(currentNumberOfNonzeros_5, currentNumberOfNonzeros_6);
     2551             			const int maxNumberOfNonzeros4 = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);
     2552             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros4, maxNumberOfNonzeros3);
     2553             
     2554             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2555             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);		
     2556             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2557             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2558             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2559             
     2560             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2561             
     2562             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2563             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2564             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2565             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2566             
     2567             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2568             
     2569             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2570             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2571             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2572             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2573             
     2574             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2575             
     2576             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2577             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2578             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2579             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2580             
     2581             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2582             
     2583             				svbool_t pg_5 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_5);
     2584             				svfloat64_t mtxValues_5 = svld1_f64(pg_5, &currentValues_5[j]);
     2585             				svuint64_t indices_5 = svld1sw_u64(pg_5, &currentColIndices_5[j]);
     2586             				svfloat64_t xvv_5 = svld1_gather_u64index_f64(pg_5, xv, indices_5);
     2587             
     2588             				contribs_5 = svmla_f64_m(pg_5, contribs_5, xvv_5, mtxValues_5);
     2589             
     2590             				svbool_t pg_6 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_6);
     2591             				svfloat64_t mtxValues_6 = svld1_f64(pg_6, &currentValues_6[j]);
     2592             				svuint64_t indices_6 = svld1sw_u64(pg_6, &currentColIndices_6[j]);
     2593             				svfloat64_t xvv_6 = svld1_gather_u64index_f64(pg_6, xv, indices_6);
     2594             
     2595             				contribs_6 = svmla_f64_m(pg_6, contribs_6, xvv_6, mtxValues_6);
     2596             			}
     2597             
     2598             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2599             			double sum_1 = rv[row_1] - totalContribution_1;
     2600             
     2601             			sum_1 += xv[row_1] * currentDiagonal_1;
     2602             			xv[row_1] = sum_1 / currentDiagonal_1;
     2603             
     2604             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2605             			double sum_2 = rv[row_2] - totalContribution_2;
     2606             
     2607             			sum_2 += xv[row_2] * currentDiagonal_2;
     2608             			xv[row_2] = sum_2 / currentDiagonal_2;
     2609             
     2610             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2611             			double sum_3 = rv[row_3] - totalContribution_3;
     2612             
     2613             			sum_3 += xv[row_3] * currentDiagonal_3;
     2614             			xv[row_3] = sum_3 / currentDiagonal_3;
     2615             
     2616             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2617             			double sum_4 = rv[row_4] - totalContribution_4;
     2618             
     2619             			sum_4 += xv[row_4] * currentDiagonal_4;
     2620             			xv[row_4] = sum_4 / currentDiagonal_4;
     2621             
     2622             			double totalContribution_5 = svaddv_f64(svptrue_b64(), contribs_5);
     2623             			double sum_5 = rv[row_5] - totalContribution_5;
     2624             
     2625             			sum_5 += xv[row_5] * currentDiagonal_5;
     2626             			xv[row_5] = sum_5 / currentDiagonal_5;
     2627             
     2628             			double totalContribution_6 = svaddv_f64(svptrue_b64(), contribs_6);
     2629             			double sum_6 = rv[row_6] - totalContribution_6;
     2630             
     2631             			sum_6 += xv[row_6] * currentDiagonal_6;
     2632             			xv[row_6] = sum_6 / currentDiagonal_6;
     2633             		}
     2634             
     2635             //#pragma omp single
     2636             		if (maxLevelSize < tdgLevelSize) {
     2637             #ifndef HPCG_NO_OPENMP
     2638             #pragma omp for SCHEDULE(runtime)
     2639             #endif
     2640             		for ( local_int_t i = maxLevelSize; i < tdgLevelSize; i++ ) {
     2641             
     2642             			local_int_t row = A.tdg[l][i];
     2643             			const double * const currentValues = A.matrixValues[row];
     2644             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2645             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2646             			const double currentDiagonal = matrixDiagonal[row][0];
     2647             			svfloat64_t contribs = svdup_f64(0.0);
     2648             
     2649             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2650             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2651             				
     2652             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2653             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2654             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2655             
     2656             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2657             			}
     2658             
     2659             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2660             			double sum = rv[row] - totalContribution;
     2661             
     2662             			sum += xv[row] * currentDiagonal;
     2663             			xv[row] = sum / currentDiagonal;
     2664             		}
     2665             		}
     2666             #ifndef HPCG_NO_OPENMP
     2667             	}
     2668             #endif
     2669             	}
     2670             
     2671             	/*
     2672             	 * BACKWARD SWEEP
     2673             	 */
     2674             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     2675             		local_int_t tdgLevelSize = A.tdg[l].size();
     2676             		local_int_t maxLevelSize = 6*(tdgLevelSize / 6);
     2677             
     2678             #ifndef HPCG_NO_OPENMP
     2679             #pragma omp parallel 
     2680             	{
     2681             		//#pragma omp single nowait 
     2682             		//{
     2683             		#pragma omp for nowait SCHEDULE(runtime)
     2684             #endif
     2685             		for ( local_int_t i = tdgLevelSize-1; i >= maxLevelSize; i-- ) {
     2686             
     2687             			local_int_t row = A.tdg[l][i];
     2688             			const double * const currentValues = A.matrixValues[row];
     2689             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2690             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2691             			const double currentDiagonal = matrixDiagonal[row][0];
     2692             			svfloat64_t contribs = svdup_f64(0.0);
     2693             
     2694             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2695             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2696             				
     2697             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2698             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2699             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2700             
     2701             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2702             			}
     2703             
     2704             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2705             			double sum = rv[row] - totalContribution;
     2706             
     2707             			sum += xv[row] * currentDiagonal;
     2708             			xv[row] = sum / currentDiagonal;
     2709             		}
     2710             #ifndef HPCG_NO_OPENMP
     2711             		//}
     2712             #pragma omp for SCHEDULE(runtime)
     2713             #endif
     2714             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 6 ) {
     2715             			local_int_t row_1 = A.tdg[l][i];
     2716             			local_int_t row_2 = A.tdg[l][i-1];
     2717             			local_int_t row_3 = A.tdg[l][i-2];
     2718             			local_int_t row_4 = A.tdg[l][i-3];
     2719             			local_int_t row_5 = A.tdg[l][i-4];
     2720             			local_int_t row_6 = A.tdg[l][i-5];
     2721             			const double * const currentValues_1 = A.matrixValues[row_1];
     2722             			const double * const currentValues_2 = A.matrixValues[row_2];
     2723             			const double * const currentValues_3 = A.matrixValues[row_3];
     2724             			const double * const currentValues_4 = A.matrixValues[row_4];
     2725             			const double * const currentValues_5 = A.matrixValues[row_5];
     2726             			const double * const currentValues_6 = A.matrixValues[row_6];
     2727             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2728             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2729             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2730             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2731             			const local_int_t * const currentColIndices_5 = A.mtxIndL[row_5];
     2732             			const local_int_t * const currentColIndices_6 = A.mtxIndL[row_6];
     2733             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2734             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2735             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2736             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2737             			const int currentNumberOfNonzeros_5 = A.nonzerosInRow[row_5];
     2738             			const int currentNumberOfNonzeros_6 = A.nonzerosInRow[row_6];
     2739             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2740             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2741             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2742             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2743             			const double currentDiagonal_5 = matrixDiagonal[row_5][0];
     2744             			const double currentDiagonal_6 = matrixDiagonal[row_6][0];
     2745             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2746             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2747             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2748             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2749             			svfloat64_t contribs_5 = svdup_f64(0.0);
     2750             			svfloat64_t contribs_6 = svdup_f64(0.0);
     2751             
     2752             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2753             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);				
     2754             			const int maxNumberOfNonzeros3 = std::max(currentNumberOfNonzeros_5, currentNumberOfNonzeros_6);				
     2755             			const int maxNumberOfNonzeros4 = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);				
     2756             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros3, maxNumberOfNonzeros4);				
     2757             							
     2758             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2759             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2760             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2761             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2762             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2763             				svbool_t pg_5 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_5);
     2764             				svbool_t pg_6 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_6);
     2765             				
     2766             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2767             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2768             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2769             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2770             				svfloat64_t mtxValues_5 = svld1_f64(pg_5, &currentValues_5[j]);
     2771             				svfloat64_t mtxValues_6 = svld1_f64(pg_6, &currentValues_6[j]);
     2772             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2773             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2774             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2775             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2776             				svuint64_t indices_5 = svld1sw_u64(pg_5, &currentColIndices_5[j]);
     2777             				svuint64_t indices_6 = svld1sw_u64(pg_6, &currentColIndices_6[j]);
     2778             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2779             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2780             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2781             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2782             				svfloat64_t xvv_5 = svld1_gather_u64index_f64(pg_5, xv, indices_5);
     2783             				svfloat64_t xvv_6 = svld1_gather_u64index_f64(pg_6, xv, indices_6);
     2784             
     2785             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2786             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2787             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2788             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2789             				contribs_5 = svmla_f64_m(pg_5, contribs_5, xvv_5, mtxValues_5);
     2790             				contribs_6 = svmla_f64_m(pg_6, contribs_6, xvv_6, mtxValues_6);
     2791             			}
     2792             
     2793             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2794             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2795             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2796             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2797             			double totalContribution_5 = svaddv_f64(svptrue_b64(), contribs_5);
     2798             			double totalContribution_6 = svaddv_f64(svptrue_b64(), contribs_6);
     2799             			double sum_1 = rv[row_1] - totalContribution_1;
     2800             			double sum_2 = rv[row_2] - totalContribution_2;
     2801             			double sum_3 = rv[row_3] - totalContribution_3;
     2802             			double sum_4 = rv[row_4] - totalContribution_4;
     2803             			double sum_5 = rv[row_5] - totalContribution_5;
     2804             			double sum_6 = rv[row_6] - totalContribution_6;
     2805             
     2806             			sum_1 += xv[row_1] * currentDiagonal_1;
     2807             			sum_2 += xv[row_2] * currentDiagonal_2;
     2808             			sum_3 += xv[row_3] * currentDiagonal_3;
     2809             			sum_4 += xv[row_4] * currentDiagonal_4;
     2810             			sum_5 += xv[row_5] * currentDiagonal_5;
     2811             			sum_6 += xv[row_6] * currentDiagonal_6;
     2812             			xv[row_1] = sum_1 / currentDiagonal_1;
     2813             			xv[row_2] = sum_2 / currentDiagonal_2;
     2814             			xv[row_3] = sum_3 / currentDiagonal_3;
     2815             			xv[row_4] = sum_4 / currentDiagonal_4;
     2816             			xv[row_5] = sum_5 / currentDiagonal_5;
     2817             			xv[row_6] = sum_6 / currentDiagonal_6;
     2818             		}
     2819             #ifndef HPCG_NO_OPENMP
     2820             	}
     2821             #endif
     2822             	}
     2823             }
Total prefetch num: 0
Optimization messages
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 125: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 127: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 127: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 138: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 138: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 139: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 139: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 152: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiEERKT_S3_S3_'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 154: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 154: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 194: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 194: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 201: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 201: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 222: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 227: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 231: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 231: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 232: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 232: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 239: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 239: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 323: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 327: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 327: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 328: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 328: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 335: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 335: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 350: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 350: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 351: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 356: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 360: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 360: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 361: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 361: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 368: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 368: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 410: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 419: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 420: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 433: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 456: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 456: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 512: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 512: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 548: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 548: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 573: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 579: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 580: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 594: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 617: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 617: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 673: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 673: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 709: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 709: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1405: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1409: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1409: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1410: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1410: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1417: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1417: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1417: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1419: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1423: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1423: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1424: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1429: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1433: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1433: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1434: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1434: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1441: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1441: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1441: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1443: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1478: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1483: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1483: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1484: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1484: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1491: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1491: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1491: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1493: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1497: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1497: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1498: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1503: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1508: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1508: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1509: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1509: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1516: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1516: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1516: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1518: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1548: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1554: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1555: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1577: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1577: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1577: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1582: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1595: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1595: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1595: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1598: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1606: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1606: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1606: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1608: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1617: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1623: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1624: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1646: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1646: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1646: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1646: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 32.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1667: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1667: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1667: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1667: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 96.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1668: Method of calculating sum or product is changed.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1669: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1680: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1680: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1680: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1680: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1681: Method of calculating sum or product is changed.
  jwd8101o-i  "/opt/FJSVstclanga/cp-1.0.21.01/bin/../include/libc++/v371/algorithm", line 2659: Inline expansion is applied to the user defined function '_ZNKSt3__16__lessIiiEclERKiS3_'.
  jwd8101o-i  "/opt/FJSVstclanga/cp-1.0.21.01/bin/../include/libc++/v371/algorithm", line 2667: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiNS_6__lessIiiEEEERKT_S5_S5_T0_'.
Statistics information
  Option information
    Command line options : -c -DHPCG_CONTIGUOUS_ARRAYS -DHPCG_NO_MPI -DENABLE_MG_COUNTERS -DHPCG_USE_SVE -DHPCG_MAN_OPT_DDOT -DDDOT_2_UNROLL -DWAXPBY_AUTO_OPT -DHPCG_MAN_OPT_SCHEDULE_ON -DHPCG_MAN_OPT_SPMV_UNROLL -DSPMV_4_UNROLL -Khpctag -Kzfill -I./src -I./src/OOKAMI_OMP_FJ -Kfast -KSVE -Kopenmp -ffast-math -funroll-loops -std=c++11 -ffp-contract=fast -march=armv8.2-a+sve -Kocl -Koptmsg=2 -Nlst=t -I../src -o src/ComputeSYMGS.o
    Effective options    : -g0 -mt -Qy -std=gnu++11 -x- -x=quick -O3 -Knoalias_const
                           -Kalign_loops -Knoarray_declaration_opt -Kassume=noshortloop
                           -Kassume=nomemory_bandwidth -Kassume=notime_saving_compilation
                           -Kcmodel=small -Keval -Keval_noconcurrent
                           -Knoextract_stride_store -Kfast_matmul -Knofenv_access
                           -Kfp_contract -Kfp_relaxed -Kfsimple -Kfz -Khpctag
                           -Kilfunc=procedure -Klargepage -Klib -Kloop_blocking
                           -Kloop_fission -Kloop_nofission_stripmining
                           -Kloop_fission_threshold=50 -Kloop_fusion -Kloop_interchange
                           -Kloop_part_simd -Kloop_perfect_nest -Kloop_noversioning
                           -Klooptype=f -Knomemalias -Kmfunc=1 -Kocl -Komitfp -Kopenmp
                           -Kopenmp_noassume_norecurrence
                           -Kopenmp_nocollapse_except_innermost
                           -Kopenmp_loop_variable=private -Kopenmp_noordered_reduction
                           -Knoopenmp_simd -Knooptlib_string -Koptmsg=2
                           -Knopc_relative_literal_loads -Knoparallel
                           -Kparallel_nofp_precision -Knopreex -Kprefetch_cache_level=all
                           -Kprefetch_noconditional -Kprefetch_noindirect -Kprefetch_noinfer
                           -Kprefetch_sequential=auto -Kprefetch_nostride -Kprefetch_strong
                           -Kprefetch_strong_L2 -Knopreload -Krdconv=1
                           -Kremove_inlinefunction -Knorestp -Ksch_post_ra -Ksch_pre_ra
                           -Ksibling_calls -Ksimd=auto -Ksimd_packed_promotion
                           -Ksimd_reduction_product -Ksimd_reg_size=512
                           -Ksimd_nouncounted_loop -Ksimd_use_multiple_structures
                           -Knostrict_aliasing -Knostriping -KA64FX -KARMV8_2_A -KSVE -Kswp
                           -Kswp_freg_rate=100 -Kswp_ireg_rate=100 -Kswp_preg_rate=100
                           -Kswp_policy=auto -Kunroll -Knounroll_and_jam -Kzfill
                           -Ncancel_overtime_compilation -Nnocoverage -Nexceptions -Nnofjcex
                           -Nfjprof -Nnohook_func -Nnohook_time -Nlibomp -Nline -Nlst=p
                           -Nlst=t -Nquickdbg=noheapchk -Nquickdbg=nosubchk -NRnotrap
                           -Nnoreordered_variable_stack -Nrt_notune -Nsetvalue=noheap
                           -Nsetvalue=nostack -Nsetvalue=noscalar -Nsetvalue=noarray
                           -Nsetvalue=nostruct -Nsrc -Nsta
