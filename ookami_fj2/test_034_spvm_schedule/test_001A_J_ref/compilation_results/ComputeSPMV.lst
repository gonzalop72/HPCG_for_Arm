Fujitsu C/C++ Version 4.7.0   Wed Oct  4 08:30:23 2023
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj2
  Source file       : ../src/ComputeSPMV.cpp
(line-no.)(optimize)
        1             /*
        2              *
        3              *  SPDX-License-Identifier: Apache-2.0
        4              *
        5              *  Copyright (C) 2019, Arm Limited and contributors
        6              *
        7              *  Licensed under the Apache License, Version 2.0 (the "License");
        8              *  you may not use this file except in compliance with the License.
        9              *  You may obtain a copy of the License at
       10              *
       11              *      http://www.apache.org/licenses/LICENSE-2.0
       12              *
       13              *  Unless required by applicable law or agreed to in writing, software
       14              *  distributed under the License is distributed on an "AS IS" BASIS,
       15              *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       16              *  See the License for the specific language governing permissions and
       17              *  limitations under the License.
       18              *
       19              */
       20             
       21             
       22             //@HEADER
       23             // ***************************************************
       24             //
       25             // HPCG: High Performance Conjugate Gradient Benchmark
       26             //
       27             // Contact:
       28             // Michael A. Heroux ( maherou@sandia.gov)
       29             // Jack Dongarra     (dongarra@eecs.utk.edu)
       30             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       31             //
       32             // ***************************************************
       33             //@HEADER
       34             
       35             /*!
       36               @file ComputeSPMV.cpp
       37             
       38               HPCG routine
       39               */
       40             
       41             #include "ComputeSPMV.hpp"
       42             #include "ComputeSPMV_ref.hpp"
       43             #include <cassert>
       44             #ifndef HPCG_NO_MPI
       45             #include "ExchangeHalo.hpp"
       46             #endif
       47             #ifdef HPCG_USE_NEON
       48             #include "arm_neon.h"
       49             #endif
       50             #ifdef HPCG_USE_SVE
       51             #include "arm_sve.h"
       52             #endif
       53             #ifdef HPCG_USE_ARMPL_SPMV
       54             #include "armpl_sparse.h"
       55             #endif
       56             
       57             /*!
       58               Routine to compute sparse matrix vector product y = Ax where:
       59             Precondition: First call exchange_externals to get off-processor values of x
       60             
       61             This routine calls the reference SpMV implementation by default, but
       62             can be replaced by a custom, optimized routine suited for
       63             the target system.
       64             
       65             @param[in]  A the known system matrix
       66             @param[in]  x the known vector
       67             @param[out] y the On exit contains the result: Ax.
       68             
       69             @return returns 0 upon success and non-zero otherwise
       70             
       71             @see ComputeSPMV_ref
       72             */
       73             
       74             /*#ifdef HPCG_MAN_OPT_SCHEDULE_ON*/
       75             	//#define SCHEDULE(T)	schedule(T)
       76             /*#elif defined(HPCG_MAN_SPVM_SCHEDULE)
       77             	#define SCHEDULE(T) schedule(static,720)
       78             /*#else
       79             	#define SCHEDULE(T)
       80             #endif*/
       81             #ifdef HPCG_MAN_SPVM_SCHEDULE_720
       82             	#define SCHEDULE(T) schedule(static,720)
       83             #elif defined(HPCG_MAN_SPVM_SCHEDULE_528)
       84             	#define SCHEDULE(T) schedule(static,720)
       85             #else
       86             	#define SCHEDULE(T)
       87             #endif
       88             
       89             #ifdef SPMV_2_UNROLL
       90             #define ComputeSPMV_unroll2 ComputeSPMV_manual
       91             #elif defined SPMV_4_UNROLL
       92             #define ComputeSPMV_unroll4 ComputeSPMV_manual
       93             #endif
       94             inline void ComputeSPMV_manual(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow);
       95             
       96             #ifdef TEST_SPMV_AS_TDG
       97             //inline void test_tdg(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow);
       98             #define test_tdg ComputeSPMV_manual
       99             #endif
      100             #ifdef TEST_SPMV_AS_TDG_REF
      101             //inline void test_ref(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow);
      102             #define test_ref ComputeSPMV_manual
      103             #endif
      104             
      105             
      106             int ComputeSPMV( const SparseMatrix & A, Vector & x, Vector & y) {
      107             
      108             	assert(x.localLength >= A.localNumberOfColumns);
      109             	assert(y.localLength >= A.localNumberOfRows);
      110             
      111             #ifndef HPCG_NO_MPI
      112             	ExchangeHalo(A,x);
      113             #endif
      114             	const double * const xv = x.values;
      115             	double * const yv = y.values;
      116             	const local_int_t nrow = A.localNumberOfRows;
      117             
      118             #if defined(HPCG_MAN_OPT_SPMV_UNROLL)
      119             	ComputeSPMV_manual(A, xv, yv, nrow);
      120             
      121             #elif defined(HPCG_USE_NEON) && !defined(HPCG_USE_ARMPL_SPMV)
      122             #ifndef HPCG_NO_OPENMP
      123             #pragma omp parallel for SCHEDULE(runtime)
      124             #endif
      125             	for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
      126             		float64x2_t sum0 = vdupq_n_f64(0.0);
      127             		float64x2_t sum1 = vdupq_n_f64(0.0);
      128             		if ( A.nonzerosInRow[i] == A.nonzerosInRow[i+1] ) {
      129             			local_int_t j = 0;
      130             			for ( j = 0; j < A.nonzerosInRow[i]-1; j+=2 ) {
      131             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      132             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      133             
      134             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      135             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      136             
      137             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      138             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      139             
      140             			}
      141             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      142             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      143             
      144             			if ( j < A.nonzerosInRow[i] ) {
      145             				s0 += A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
      146             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      147             			}
      148             			yv[i  ] = s0;
      149             			yv[i+1] = s1;
      150             		} else if ( A.nonzerosInRow[i] > A.nonzerosInRow[i+1] ) {
      151             			local_int_t j = 0;
      152             			for ( j = 0; j < A.nonzerosInRow[i+1]-1; j+=2 ) {
      153             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      154             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      155             
      156             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      157             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      158             
      159             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      160             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      161             
      162             			}
      163             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      164             			if ( j < A.nonzerosInRow[i+1] ) {
      165             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      166             			}
      167             
      168             			for ( ; j < A.nonzerosInRow[i]-1; j+=2 ) {
      169             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      170             
      171             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      172             
      173             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      174             			}
      175             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      176             			if ( j < A.nonzerosInRow[i] ) {
      177             				s0 += A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
      178             			}
      179             			yv[i  ] = s0;
      180             			yv[i+1] = s1;
      181             		} else { // A.nonzerosInRow[i] < A.nonzerosInRow[i+1]
      182             			local_int_t j = 0;
      183             			for ( j = 0; j < A.nonzerosInRow[i]-1; j+=2 ) {
      184             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      185             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      186             
      187             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      188             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      189             
      190             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      191             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      192             
      193             			}
      194             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      195             			if ( j < A.nonzerosInRow[i] ) {
      196             				s0 += A.matrixValues[i][j] * xv[A.mtxIndL[i][j]];
      197             			}
      198             
      199             			for ( ; j < A.nonzerosInRow[i+1]-1; j+=2 ) {
      200             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      201             
      202             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      203             
      204             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      205             			}
      206             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      207             			if ( j < A.nonzerosInRow[i+1] ) {
      208             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      209             			}
      210             			yv[i  ] = s0;
      211             			yv[i+1] = s1;
      212             		}
      213             	}
      214             #elif defined(HPCG_USE_SVE) && !defined(HPCG_USE_ARMPL_SPMV)
      215             
      216             	if ( nrow % 4 == 0 ) {
      217             #ifndef HPCG_NO_OPENMP
      218             #pragma omp parallel for SCHEDULE(runtime)
      219             #endif
      220             		for ( local_int_t i = 0; i < nrow-3; i+=4 ) {
      221             			local_int_t maxnnz01 = A.nonzerosInRow[i  ] > A.nonzerosInRow[i+1] ? A.nonzerosInRow[i  ] : A.nonzerosInRow[i+1];
      222             			local_int_t maxnnz23 = A.nonzerosInRow[i+2] > A.nonzerosInRow[i+3] ? A.nonzerosInRow[i+2] : A.nonzerosInRow[i+3];
      223             			local_int_t maxnnz = maxnnz01 > maxnnz23 ? maxnnz01 : maxnnz23;
      224             			svfloat64_t svsum0 = svdup_f64(0.0);
      225             			svfloat64_t svsum1 = svdup_f64(0.0);
      226             			svfloat64_t svsum2 = svdup_f64(0.0);
      227             			svfloat64_t svsum3 = svdup_f64(0.0);
      228             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      229             				svbool_t pg0 = svwhilelt_b64(j, A.nonzerosInRow[i  ]);
      230             				svbool_t pg1 = svwhilelt_b64(j, A.nonzerosInRow[i+1]);
      231             				svbool_t pg2 = svwhilelt_b64(j, A.nonzerosInRow[i+2]);
      232             				svbool_t pg3 = svwhilelt_b64(j, A.nonzerosInRow[i+3]);
      233             				svfloat64_t values0 = svld1_f64(pg0, &A.matrixValues[i  ][j]);
      234             				svfloat64_t values1 = svld1_f64(pg1, &A.matrixValues[i+1][j]);
      235             				svfloat64_t values2 = svld1_f64(pg2, &A.matrixValues[i+2][j]);
      236             				svfloat64_t values3 = svld1_f64(pg3, &A.matrixValues[i+3][j]);
      237             				svuint64_t indices0 = svld1sw_u64(pg0, &A.mtxIndL[i  ][j]);
      238             				svuint64_t indices1 = svld1sw_u64(pg1, &A.mtxIndL[i+1][j]);
      239             				svuint64_t indices2 = svld1sw_u64(pg2, &A.mtxIndL[i+2][j]);
      240             				svuint64_t indices3 = svld1sw_u64(pg3, &A.mtxIndL[i+3][j]);
      241             
      242             				svfloat64_t svxv0 = svld1_gather_u64index_f64(pg0, xv, indices0);
      243             				svfloat64_t svxv1 = svld1_gather_u64index_f64(pg1, xv, indices1);
      244             				svfloat64_t svxv2 = svld1_gather_u64index_f64(pg2, xv, indices2);
      245             				svfloat64_t svxv3 = svld1_gather_u64index_f64(pg3, xv, indices3);
      246             				svsum0 = svmla_f64_m(pg0, svsum0, values0, svxv0);
      247             				svsum1 = svmla_f64_m(pg1, svsum1, values1, svxv1);
      248             				svsum2 = svmla_f64_m(pg2, svsum2, values2, svxv2);
      249             				svsum3 = svmla_f64_m(pg3, svsum3, values3, svxv3);
      250             			}
      251             			yv[i  ] = svaddv(svptrue_b64(), svsum0);
      252             			yv[i+1] = svaddv(svptrue_b64(), svsum1);
      253             			yv[i+2] = svaddv(svptrue_b64(), svsum2);
      254             			yv[i+3] = svaddv(svptrue_b64(), svsum3);
      255             		}
      256             	} else if ( nrow % 2 == 0 ) {
      257             #ifndef HPCG_NO_OPENMP
      258             #pragma omp parallel for SCHEDULE(runtime)
      259             #endif
      260             		for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
      261             			local_int_t maxnnz = A.nonzerosInRow[i] > A.nonzerosInRow[i+1] ? A.nonzerosInRow[i] : A.nonzerosInRow[i+1];
      262             			svfloat64_t svsum0 = svdup_f64(0.0);
      263             			svfloat64_t svsum1 = svdup_f64(0.0);
      264             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      265             				svbool_t pg0 = svwhilelt_b64(j, A.nonzerosInRow[i]);
      266             				svbool_t pg1 = svwhilelt_b64(j, A.nonzerosInRow[i+1]);
      267             				svfloat64_t values0 = svld1_f64(pg0, &A.matrixValues[i][j]);
      268             				svfloat64_t values1 = svld1_f64(pg1, &A.matrixValues[i+1][j]);
      269             				svuint64_t indices0 = svld1sw_u64(pg0, &A.mtxIndL[i][j]);
      270             				svuint64_t indices1 = svld1sw_u64(pg1, &A.mtxIndL[i+1][j]);
      271             
      272             				svfloat64_t svxv0 = svld1_gather_u64index_f64(pg0, xv, indices0);
      273             				svfloat64_t svxv1 = svld1_gather_u64index_f64(pg1, xv, indices1);
      274             				svsum0 = svmla_f64_m(pg0, svsum0, values0, svxv0);
      275             				svsum1 = svmla_f64_m(pg1, svsum1, values1, svxv1);
      276             			}
      277             			yv[i] = svaddv(svptrue_b64(), svsum0);
      278             			yv[i+1] = svaddv(svptrue_b64(), svsum1);
      279             		}
      280             	} else {
      281             #ifndef HPCG_NO_OPENMP
      282             #pragma omp parallel for SCHEDULE(runtime)
      283             #endif
      284             		for ( local_int_t i = 0; i < nrow; i++ ) {
      285             			local_int_t maxnnz = A.nonzerosInRow[i];
      286             			svfloat64_t svsum = svdup_f64(0.0);
      287             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      288             				svbool_t pg = svwhilelt_b64(j, maxnnz);
      289             				svfloat64_t values = svld1_f64(pg, &A.matrixValues[i][j]);
      290             				svuint64_t indices = svld1sw_u64(pg, &A.mtxIndL[i][j]);
      291             
      292             				svfloat64_t svxv = svld1_gather_u64index_f64(pg, xv, indices);
      293             				svsum = svmla_f64_m(pg, svsum, values, svxv);
      294             			}
      295             			yv[i] = svaddv(svptrue_b64(), svsum);
      296             		}
      297             	}
      298             #elif defined(HPCG_USE_ARMPL_SPMV)
      299             	double alpha = 1.0;
      300             	double beta = 0.0;
      301             
      302             	armpl_spmv_exec_d(ARMPL_SPARSE_OPERATION_NOTRANS, alpha, A.armpl_mat, xv, beta, yv);
      303             
      304             #else
      305             //#pragma statement scache_isolate_way L2=10
      306             //#pragma statement scache_isolate_assign xv
      307             
      308             #ifndef HPCG_NO_OPENMP
      309             #pragma omp parallel for SCHEDULE(runtime)
      310             #endif
      311             //#pragma loop nounroll
      312             	for ( local_int_t i = 0; i < nrow; i++ ) {
      313             		double sum = 0.0;
      314             		//#pragma fj loop zfill
      315             	//#pragma loop nounroll
      316             		for ( local_int_t j = 0; j < A.nonzerosInRow[i]; j++ ) {
      317             			local_int_t curCol = A.mtxIndL[i][j];
      318             			sum += A.matrixValues[i][j] * xv[curCol];
      319             		}
      320             		yv[i] = sum;
      321             	}
      322             	//#pragma statement end_scache_isolate_assign
      323             	//#pragma statement end_scache_isolate_way
      324             #endif
      325             
      326             	return 0;
      327             }
      328             
      329             #ifdef HPCG_MAN_OPT_SPMV_UNROLL
      330             inline void ComputeSPMV_unroll2(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow) {
      331             //#pragma fj loop zfill
      332             //#pragma statement scache_isolate_way L2=10
      333             //#pragma statement scache_isolate_assign xv
      334             
      335             #ifndef HPCG_NO_OPENMP
      336             #pragma omp parallel for SCHEDULE(runtime)
      337             #endif
      338             //#pragma loop nounroll_and_jam
      339             	for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
      340             		double sum0 = 0.0;
      341             		double sum1 = 0.0;
      342             		if (A.nonzerosInRow[i] == A.nonzerosInRow[i+1]) {
      343             			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      344             				local_int_t curCol0 = A.mtxIndL[i][j];
      345             				local_int_t curCol1 = A.mtxIndL[i+1][j];
      346             
      347             				sum0 += A.matrixValues[i][j] * xv[curCol0];
      348             				sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      349             			}
      350             		}
      351             		else {
      352             			local_int_t min0 = A.nonzerosInRow[i] < A.nonzerosInRow[i+1] ? A.nonzerosInRow[i] : A.nonzerosInRow[i+1];
      353             			for ( local_int_t j = 0; j < min0; ++j ) {
      354             				local_int_t curCol0 = A.mtxIndL[i][j];
      355             				local_int_t curCol1 = A.mtxIndL[i+1][j];
      356             
      357             				sum0 += A.matrixValues[i][j] * xv[curCol0];
      358             				sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      359             			}
      360             
      361             			for ( local_int_t j = min0; j < A.nonzerosInRow[i]; ++j ) {
      362             				local_int_t curCol = A.mtxIndL[i][j];
      363             				sum0 += A.matrixValues[i][j] * xv[curCol];
      364             			}
      365             			for ( local_int_t j = min0; j < A.nonzerosInRow[i+1]; ++j ) {
      366             				local_int_t curCol = A.mtxIndL[i+1][j];
      367             				sum1 += A.matrixValues[i+1][j] * xv[curCol];
      368             			}
      369             		}
      370             		yv[i] = sum0;
      371             		yv[i+1] = sum1;
      372             	}
      373             	//#pragma statement end_scache_isolate_assign
      374             	//#pragma statement end_scache_isolate_way
      375             }
      376             
      377             inline void ComputeSPMV_unroll4(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow) {
      378             
      379             #ifndef HPCG_NO_OPENMP
      380             #pragma omp parallel for SCHEDULE(runtime)
      381             #endif
      382             	//#pragma statement scache_isolate_way L2=10
      383             	//#pragma statement scache_isolate_assign xv
      384             	#pragma loop nounroll_and_jam
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      385   p         	for ( local_int_t i = 0; i < nrow-3; i+=4) {
      386   p         		double sum0 = 0.0, sum1 = 0.0, sum2 = 0.0, sum3 = 0.0;
      387             
      388   p         		if ((A.nonzerosInRow[i] == A.nonzerosInRow[i+1]) && (A.nonzerosInRow[i] == A.nonzerosInRow[i+2]) && (A.nonzerosInRow[i] == A.nonzerosInRow[i+3]) ){
      389             			#pragma fj loop zfill
      390             			#pragma loop nounroll
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.80, ITR: 48, MVE: 3, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      391   p      v  			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      392   p      v  				local_int_t curCol0 = A.mtxIndL[i][j];
      393   p      v  				local_int_t curCol1 = A.mtxIndL[i+1][j];
      394   p      v  				local_int_t curCol2 = A.mtxIndL[i+2][j];
      395   p      v  				local_int_t curCol3 = A.mtxIndL[i+3][j];
      396             
      397   p      v  				sum0 += A.matrixValues[i][j] * xv[curCol0];
      398   p      v  				sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      399   p      v  				sum2 += A.matrixValues[i+2][j] * xv[curCol2];
      400   p      v  				sum3 += A.matrixValues[i+3][j] * xv[curCol3];
      401   p      v  			}
      402             
      403   p         			yv[i] = sum0;
      404   p         			yv[i+1] = sum1;
      405   p         			yv[i+2] = sum2;
      406   p         			yv[i+3] = sum3;
      407   p         		}
      408   p         		else {
      409             			#pragma fj loop zfill			
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      410   p         			for ( local_int_t ix = 0; ix < 4; ++ix) {
      411   p         				double sum = 0.0;
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      412   p     8v  				for ( local_int_t j = 0; j < A.nonzerosInRow[i+ix]; ++j ) {
      413   p     8v  					local_int_t curCol = A.mtxIndL[i+ix][j];
      414   p     8v  					sum += A.matrixValues[i+ix][j] * xv[curCol];
      415   p     8v  				}
      416   p         				yv[i+ix] = sum;
      417   p         			}
      418   p         		}
      419   p         	}
      420             	//#pragma statement end_scache_isolate_assign
      421                 //#pragma statement end_scache_isolate_way	
      422             }
      423             #endif //HPCG_MAN_OPT_SPMV_UNROLL
      424             
      425             inline void test_ref(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow) {
      426             
      427             #ifndef HPCG_NO_OPENMP
      428             #pragma omp parallel for SCHEDULE(runtime)
      429             #endif
      430             	//#pragma loop nounroll
      431             	for ( local_int_t i = 0; i < nrow; i++ ) {
      432             		double sum = 0.0;
      433             		//#pragma fj loop zfill
      434             		//#pragma loop nounroll
      435             		for ( local_int_t j = 0; j < A.nonzerosInRow[i]; j++ ) {
      436             			local_int_t curCol = A.mtxIndL[i][j];
      437             			sum += A.matrixValues[i][j] * xv[curCol];
      438             		}
      439             		yv[i] = sum;
      440             	}
      441             }
      442             
      443             inline void test_tdg(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow) {
      444             
      445             #ifndef HPCG_NO_OPENMP
      446             #pragma omp parallel
      447             {
      448             #endif
      449             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      450             #ifndef HPCG_NO_OPENMP
      451             #pragma omp for SCHEDULE(runtime)
      452             #endif
      453             		//#pragma loop nounroll
      454             		for ( local_int_t ix = 0; ix < A.tdg[l].size(); ix++ ) {
      455             			local_int_t i = A.tdg[l][ix];
      456             			double sum = 0.0;
      457             
      458             			//#pragma fj loop zfill
      459             			//#pragma loop nounroll
      460             			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; j++ ) {
      461             				local_int_t curCol = A.mtxIndL[i][j];
      462             				sum += A.matrixValues[i][j] * xv[curCol];
      463             			}
      464             			yv[i] = sum;
      465             		}
      466             	}
      467             #ifndef HPCG_NO_OPENMP
      468             }
      469             #endif	
      470             }
Total prefetch num: 0
Optimization messages
  jwd6004s-i  "../src/ComputeSPMV.cpp", line 391: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSPMV.cpp", line 391: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSPMV.cpp", line 391: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 48.
  jwd6004s-i  "../src/ComputeSPMV.cpp", line 412: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSPMV.cpp", line 412: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSPMV.cpp", line 412: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSPMV.cpp", line 414: Method of calculating sum or product is changed.
Statistics information
  Option information
    Command line options : -c -DHPCG_CONTIGUOUS_ARRAYS -DHPCG_NO_MPI -DENABLE_MG_COUNTERS -DHPCG_USE_SVE -DHPCG_MAN_OPT_DDOT -DDDOT_4_UNROLL -DWAXPBY_AUTO_OPT -HPCG_MAN_OPT_SCHEDULE_ON -DHPCG_MAN_OPT_SPMV_UNROLL -DSPMV_4_UNROLL -Khpctag -Kzfill -DREF_UNROLLING_4 -I./src -I./src/OOKAMI_OMP_FJ -Kfast -KSVE -Kopenmp -ffast-math -funroll-loops -std=c++11 -ffp-contract=fast -march=armv8.2-a+sve -Kocl -Koptmsg=2 -Nlst=t -I../src -o src/ComputeSPMV.o
    Effective options    : -g0 -mt -Qy -std=gnu++11 -x- -x=quick -O3 -Knoalias_const
                           -Kalign_loops -Knoarray_declaration_opt -Kassume=noshortloop
                           -Kassume=nomemory_bandwidth -Kassume=notime_saving_compilation
                           -Kcmodel=small -Keval -Keval_noconcurrent
                           -Knoextract_stride_store -Kfast_matmul -Knofenv_access
                           -Kfp_contract -Kfp_relaxed -Kfsimple -Kfz -Khpctag
                           -Kilfunc=procedure -Klargepage -Klib -Kloop_blocking
                           -Kloop_fission -Kloop_nofission_stripmining
                           -Kloop_fission_threshold=50 -Kloop_fusion -Kloop_interchange
                           -Kloop_part_simd -Kloop_perfect_nest -Kloop_noversioning
                           -Klooptype=f -Knomemalias -Kmfunc=1 -Kocl -Komitfp -Kopenmp
                           -Kopenmp_noassume_norecurrence
                           -Kopenmp_nocollapse_except_innermost
                           -Kopenmp_loop_variable=private -Kopenmp_noordered_reduction
                           -Knoopenmp_simd -Knooptlib_string -Koptmsg=2
                           -Knopc_relative_literal_loads -Knoparallel
                           -Kparallel_nofp_precision -Knopreex -Kprefetch_cache_level=all
                           -Kprefetch_noconditional -Kprefetch_noindirect -Kprefetch_noinfer
                           -Kprefetch_sequential=auto -Kprefetch_nostride -Kprefetch_strong
                           -Kprefetch_strong_L2 -Knopreload -Krdconv=1
                           -Kremove_inlinefunction -Knorestp -Ksch_post_ra -Ksch_pre_ra
                           -Ksibling_calls -Ksimd=auto -Ksimd_packed_promotion
                           -Ksimd_reduction_product -Ksimd_reg_size=512
                           -Ksimd_nouncounted_loop -Ksimd_use_multiple_structures
                           -Knostrict_aliasing -Knostriping -KA64FX -KARMV8_2_A -KSVE -Kswp
                           -Kswp_freg_rate=100 -Kswp_ireg_rate=100 -Kswp_preg_rate=100
                           -Kswp_policy=auto -Kunroll -Knounroll_and_jam -Kzfill
                           -Ncancel_overtime_compilation -Nnocoverage -Nexceptions -Nnofjcex
                           -Nfjprof -Nnohook_func -Nnohook_time -Nlibomp -Nline -Nlst=p
                           -Nlst=t -Nquickdbg=noheapchk -Nquickdbg=nosubchk -NRnotrap
                           -Nnoreordered_variable_stack -Nrt_notune -Nsetvalue=noheap
                           -Nsetvalue=nostack -Nsetvalue=noscalar -Nsetvalue=noarray
                           -Nsetvalue=nostruct -Nsrc -Nsta
