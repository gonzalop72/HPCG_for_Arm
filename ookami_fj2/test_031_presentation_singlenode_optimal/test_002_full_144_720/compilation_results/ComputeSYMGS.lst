Fujitsu C/C++ Version 4.7.0   Mon Jul 10 14:33:36 2023
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj2
  Source file       : ../src/ComputeSYMGS.cpp
(line-no.)(optimize)
        1             /*
        2              *
        3              *  SPDX-License-Identifier: Apache-2.0
        4              *
        5              *  Copyright (C) 2019, Arm Limited and contributors
        6              *
        7              *  Licensed under the Apache License, Version 2.0 (the "License");
        8              *  you may not use this file except in compliance with the License.
        9              *  You may obtain a copy of the License at
       10              *
       11              *      http://www.apache.org/licenses/LICENSE-2.0
       12              *
       13              *  Unless required by applicable law or agreed to in writing, software
       14              *  distributed under the License is distributed on an "AS IS" BASIS,
       15              *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       16              *  See the License for the specific language governing permissions and
       17              *  limitations under the License.
       18              *
       19              */
       20             
       21             //@HEADER
       22             // ***************************************************
       23             //
       24             // HPCG: High Performance Conjugate Gradient Benchmark
       25             //
       26             // Contact:
       27             // Michael A. Heroux ( maherou@sandia.gov)
       28             // Jack Dongarra     (dongarra@eecs.utk.edu)
       29             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       30             //
       31             // ***************************************************
       32             //@HEADER
       33             
       34             /*!
       35              @file ComputeSYMGS.cpp
       36             
       37              HPCG routine
       38              */
       39             
       40             #include "ComputeSYMGS.hpp"
       41             #include "ComputeSYMGS_ref.hpp"
       42             #ifndef HPCG_NO_MPI
       43             #include "ExchangeHalo.hpp"
       44             #endif
       45             
       46             #include "likwid_instrumentation.hpp"
       47             
       48             #ifdef HPCG_MAN_OPT_SCHEDULE_ON
       49             	#define SCHEDULE(T)	schedule(T)
       50             #else
       51             	#define SCHEDULE(T)
       52             #endif
       53             
       54             #define MANUAL_TASK_DISTRIBUTION
       55             #ifdef MANUAL_TASK_DISTRIBUTION
       56             	#include "omp.h"
       57             
       58             	#define MAXI(x,y) (x>y) ? x : y
       59             	#define MINI(x,y) (x<y) ? x : y
       60             #endif
       61             /**************************************************************************************************/
       62             /**************************************************************************************************/
       63             /**************************************************************************************************/
       64             /* SVE IMPLEMENTATIONS                                                                            */
       65             /**************************************************************************************************/
       66             /**************************************************************************************************/
       67             /**************************************************************************************************/
       68             
       69             #include "arm_sve.h"
       70             #ifdef HPCG_USE_SVE
       71             #include "arm_sve.h"
       72             
       73             inline void SYMGS_VERSION_1(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-2
       74             inline void SYMGS_VERSION_2(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-2 V2
       75             inline void SYMGS_VERSION_3(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-4 - OPTIMUM
       76             inline void SYMGS_VERSION_4(const SparseMatrix& A, double * const& xv, const double * const& rv);	//UNROLL-6
       77             
       78             /*
       79              * TDG VERSION
       80              */
       81             int ComputeSYMGS_TDG_SVE(const SparseMatrix & A, const Vector & r, Vector & x, TraceData &trace) {
       82             	assert(x.localLength == A.localNumberOfColumns);
       83             
       84             #ifndef HPCG_NO_MPI
       85             	ExchangeHalo(A, x);
       86             #endif
       87             
       88             	const double * const rv = r.values;
       89             	double * const xv = x.values;
       90             	double **matrixDiagonal = A.matrixDiagonal;
       91             
       92             LIKWID_START(trace.enabled, "symgs_tdg");
       93             
       94             #ifndef TEST_XX
       95             SYMGS_VERSION_3(A, xv, rv);
       96             #else
       97             
       98             //#pragma statement scache_isolate_way L2=10
       99             //#pragma statement scache_isolate_assign xv
      100             	/*
      101             	 * FORWARD SWEEP
      102             	 */
      103             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      104             
      105             		local_int_t totalSize = A.tdg[l].size();
      106             		local_int_t size1 = 2*(totalSize/2);
      107             		//#pragma loop nounroll
      108             		//#pragma loop nounroll_and_jam
      109             		//if((A.tdg[l].size()%2) == 0) {
      110             #ifndef HPCG_NO_OPENMP
      111             #pragma omp parallel
      112             {
      113             #pragma omp for nowait SCHEDULE(runtime)
      114             #endif
      115             		for ( local_int_t i = 0; i < size1; i+=2 ) {
      116             			local_int_t row_1 = A.tdg[l][i];
      117             			local_int_t row_2 = A.tdg[l][i+1];
      118             			const double * const currentValues_1 = A.matrixValues[row_1];
      119             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
      120             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
      121             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
      122             			svfloat64_t contribs_1 = svdup_f64(0.0);
      123             
      124             			const double * const currentValues_2 = A.matrixValues[row_2];
      125             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
      126             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
      127             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
      128             			svfloat64_t contribs_2 = svdup_f64(0.0);
      129             			
      130             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
      131             
      132             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
      133             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
      134             				
      135             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
      136             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
      137             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
      138             
      139             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
      140             
      141             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
      142             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
      143             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
      144             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
      145             
      146             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
      147             			}
      148             
      149             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
      150             			double sum_1 = rv[row_1] - totalContribution_1;
      151             
      152             			sum_1 += xv[row_1] * currentDiagonal_1;
      153             			xv[row_1] = sum_1 / currentDiagonal_1;
      154             
      155             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
      156             			double sum_2 = rv[row_2] - totalContribution_2;
      157             
      158             			sum_2 += xv[row_2] * currentDiagonal_2;
      159             			xv[row_2] = sum_2 / currentDiagonal_2;
      160             		}
      161             		//}
      162             		//else
      163             		//{
      164             #ifndef HPCG_NO_OPENMP
      165             //#pragma omp parallel for SCHEDULE(runtime)
      166             #pragma omp single 
      167             {
      168             #endif
      169             		if (size1 < totalSize) {
      170             			local_int_t i = size1;
      171             		//for ( local_int_t i = size1; i < totalSize; i++ ) {
      172             			local_int_t row = A.tdg[l][i];
      173             			const double * const currentValues = A.matrixValues[row];
      174             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      175             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      176             			const double currentDiagonal = matrixDiagonal[row][0];
      177             			svfloat64_t contribs = svdup_f64(0.0);
      178             
      179             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      180             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      181             				
      182             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      183             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      184             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      185             
      186             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      187             			}
      188             
      189             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      190             			double sum = rv[row] - totalContribution;
      191             
      192             			sum += xv[row] * currentDiagonal;
      193             			xv[row] = sum / currentDiagonal;
      194             		//}
      195             		}
      196             #ifndef HPCG_NO_OPENMP
      197             }
      198             }
      199             #endif
      200             	}
      201             
      202             	/*
      203             	 * BACKWARD SWEEP
      204             	 */
      205             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      206             #ifndef HPCG_NO_OPENMP
      207             #pragma omp parallel for SCHEDULE(runtime)
      208             #endif
      209             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      210             			local_int_t row = A.tdg[l][i];
      211             			const double * const currentValues = A.matrixValues[row];
      212             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      213             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      214             			const double currentDiagonal = matrixDiagonal[row][0];
      215             			svfloat64_t contribs = svdup_f64(0.0);
      216             
      217             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      218             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      219             				
      220             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      221             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      222             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      223             
      224             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      225             			}
      226             
      227             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      228             			double sum = rv[row] - totalContribution;
      229             
      230             			sum += xv[row] * currentDiagonal;
      231             			xv[row] = sum / currentDiagonal;
      232             		}
      233             
      234             /*#ifndef HPCG_NO_OPENMP
      235             #pragma omp parallel for SCHEDULE(runtime)
      236             #endif
      237             		for ( local_int_t i = size1-1; i >= 0; i-= 2 ) {
      238             			local_int_t row_1 = A.tdg[l][i];
      239             			local_int_t row_2 = A.tdg[l][i-1];
      240             			const double * const currentValues_1 = A.matrixValues[row_1];
      241             			const double * const currentValues_2 = A.matrixValues[row_2];
      242             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
      243             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
      244             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
      245             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
      246             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
      247             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
      248             			svfloat64_t contribs_1 = svdup_f64(0.0);
      249             			svfloat64_t contribs_2 = svdup_f64(0.0);
      250             
      251             			//#pragma loop nounroll
      252             			//#pragma loop nounroll_and_jam
      253             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      254             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      255             				
      256             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
      257             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      258             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      259             
      260             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      261             			}
      262             
      263             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      264             			double sum = rv[row] - totalContribution;
      265             
      266             			sum += xv[row] * currentDiagonal;
      267             			xv[row] = sum / currentDiagonal;
      268             		}*/
      269             	}
      270             //#pragma statement end_scache_isolate_assign
      271             //#pragma statement end_scache_isolate_way
      272             
      273             #endif //TEST_XX
      274             
      275             LIKWID_STOP(trace.enabled, "symgs_tdg");
      276             
      277             	return 0;
      278             }
      279             /*
      280              * END OF TDG VERSION
      281              */
      282             
      283             /*
      284              * TDG FUSED SYMGS-SPMV VERSION
      285              */
      286             int ComputeFusedSYMGS_SPMV_SVE(const SparseMatrix & A, const Vector & r, Vector & x, Vector & y, TraceData& trace) {
      287             	assert(x.localLength == A.localNumberOfColumns);
      288             
      289             #ifndef HPCG_NO_MPI
      290             	ExchangeHalo(A, x);
      291             #endif
      292             
      293             	const double * const rv = r.values;
      294             	double * const xv = x.values;
      295             	double **matrixDiagonal = A.matrixDiagonal;
      296             	double * const yv = y.values;
      297             
      298             	/*
      299             	 * FORWARD SWEEP
      300             	 */
      301    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      302             #ifndef HPCG_NO_OPENMP
      303             #pragma omp parallel for SCHEDULE(runtime)
      304             #endif
      305   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      306   p         			local_int_t row = A.tdg[l][i];
      307   p         			const double * const currentValues = A.matrixValues[row];
      308   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      309   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      310   p         			const double currentDiagonal = matrixDiagonal[row][0];
      311   p         			svfloat64_t contribs = svdup_f64(0.0);
      312             
      313   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      314   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      315             				
      316   p      s  				svfloat64_t mtxValues = svld1(pg, &currentValues[j]);
      317   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      318   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      319             
      320   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      321   p      s  			}
      322             
      323   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      324   p         			double sum = rv[row] - totalContribution;
      325             
      326   p         			sum += xv[row] * currentDiagonal;
      327   p         			xv[row] = sum / currentDiagonal;
      328   pi        		}
      329    i        	}
      330             
      331             	/*
      332             	 * BACKWARD SWEEP
      333             	 */
      334    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      335             #ifndef HPCG_NO_OPENMP
      336             #pragma omp parallel for SCHEDULE(runtime)
      337             #endif
      338   pi        		for ( local_int_t i = A.tdg[l].size(); i >= 0; i-- ) {
      339   p         			local_int_t row = A.tdg[l][i];
      340   p         			const double * const currentValues = A.matrixValues[row];
      341   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
      342   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      343   p         			const double currentDiagonal = matrixDiagonal[row][0];
      344   p         			svfloat64_t contribs = svdup_f64(0.0);
      345             
      346   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
      347   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      348             				
      349   p      s  				svfloat64_t mtxValues = svld1(pg, &currentValues[j]);
      350   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
      351   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
      352             
      353   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
      354   p      s  			}
      355             
      356   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
      357   p         			totalContribution -= xv[row] * currentDiagonal; // remove diagonal contribution
      358   p         			double sum = rv[row] - totalContribution; // substract contributions from RHS
      359   p         			xv[row] = sum / currentDiagonal; // update row
      360             
      361             			// SPMV part
      362   p         			totalContribution += xv[row] * currentDiagonal; // add updated diagonal contribution
      363   p         			yv[row] = totalContribution; // update SPMV output vector
      364             			
      365   p         		}
      366             	}
      367             
      368             	return 0;
      369             }
      370             /*
      371              * END OF TDG FUSED SYMGS-SPMV VERSION
      372              */
      373             
      374             /*
      375              * BLOCK COLORED VERSION
      376              */
      377             int ComputeSYMGS_BLOCK_SVE(const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
      378             	assert(x.localLength >= A.localNumberOfColumns);
      379             
      380             #ifndef HPCG_NO_MPI
      381             	ExchangeHalo(A, x);
      382             #endif
      383             
      384             	double **matrixDiagonal = A.matrixDiagonal;
      385             	const double * const rv = r.values;
      386             	double * const xv = x.values;
      387             	local_int_t firstBlock = 0;
      388    i        	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
      389             
      390             LIKWID_START(trace.enabled, "symgs_bc");		
      391             
      392             	/*
      393             	 * FORWARD SWEEP
      394             	 */
      395             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) { // for each color
      396             		if ( color > 0 ) {
      397    i        			firstBlock += A.numberOfBlocksInColor[color-1];
      398    i        			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
      399             		}
      400             #ifndef HPCG_NO_OPENMP
      401             #pragma omp parallel for SCHEDULE(runtime)
      402             #endif
      403   p         		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) { // for each superblock with the same color
      404   p         			local_int_t firstRow = block * A.blockSize;
      405   p         			local_int_t firstChunk = firstRow / A.chunkSize;
      406   p         			local_int_t lastChunk = (firstRow + A.blockSize * A.chunkSize) / A.chunkSize;
      407             
      408   p         			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) { // for each chunk of this super block
      409   p         				local_int_t first = A.chunkSize * chunk;
      410   p         				local_int_t last = first + A.chunkSize;
      411   p         				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      412   p         				local_int_t i = first;
      413   p         				if ( A.chunkSize == 4 ) {
      414   p         					const double * const currentValues0 = A.matrixValues[i  ];
      415   p         					const double * const currentValues1 = A.matrixValues[i+1];
      416   p         					const double * const currentValues2 = A.matrixValues[i+2];
      417   p         					const double * const currentValues3 = A.matrixValues[i+3];
      418             
      419   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      420   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      421   p         					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      422   p         					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      423             
      424   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      425   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      426   p         					const double currentDiagonal2 = matrixDiagonal[i+2][0];
      427   p         					const double currentDiagonal3 = matrixDiagonal[i+3][0];
      428             
      429   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      430   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      431   p         					svfloat64_t contribs2 = svdup_f64(0.0);
      432   p         					svfloat64_t contribs3 = svdup_f64(0.0);
      433             
      434   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      435   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      436             
      437   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      438   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      439   p      s  						svfloat64_t values2 = svld1_f64(pg, &currentValues2[j]);
      440   p      s  						svfloat64_t values3 = svld1_f64(pg, &currentValues3[j]);
      441             
      442   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      443   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      444   p      s  						svuint64_t indices2 = svld1sw_u64(pg, &currentColIndices2[j]);
      445   p      s  						svuint64_t indices3 = svld1sw_u64(pg, &currentColIndices3[j]);
      446             
      447   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      448   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      449   p      s  						svfloat64_t xvv2 = svld1_gather_u64index_f64(pg, xv, indices2);
      450   p      s  						svfloat64_t xvv3 = svld1_gather_u64index_f64(pg, xv, indices3);
      451             
      452   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      453   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1);
      454   p      s  						contribs2 = svmla_f64_m(pg, contribs2, xvv2, values2);
      455   p      s  						contribs3 = svmla_f64_m(pg, contribs3, xvv3, values3);
      456   p      s  					}
      457             
      458   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      459   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      460   p         					double totalContribution2 = svaddv_f64(svptrue_b64(), contribs2);
      461   p         					double totalContribution3 = svaddv_f64(svptrue_b64(), contribs3);
      462             
      463   p         					double sum0 = rv[i  ] - totalContribution0;
      464   p         					double sum1 = rv[i+1] - totalContribution1;
      465   p         					double sum2 = rv[i+2] - totalContribution2;
      466   p         					double sum3 = rv[i+3] - totalContribution3;
      467             
      468   p         					sum0 += xv[i  ] * currentDiagonal0;
      469   p         					sum1 += xv[i+1] * currentDiagonal1;
      470   p         					sum2 += xv[i+2] * currentDiagonal2;
      471   p         					sum3 += xv[i+3] * currentDiagonal3;
      472             
      473   p         					xv[i  ] = sum0 / currentDiagonal0;
      474   p         					xv[i+1] = sum1 / currentDiagonal1;
      475   p         					xv[i+2] = sum2 / currentDiagonal2;
      476   p         					xv[i+3] = sum3 / currentDiagonal3;
      477   p         				} else if ( A.chunkSize == 2 ) {
      478   p         					const double * const currentValues0 = A.matrixValues[i  ];
      479   p         					const double * const currentValues1 = A.matrixValues[i+1];
      480             
      481   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      482   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      483             
      484   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      485   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      486             
      487   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      488   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      489             
      490   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      491   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      492             
      493   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      494   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      495             
      496   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      497   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      498             
      499   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      500   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      501             
      502   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      503   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1);
      504   p      s  					}
      505             
      506   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      507   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      508             
      509   p         					double sum0 = rv[i  ] - totalContribution0;
      510   p         					double sum1 = rv[i+1] - totalContribution1;
      511             
      512   p         					sum0 += xv[i  ] * currentDiagonal0;
      513   p         					sum1 += xv[i+1] * currentDiagonal1;
      514             
      515   p         					xv[i  ] = sum0 / currentDiagonal0;
      516   p         					xv[i+1] = sum1 / currentDiagonal1;
      517   p         				} else { //A.chunkSize == 1
      518   p         					const double * const currentValues0 = A.matrixValues[i  ];
      519             
      520   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      521             
      522   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      523             
      524   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      525             
      526   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      527   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      528             
      529   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      530             
      531   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      532             
      533   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      534             
      535   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0);
      536   p      s  					}
      537             
      538   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      539             
      540   p         					double sum0 = rv[i  ] - totalContribution0;
      541             
      542   p         					sum0 += xv[i  ] * currentDiagonal0;
      543             
      544   p         					xv[i  ] = sum0 / currentDiagonal0;
      545   p         				}
      546   p         			}
      547   p         		}
      548             	}
      549             
      550             	firstBlock = A.numberOfBlocks-1;
      551    i        	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
      552             	/*
      553             	 * BACKWARD SWEEP
      554             	 */
      555             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
      556             		if ( color < A.numberOfColors-1 ) {
      557    i        			firstBlock -= A.numberOfBlocksInColor[color+1];
      558    i        			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
      559             		}
      560             #ifndef HPCG_NO_OPENMP
      561             #pragma omp parallel for SCHEDULE(runtime)
      562             #endif
      563   p         		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) {
      564   p         			local_int_t firstRow = ((block+1) * A.blockSize) - 1;
      565   p         			local_int_t firstChunk = firstRow / A.chunkSize;
      566   p         			local_int_t lastChunk = (firstRow - A.blockSize * A.chunkSize) / A.chunkSize;
      567             
      568   p         			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
      569   p         				local_int_t first = A.chunkSize * chunk;
      570   p         				local_int_t last = first + A.chunkSize;
      571             
      572   p         				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      573   p         				local_int_t i = first;
      574   p         				if ( A.chunkSize == 4 ) {
      575   p         					const double * const currentValues3 = A.matrixValues[i+3];
      576   p         					const double * const currentValues2 = A.matrixValues[i+2];
      577   p         					const double * const currentValues1 = A.matrixValues[i+1];
      578   p         					const double * const currentValues0 = A.matrixValues[i  ];
      579             
      580   p         					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      581   p         					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      582   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      583   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      584             
      585   p         					const double currentDiagonal3 = matrixDiagonal[i+3][0];
      586   p         					const double currentDiagonal2 = matrixDiagonal[i+2][0];
      587   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      588   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      589             
      590   p         					svfloat64_t contribs3 = svdup_f64(0.0);
      591   p         					svfloat64_t contribs2 = svdup_f64(0.0);
      592   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      593   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      594             
      595   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      596   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      597             
      598   p      s  						svfloat64_t values3 = svld1_f64(pg, &currentValues3[j]);
      599   p      s  						svfloat64_t values2 = svld1_f64(pg, &currentValues2[j]);
      600   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      601   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      602             
      603   p      s  						svuint64_t indices3 = svld1sw_u64(pg, &currentColIndices3[j]);
      604   p      s  						svuint64_t indices2 = svld1sw_u64(pg, &currentColIndices2[j]);
      605   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      606   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      607             
      608   p      s  						svfloat64_t xvv3 = svld1_gather_u64index_f64(pg, xv, indices3);
      609   p      s  						svfloat64_t xvv2 = svld1_gather_u64index_f64(pg, xv, indices2);
      610   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      611   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      612             
      613   p      s  						contribs3 = svmla_f64_m(pg, contribs3, xvv3, values3 );
      614   p      s  						contribs2 = svmla_f64_m(pg, contribs2, xvv2, values2 );
      615   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1 );
      616   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      617   p      s  					}
      618             
      619   p         					double totalContribution3 = svaddv_f64(svptrue_b64(), contribs3);
      620   p         					double totalContribution2 = svaddv_f64(svptrue_b64(), contribs2);
      621   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      622   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      623             
      624   p         					double sum3 = rv[i+3] - totalContribution3;
      625   p         					double sum2 = rv[i+2] - totalContribution2;
      626   p         					double sum1 = rv[i+1] - totalContribution1;
      627   p         					double sum0 = rv[i  ] - totalContribution0;
      628             
      629   p         					sum3 += xv[i+3] * currentDiagonal3;
      630   p         					sum2 += xv[i+2] * currentDiagonal2;
      631   p         					sum1 += xv[i+1] * currentDiagonal1;
      632   p         					sum0 += xv[i  ] * currentDiagonal0;
      633             					
      634   p         					xv[i+3] = sum3 / currentDiagonal3;
      635   p         					xv[i+2] = sum2 / currentDiagonal2;
      636   p         					xv[i+1] = sum1 / currentDiagonal1;
      637   p         					xv[i  ] = sum0 / currentDiagonal0;
      638   p         				} else if ( A.chunkSize == 2 ) {
      639   p         					const double * const currentValues1 = A.matrixValues[i+1];
      640   p         					const double * const currentValues0 = A.matrixValues[i  ];
      641             
      642   p         					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      643   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      644             
      645   p         					const double currentDiagonal1 = matrixDiagonal[i+1][0];
      646   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      647             
      648   p         					svfloat64_t contribs1 = svdup_f64(0.0);
      649   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      650             
      651   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      652   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      653             
      654   p      s  						svfloat64_t values1 = svld1_f64(pg, &currentValues1[j]);
      655   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      656             
      657   p      s  						svuint64_t indices1 = svld1sw_u64(pg, &currentColIndices1[j]);
      658   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      659             
      660   p      s  						svfloat64_t xvv1 = svld1_gather_u64index_f64(pg, xv, indices1);
      661   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      662             
      663   p      s  						contribs1 = svmla_f64_m(pg, contribs1, xvv1, values1 );
      664   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      665   p      s  					}
      666             
      667   p         					double totalContribution1 = svaddv_f64(svptrue_b64(), contribs1);
      668   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      669             
      670   p         					double sum1 = rv[i+1] - totalContribution1;
      671   p         					double sum0 = rv[i  ] - totalContribution0;
      672             
      673   p         					sum1 += xv[i+1] * currentDiagonal1;
      674   p         					sum0 += xv[i  ] * currentDiagonal0;
      675             					
      676   p         					xv[i+1] = sum1 / currentDiagonal1;
      677   p         					xv[i  ] = sum0 / currentDiagonal0;
      678   p         				} else { // A.chunkSize == 1
      679   p         					const double * const currentValues0 = A.matrixValues[i  ];
      680             
      681   p         					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      682             
      683   p         					const double currentDiagonal0 = matrixDiagonal[i  ][0];
      684             
      685   p         					svfloat64_t contribs0 = svdup_f64(0.0);
      686             
      687   p      s  					for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd() ) {
      688   p      s  						svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
      689             
      690   p      s  						svfloat64_t values0 = svld1_f64(pg, &currentValues0[j]);
      691             
      692   p      s  						svuint64_t indices0 = svld1sw_u64(pg, &currentColIndices0[j]);
      693             
      694   p      s  						svfloat64_t xvv0 = svld1_gather_u64index_f64(pg, xv, indices0);
      695             
      696   p      s  						contribs0 = svmla_f64_m(pg, contribs0, xvv0, values0 );
      697   p      s  					}
      698             
      699   p         					double totalContribution0 = svaddv_f64(svptrue_b64(), contribs0);
      700             
      701   p         					double sum0 = rv[i  ] - totalContribution0;
      702             
      703   p         					sum0 += xv[i  ] * currentDiagonal0;
      704             					
      705   p         				}
      706   p         			}
      707   p         		}
      708             	}
      709             LIKWID_STOP(trace.enabled, "symgs_bc");			
      710             
      711             	return 0;
      712             }
      713             /*
      714              * END OF BLOCK COLORED VERSION
      715              */
      716             #elif defined(HPCG_USE_NEON)
      717             
      718             /**************************************************************************************************/
      719             /**************************************************************************************************/
      720             /**************************************************************************************************/
      721             /* NEON IMPLEMENTATIONS                                                                           */
      722             /**************************************************************************************************/
      723             /**************************************************************************************************/
      724             /**************************************************************************************************/
      725             
      726             #include "arm_neon.h"
      727             
      728             /*
      729              * TDG VERSION
      730              */
      731             int ComputeSYMGS_TDG_NEON(const SparseMatrix & A, const Vector & r, Vector & x) {
      732             	assert(x.localLength == A.localNumberOfColumns);
      733             
      734             #ifndef HPCG_NO_MPI
      735             	ExchangeHalo(A, x);
      736             #endif
      737             
      738             	const double * const rv = r.values;
      739             	double * const xv = x.values;
      740             	double **matrixDiagonal = A.matrixDiagonal;
      741             
      742             	/*
      743             	 * FORWARD
      744             	 */
      745             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      746             #ifndef HPCG_NO_OPENMP
      747             #pragma omp parallel for SCHEDULE(runtime)
      748             #endif
      749             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      750             			local_int_t row = A.tdg[l][i];
      751             			const double * const currentValues = A.matrixValues[row];
      752             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      753             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      754             			const double currentDiagonal = matrixDiagonal[row][0];
      755             			float64x2_t contribs = vdupq_n_f64(0.0);
      756             
      757             			local_int_t j = 0;
      758             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      759             				// Load the needed j values
      760             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      761             				// Load the needed x values
      762             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      763             				float64x2_t xvv = vld1q_f64(aux);
      764             				// Add the contribution
      765             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      766             			}
      767             			// reduce contributions
      768             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      769             			double sum = rv[row] - totalContribution;
      770             			// Add missing values from last loop
      771             			if ( j < currentNumberOfNonzeros ) {
      772             				sum -= currentValues[j] * xv[currentColIndices[j]];
      773             			}
      774             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      775             			xv[row] = sum / currentDiagonal; // update row
      776             		}
      777             	}
      778             
      779             	/*
      780             	 * BACKWARD
      781             	 */
      782             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      783             #ifndef HPCG_NO_OPENMP
      784             #pragma omp parallel for SCHEDULE(runtime)
      785             #endif
      786             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      787             			local_int_t row = A.tdg[l][i];
      788             			const double * const currentValues = A.matrixValues[row];
      789             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      790             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      791             			const double currentDiagonal = matrixDiagonal[row][0];
      792             			float64x2_t contribs = vdupq_n_f64(0.0);
      793             
      794             			local_int_t j = 0;
      795             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      796             				// Load the needed j values
      797             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      798             				// Load the needed x values
      799             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      800             				float64x2_t xvv = vld1q_f64(aux);
      801             				// Add the contribution
      802             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      803             			}
      804             			// reduce contributions
      805             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      806             			double sum = rv[row] - totalContribution;
      807             			// Add missing values from last loop
      808             			if ( j < currentNumberOfNonzeros ) {
      809             				sum -= currentValues[j] * xv[currentColIndices[j]];
      810             			}
      811             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      812             			xv[row] = sum / currentDiagonal; // update row
      813             		}
      814             	}
      815             
      816             	return 0;
      817             }
      818             /*
      819              *
      820              */
      821             ////////////////////////////////////////////////////////////////////////////////
      822             ////////////////////////////////////////////////////////////////////////////////
      823             ////////////////////////////////////////////////////////////////////////////////
      824             /*
      825              * TDG FUSED VERSION
      826              */
      827             int ComputeFusedSYMGS_SPMV_NEON(const SparseMatrix & A, const Vector & r, Vector & x, Vector & y) {
      828             	assert(x.localLength == A.localNumberOfColumns);
      829             
      830             #ifndef HPCG_NO_MPI
      831             	ExchangeHalo(A, x);
      832             #endif
      833             
      834             	const double * const rv = r.values;
      835             	double * const xv = x.values;
      836             	double * const yv = y.values;
      837             	double **matrixDiagonal = A.matrixDiagonal;
      838             
      839             	/*
      840             	 * FORWARD
      841             	 */
      842             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
      843             #ifndef HPCG_NO_OPENMP
      844             #pragma omp parallel for SCHEDULE(runtime)
      845             #endif
      846             		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
      847             			local_int_t row = A.tdg[l][i];
      848             			const double * const currentValues = A.matrixValues[row];
      849             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      850             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      851             			const double currentDiagonal = matrixDiagonal[row][0];
      852             			float64x2_t contribs = vdupq_n_f64(0.0);
      853             
      854             			local_int_t j = 0;
      855             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      856             				// Load the needed j values
      857             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      858             				// Load the needed x values
      859             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      860             				float64x2_t xvv = vld1q_f64(aux);
      861             				// Add the contribution
      862             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      863             			}
      864             			// reduce contributions
      865             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      866             			double sum = rv[row] - totalContribution;
      867             			// Add missing values from last loop
      868             			if ( j < currentNumberOfNonzeros ) {
      869             				sum -= currentValues[j] * xv[currentColIndices[j]];
      870             			}
      871             			sum += xv[row] * currentDiagonal; // remove diagonal contribution
      872             			xv[row] = sum / currentDiagonal; // update row
      873             		}
      874             	}
      875             
      876             	/*
      877             	 * BACKWARD (fusing SYMGS and SPMV)
      878             	 */
      879             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
      880             #ifndef HPCG_NO_OPENMP
      881             #pragma omp parallel for SCHEDULE(runtime)
      882             #endif
      883             		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
      884             			local_int_t row = A.tdg[l][i];
      885             			const double * const currentValues = A.matrixValues[row];
      886             			const local_int_t * const currentColIndices = A.mtxIndL[row];
      887             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
      888             			const double currentDiagonal = matrixDiagonal[row][0];
      889             			float64x2_t contribs = vdupq_n_f64(0.0);
      890             
      891             			local_int_t j = 0;
      892             			for ( j = 0; j < currentNumberOfNonzeros-1; j+=2 ) {
      893             				// Load the needed j values
      894             				float64x2_t mtxValues = vld1q_f64(&currentValues[j]);
      895             				// Load the needed x values
      896             				double aux[] = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
      897             				float64x2_t xvv = vld1q_f64(aux);
      898             				// Add the contribution
      899             				contribs = vfmaq_f64(contribs, mtxValues, xvv);
      900             			}
      901             			// reduce contributions
      902             			double totalContribution = vgetq_lane_f64(contribs, 0) + vgetq_lane_f64(contribs, 1);
      903             			// Add missing values from last loop
      904             			if ( j < currentNumberOfNonzeros ) {
      905             				totalContribution += currentValues[j] * xv[currentColIndices[j]];
      906             			}
      907             			totalContribution -= xv[row] * currentDiagonal; // remove diagonal contribution
      908             			double sum = rv[row] - totalContribution; // substract contributions from RHS
      909             			xv[row] = sum / currentDiagonal; // update row
      910             			// Fusion part
      911             			totalContribution += xv[row] * currentDiagonal; // add updated diagonal contribution
      912             			yv[row] = totalContribution; // update SPMV output vector
      913             		}
      914             	}
      915             
      916             	return 0;
      917             }
      918             /*
      919              *
      920              */
      921             ////////////////////////////////////////////////////////////////////////////////
      922             ////////////////////////////////////////////////////////////////////////////////
      923             ////////////////////////////////////////////////////////////////////////////////
      924             /*
      925              * BLOCK COLORED VERSION
      926              */
      927             int ComputeSYMGS_BLOCK_NEON(const SparseMatrix & A, const Vector & r, Vector & x) {
      928             
      929             	assert(x.localLength >= A.localNumberOfColumns);
      930             	
      931             #ifndef HPCG_NO_MPI
      932             	ExchangeHalo(A, x);
      933             #endif
      934             
      935             	double **matrixDiagonal = A.matrixDiagonal;
      936             	const double * const rv = r.values;
      937             	double * const xv = x.values;
      938             
      939             	local_int_t firstBlock = 0;
      940             	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
      941             	/*
      942             	 * FORWARD
      943             	 */
      944             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) { // for each color
      945             		if ( color > 0 ) {
      946             			firstBlock += A.numberOfBlocksInColor[color-1];
      947             			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
      948             		}
      949             #ifndef HPCG_NO_OPENMP
      950             #pragma omp parallel for SCHEDULE(runtime)
      951             #endif
      952             		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) { // for each super block with the same color
      953             			local_int_t firstRow = block * A.blockSize;
      954             			local_int_t firstChunk = firstRow / A.chunkSize;
      955             			local_int_t lastChunk = (firstRow + A.blockSize*A.chunkSize) / A.chunkSize;
      956             
      957             			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) { // for each chunk of this super block
      958             				local_int_t first = A.chunkSize * chunk;
      959             				local_int_t last = first + A.chunkSize;
      960             
      961             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
      962             				local_int_t i = first;
      963             				if ( A.chunkSize == 4 ) {
      964             					const double * const currentValues0 = A.matrixValues[i  ];
      965             					const double * const currentValues1 = A.matrixValues[i+1];
      966             					const double * const currentValues2 = A.matrixValues[i+2];
      967             					const double * const currentValues3 = A.matrixValues[i+3];
      968             
      969             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
      970             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
      971             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
      972             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
      973             
      974             					const double currentDiagonal[4] = { matrixDiagonal[i  ][0],\
      975             														matrixDiagonal[i+1][0],\
      976             														matrixDiagonal[i+2][0],\
      977             														matrixDiagonal[i+3][0]};
      978             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
      979             					float64x2_t diagonal23 = vld1q_f64(&currentDiagonal[2]);
      980             
      981             					float64x2_t contribs0 = vdupq_n_f64(0.0);
      982             					float64x2_t contribs1 = vdupq_n_f64(0.0);
      983             					float64x2_t contribs2 = vdupq_n_f64(0.0);
      984             					float64x2_t contribs3 = vdupq_n_f64(0.0);
      985             
      986             					float64x2_t vrv01 = vld1q_f64(&rv[i]);
      987             					float64x2_t vrv23 = vld1q_f64(&rv[i+2]);
      988             
      989             					float64x2_t vxv01 = vld1q_f64(&xv[i]);
      990             					float64x2_t vxv23 = vld1q_f64(&xv[i+2]);
      991             
      992             					local_int_t j = 0;
      993             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
      994             						// Load values
      995             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
      996             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
      997             						float64x2_t values2 = vld1q_f64(&currentValues2[j]);
      998             						float64x2_t values3 = vld1q_f64(&currentValues3[j]);
      999             
     1000             						// Load x
     1001             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1002             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1003             						float64x2_t vxv2 = { xv[currentColIndices2[j]], xv[currentColIndices2[j+1]] };
     1004             						float64x2_t vxv3 = { xv[currentColIndices3[j]], xv[currentColIndices3[j+1]] };
     1005             
     1006             						// Add contribution
     1007             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1008             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1009             						contribs2 = vfmaq_f64(contribs2, values2, vxv2);
     1010             						contribs3 = vfmaq_f64(contribs3, values3, vxv3);
     1011             					}
     1012             					// Reduce contribution
     1013             					// First for i and i+1
     1014             					float64x2_t totalContribution01;
     1015             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1016             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1017             
     1018             					// Then for i+2 and i+3
     1019             					float64x2_t totalContribution23;
     1020             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs2), totalContribution23, 0);
     1021             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs3), totalContribution23, 1);
     1022             
     1023             					// Substract contributions from RHS
     1024             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1025             					float64x2_t sum23 = vsubq_f64(vrv23, totalContribution23);
     1026             
     1027             					// Add contributions from missing elements (if any)
     1028             					if ( j < currentNumberOfNonzeros ) {
     1029             						// Load current values
     1030             						float64x2_t values01 = { currentValues0[j], currentValues1[j] };
     1031             						float64x2_t values23 = { currentValues2[j], currentValues3[j] };
     1032             
     1033             						// Load x
     1034             						float64x2_t vx01 = { xv[currentColIndices0[j]], xv[currentColIndices1[j]] };
     1035             						float64x2_t vx23 = { xv[currentColIndices2[j]], xv[currentColIndices3[j]] };
     1036             
     1037             						// Add contributions
     1038             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1039             						sum23 = vfmsq_f64(sum23, values23, vx23);
     1040             					}
     1041             
     1042             					// Remove diagonal contribution and update rows i and i+1
     1043             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1044             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1045             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1046             
     1047             					// Remove diagonal contribution and update rows i+2 and i+3
     1048             					sum23 = vfmaq_f64(sum23, vxv23, diagonal23);
     1049             					xv[i+2] = vgetq_lane_f64(sum23, 0) / currentDiagonal[2];
     1050             					xv[i+3] = vgetq_lane_f64(sum23, 1) / currentDiagonal[3];
     1051             				} else if ( A.chunkSize == 2 ) {
     1052             					const double * const currentValues0 = A.matrixValues[i  ];
     1053             					const double * const currentValues1 = A.matrixValues[i+1];
     1054             
     1055             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1056             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1057             
     1058             					const double currentDiagonal[2] = { matrixDiagonal[i  ][0],\
     1059             														matrixDiagonal[i+1][0]};
     1060             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1061             
     1062             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1063             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1064             
     1065             					float64x2_t vrv01 = vld1q_f64(&rv[i]);
     1066             
     1067             					float64x2_t vxv01 = vld1q_f64(&xv[i]);
     1068             
     1069             					local_int_t j = 0;
     1070             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1071             						// Load values
     1072             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1073             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1074             
     1075             						// Load x
     1076             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1077             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1078             
     1079             						// Add contribution
     1080             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1081             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1082             					}
     1083             					// Reduce contribution
     1084             					// First for i and i+1
     1085             					float64x2_t totalContribution01;
     1086             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1087             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1088             
     1089             					// Substract contributions from RHS
     1090             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1091             
     1092             					// Add contributions from missing elements (if any)
     1093             					if ( j < currentNumberOfNonzeros ) {
     1094             						// Load current values
     1095             						float64x2_t values01 = { currentValues0[j], currentValues1[j] };
     1096             
     1097             						// Load x
     1098             						float64x2_t vx01 = { xv[currentColIndices0[j]], xv[currentColIndices1[j]] };
     1099             
     1100             						// Add contributions
     1101             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1102             					}
     1103             
     1104             					// Remove diagonal contribution and update rows i and i+1
     1105             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1106             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1107             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1108             				} else { // A.chunkSize == 1
     1109             					const double * const currentValues = A.matrixValues[i];
     1110             					const local_int_t * const currentColIndices = A.mtxIndL[i];
     1111             					const double currentDiagonal = matrixDiagonal[i][0];
     1112             					float64x2_t contribs = vdupq_n_f64(0.0);
     1113             
     1114             					local_int_t j = 0;
     1115             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1116             						// Load values
     1117             						float64x2_t values = vld1q_f64(&currentValues[j]);
     1118             
     1119             						// Load x
     1120             						float64x2_t vxv = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
     1121             
     1122             						// Add contribution
     1123             						contribs = vfmaq_f64(contribs, values, vxv);
     1124             					}
     1125             					// Reduce contribution
     1126             					// First for i and i+1
     1127             					double totalContribution;
     1128             					totalContribution = vaddvq_f64(contribs);
     1129             
     1130             					// Substract contributions from RHS
     1131             					double sum = rv[i] - totalContribution;
     1132             
     1133             					// Add contributions from missing elements (if any)
     1134             					if ( j < currentNumberOfNonzeros ) {
     1135             						sum -= currentValues[j] * xv[currentColIndices[j]];
     1136             					}
     1137             
     1138             					// Remove diagonal contribution and update rows i and i+1
     1139             					sum += xv[i] * currentDiagonal;
     1140             					xv[i] = sum / currentDiagonal;
     1141             				}
     1142             			}
     1143             		}
     1144             	}
     1145             
     1146             	firstBlock = A.numberOfBlocks-1;
     1147             	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
     1148             	/*
     1149             	 * BACKWARD
     1150             	 */
     1151             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
     1152             		if ( color < A.numberOfColors-1 ) {
     1153             			firstBlock -= A.numberOfBlocksInColor[color+1];
     1154             			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
     1155             		}
     1156             #ifndef HPCG_NO_OPENMP
     1157             #pragma omp parallel for SCHEDULE(runtime)
     1158             #endif
     1159             		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) { // we skip a whole superblock on each iteration
     1160             			local_int_t firstRow = ((block+1) * A.blockSize) - 1; // this is the last row of the last block (i.e., next block first row - 1)
     1161             			local_int_t firstChunk = firstRow / A.chunkSize; // this is the  chunk of the row above
     1162             			local_int_t lastChunk = (firstRow - A.blockSize*A.chunkSize) / A.chunkSize; 
     1163             
     1164             			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
     1165             				local_int_t first = A.chunkSize * chunk;
     1166             				local_int_t last = first + A.chunkSize;
     1167             
     1168             				const int currentNumberOfNonzeros = A.nonzerosInChunk[chunk];
     1169             				if ( A.chunkSize == 4 ) {
     1170             					local_int_t i = last-1-3;
     1171             
     1172             					const double * const currentValues3 = A.matrixValues[i+3];
     1173             					const double * const currentValues2 = A.matrixValues[i+2];
     1174             					const double * const currentValues1 = A.matrixValues[i+1];
     1175             					const double * const currentValues0 = A.matrixValues[i  ];
     1176             
     1177             					const local_int_t * const currentColIndices3 = A.mtxIndL[i+3];
     1178             					const local_int_t * const currentColIndices2 = A.mtxIndL[i+2];
     1179             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1180             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1181             
     1182             					const double currentDiagonal[4] = {\
     1183             							matrixDiagonal[i  ][0],\
     1184             							matrixDiagonal[i+1][0],\
     1185             							matrixDiagonal[i+2][0],\
     1186             							matrixDiagonal[i+3][0]};
     1187             
     1188             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1189             					float64x2_t diagonal23 = vld1q_f64(&currentDiagonal[2]);
     1190             
     1191             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1192             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1193             					float64x2_t contribs2 = vdupq_n_f64(0.0);
     1194             					float64x2_t contribs3 = vdupq_n_f64(0.0);
     1195             
     1196             					float64x2_t vrv23 = vld1q_f64(&rv[i+2]);
     1197             					float64x2_t vrv01 = vld1q_f64(&rv[i  ]);
     1198             
     1199             					float64x2_t vxv23 = vld1q_f64(&xv[i+2]);
     1200             					float64x2_t vxv01 = vld1q_f64(&xv[i  ]);
     1201             
     1202             					local_int_t j = 0;
     1203             					for ( j = currentNumberOfNonzeros-2; j >= 0; j -= 2 ) {
     1204             						// Load values
     1205             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1206             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1207             						float64x2_t values2 = vld1q_f64(&currentValues2[j]);
     1208             						float64x2_t values3 = vld1q_f64(&currentValues3[j]);
     1209             
     1210             						// Load x
     1211             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1212             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1213             						float64x2_t vxv2 = { xv[currentColIndices2[j]], xv[currentColIndices2[j+1]] };
     1214             						float64x2_t vxv3 = { xv[currentColIndices3[j]], xv[currentColIndices3[j+1]] };
     1215             
     1216             						// Add contribution
     1217             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1218             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1219             						contribs2 = vfmaq_f64(contribs2, values2, vxv2);
     1220             						contribs3 = vfmaq_f64(contribs3, values3, vxv3);
     1221             					}
     1222             					// Reduce contribution
     1223             					// First for i and i-1
     1224             					float64x2_t totalContribution01;
     1225             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1226             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1227             
     1228             					// Then for i-2 and i-3
     1229             					float64x2_t totalContribution23;
     1230             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs2), totalContribution23, 0);
     1231             					totalContribution23 = vsetq_lane_f64(vaddvq_f64(contribs3), totalContribution23, 1);
     1232             
     1233             					// Substract contributions from RHS
     1234             					float64x2_t sum23 = vsubq_f64(vrv23, totalContribution23);
     1235             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1236             
     1237             					// Add contributions from missing elements (if any)
     1238             					if ( j == -1 ) {
     1239             						// Load current values
     1240             						float64x2_t values23 = { currentValues2[j+1], currentValues3[j+1] };
     1241             						float64x2_t values01 = { currentValues0[j+1], currentValues1[j+1] };
     1242             
     1243             						// Load x
     1244             						float64x2_t vx23 = { xv[currentColIndices2[j+1]], xv[currentColIndices3[j+1]] };
     1245             						float64x2_t vx01 = { xv[currentColIndices0[j+1]], xv[currentColIndices1[j+1]] };
     1246             
     1247             						// Add contributions
     1248             						sum23 = vfmsq_f64(sum23, values23, vx23);
     1249             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1250             					}
     1251             
     1252             					// Remove diagonal contribution and update rows i-2 and i-3
     1253             					sum23 = vfmaq_f64(sum23, vxv23, diagonal23);
     1254             					xv[i+3] = vgetq_lane_f64(sum23, 1) / currentDiagonal[3];
     1255             					xv[i+2] = vgetq_lane_f64(sum23, 0) / currentDiagonal[2];
     1256             
     1257             					// Remove diagonal contribution and update rows i and i-1
     1258             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1259             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1260             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1261             				} else if ( A.chunkSize == 2 ) {
     1262             					local_int_t i = last-1-1;
     1263             
     1264             					const double * const currentValues1 = A.matrixValues[i+1];
     1265             					const double * const currentValues0 = A.matrixValues[i  ];
     1266             
     1267             					const local_int_t * const currentColIndices1 = A.mtxIndL[i+1];
     1268             					const local_int_t * const currentColIndices0 = A.mtxIndL[i  ];
     1269             
     1270             					const double currentDiagonal[2] = {\
     1271             							matrixDiagonal[i  ][0],\
     1272             							matrixDiagonal[i+1][0]};
     1273             
     1274             					float64x2_t diagonal01 = vld1q_f64(&currentDiagonal[0]);
     1275             
     1276             					float64x2_t contribs0 = vdupq_n_f64(0.0);
     1277             					float64x2_t contribs1 = vdupq_n_f64(0.0);
     1278             
     1279             					float64x2_t vrv01 = vld1q_f64(&rv[i  ]);
     1280             
     1281             					float64x2_t vxv01 = vld1q_f64(&xv[i  ]);
     1282             
     1283             					local_int_t j = 0;
     1284             					for ( j = currentNumberOfNonzeros-2; j >= 0; j -= 2 ) {
     1285             						// Load values
     1286             						float64x2_t values0 = vld1q_f64(&currentValues0[j]);
     1287             						float64x2_t values1 = vld1q_f64(&currentValues1[j]);
     1288             
     1289             						// Load x
     1290             						float64x2_t vxv0 = { xv[currentColIndices0[j]], xv[currentColIndices0[j+1]] };
     1291             						float64x2_t vxv1 = { xv[currentColIndices1[j]], xv[currentColIndices1[j+1]] };
     1292             
     1293             						// Add contribution
     1294             						contribs0 = vfmaq_f64(contribs0, values0, vxv0);
     1295             						contribs1 = vfmaq_f64(contribs1, values1, vxv1);
     1296             					}
     1297             					// Reduce contribution
     1298             					// First for i and i-1
     1299             					float64x2_t totalContribution01;
     1300             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs0), totalContribution01, 0);
     1301             					totalContribution01 = vsetq_lane_f64(vaddvq_f64(contribs1), totalContribution01, 1);
     1302             
     1303             					// Substract contributions from RHS
     1304             					float64x2_t sum01 = vsubq_f64(vrv01, totalContribution01);
     1305             
     1306             					// Add contributions from missing elements (if any)
     1307             					if ( j == -1 ) {
     1308             						// Load current values
     1309             						float64x2_t values01 = { currentValues0[j+1], currentValues1[j+1] };
     1310             
     1311             						// Load x
     1312             						float64x2_t vx01 = { xv[currentColIndices0[j+1]], xv[currentColIndices1[j+1]] };
     1313             
     1314             						// Add contributions
     1315             						sum01 = vfmsq_f64(sum01, values01, vx01);
     1316             					}
     1317             
     1318             					// Remove diagonal contribution and update rows i and i-1
     1319             					sum01 = vfmaq_f64(sum01, vxv01, diagonal01);
     1320             					xv[i+1] = vgetq_lane_f64(sum01, 1) / currentDiagonal[1];
     1321             					xv[i  ] = vgetq_lane_f64(sum01, 0) / currentDiagonal[0];
     1322             				} else { // A.chunkSize == 1
     1323             					local_int_t i = last - 1; // == first
     1324             					const double * const currentValues = A.matrixValues[i];
     1325             					const local_int_t * const currentColIndices = A.mtxIndL[i];
     1326             					const double currentDiagonal = matrixDiagonal[i][0];
     1327             
     1328             					float64x2_t contribs = vdupq_n_f64(0.0);
     1329             
     1330             					local_int_t j = 0;
     1331             					for ( j = 0; j < currentNumberOfNonzeros-1; j += 2 ) {
     1332             						// Load values
     1333             						float64x2_t values = vld1q_f64(&currentValues[j]);
     1334             
     1335             						// Load x
     1336             						float64x2_t vxv = { xv[currentColIndices[j]], xv[currentColIndices[j+1]] };
     1337             
     1338             						// Add contribution
     1339             						contribs = vfmaq_f64(contribs, values, vxv);
     1340             					}
     1341             					// Reduce contribution
     1342             					double totalContribution = vaddvq_f64(contribs);
     1343             
     1344             					// Substract contribution from RHS
     1345             					double sum = rv[i] - totalContribution;
     1346             
     1347             					// Add contributions from missing elements (if any)
     1348             					if ( j < currentNumberOfNonzeros ) {
     1349             						sum -= currentValues[j] * xv[currentColIndices[j]];
     1350             					}
     1351             
     1352             					// Remove diagonal contribution and updated row i
     1353             					sum += xv[i] * currentDiagonal;
     1354             					xv[i] = sum / currentDiagonal;
     1355             				}
     1356             			}
     1357             		}
     1358             	}
     1359             
     1360             	return 0;
     1361             }
     1362             /*
     1363              *
     1364              */
     1365             #endif
     1366             //#else // !HPCG_USE_SVE ! HPCG_USE_NEON
     1367             
     1368             int ComputeFusedSYMGS_SPMV ( const SparseMatrix & A, const Vector & r, Vector & x, Vector & y ) {
     1369             	assert(x.localLength == A.localNumberOfColumns);
     1370             
     1371             #ifndef HPCG_NO_MPI
     1372             	ExchangeHalo(A, x);
     1373             #endif
     1374             
     1375             	const double * const rv = r.values;
     1376             	double * const xv = x.values;
     1377             	double * const yv = y.values;
     1378             	double **matrixDiagonal = A.matrixDiagonal;
     1379             
     1380             	/*
     1381             	 * FORWARD
     1382             	 */
     1383    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1384             #ifndef HPCG_NO_OPENMP
     1385             #pragma omp parallel for SCHEDULE(runtime)
     1386             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1387   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
     1388   p         			local_int_t row = A.tdg[l][i];
     1389   p         			const double * const currentValues = A.matrixValues[row];
     1390   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1391   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1392   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1393   p         			double sum = rv[row];
     1394             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1395   p     8v  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j++ ) {
     1396   p     8v  				local_int_t curCol = currentColIndices[j];
     1397   p     8v  				sum -= currentValues[j] * xv[curCol];
     1398   p     8v  			}
     1399   p         			sum += xv[row] * currentDiagonal;
     1400   p         			xv[row] = sum / currentDiagonal;
     1401   pi        		}
     1402    i        	}
     1403             
     1404             	/*
     1405             	 * BACKWARD (fusing SYMGS and SPMV)
     1406             	 */
     1407    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1408             #ifndef HPCG_NO_OPENMP
     1409             #pragma omp parallel for SCHEDULE(runtime)
     1410             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1411   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
     1412   p         			local_int_t row = A.tdg[l][i];
     1413   p         			const double * const currentValues = A.matrixValues[row];
     1414   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1415   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1416   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1417   p         			double sum = 0.0;
     1418             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1419   p     8v  			for ( local_int_t j = currentNumberOfNonzeros-1; j >= 0; j-- ) {
     1420   p     8v  				local_int_t curCol = currentColIndices[j];
     1421   p     8v  				sum += currentValues[j] * xv[curCol];
     1422   p     8v  			}
     1423   p         			sum -= xv[row] * currentDiagonal;
     1424   p         			xv[row] = (rv[row] - sum) / currentDiagonal;
     1425   p         			sum += xv[row] * currentDiagonal;
     1426   p         			yv[row] = sum;
     1427   p         		}
     1428             	}
     1429             
     1430             	return 0;
     1431             }
     1432             
     1433             int ComputeSYMGS_TDG ( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
     1434             
     1435             	assert( x.localLength == A.localNumberOfColumns);
     1436             
     1437             #ifndef HPCG_NO_MPI
     1438             	ExchangeHalo(A,x);
     1439             #endif
     1440             
     1441             	const double * const rv = r.values;
     1442             	double * const xv = x.values;
     1443             	double **matrixDiagonal = A.matrixDiagonal;
     1444             
     1445             /*#ifndef HPCG_NO_OPENMP
     1446             #pragma omp parallel SCHEDULE(runtime)
     1447             {
     1448             #endif
     1449             */
     1450             #pragma statement scache_isolate_way L2=10
     1451             #pragma statement scache_isolate_assign xv
     1452             
     1453             	/*
     1454             	 * FORWARD
     1455             	 */
     1456    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1457             #ifndef HPCG_NO_OPENMP
     1458             #pragma omp parallel for SCHEDULE(runtime)
     1459             #endif
     1460             		#pragma loop unroll_and_jam(4)
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1461   pi        		for ( local_int_t i = 0; i < A.tdg[l].size(); i++ ) {
     1462   p         			local_int_t row = A.tdg[l][i];
     1463   p         			const double * const currentValues = A.matrixValues[row];
     1464   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1465   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1466   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1467   p         			double sum = rv[row];
     1468             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1469   p     8v  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j++ ) {
     1470   p     8v  				local_int_t curCol = currentColIndices[j];
     1471   p     8v  				sum -= currentValues[j] * xv[curCol];
     1472   p     8v  			}
     1473   p         			sum += xv[row] * currentDiagonal;
     1474   p         			xv[row] = sum / currentDiagonal;
     1475   pi        		}
     1476    i        	}
     1477             
     1478             	/*
     1479             	 * BACKWARD
     1480             	 */
     1481    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1482             #ifndef HPCG_NO_OPENMP
     1483             #pragma omp parallel for SCHEDULE(runtime)
     1484             #endif
     1485             		#pragma loop unroll_and_jam(4)
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1486   pi        		for ( local_int_t i = A.tdg[l].size()-1; i >= 0; i-- ) {
     1487   p         			local_int_t row = A.tdg[l][i];
     1488   p         			const double * const currentValues = A.matrixValues[row];
     1489   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1490   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1491   p         			const double currentDiagonal = matrixDiagonal[row][0];
     1492   p         			double sum = rv[row];
     1493             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1494   p     8v  			for ( local_int_t j = currentNumberOfNonzeros-1; j >= 0; j-- ) {
     1495   p     8v  				local_int_t curCol = currentColIndices[j];
     1496   p     8v  				sum -= currentValues[j] * xv[curCol];
     1497   p     8v  			}
     1498   p         			sum += xv[row] * currentDiagonal;
     1499   p         			xv[row] = sum / currentDiagonal;
     1500   p         		}
     1501             	}
     1502             
     1503             	#pragma statement end_scache_isolate_assign
     1504             	#pragma statement end_scache_isolate_way
     1505             /*#ifndef HPCG_NO_OPENMP
     1506             }
     1507             #endif*/
     1508             
     1509             	return 0;
     1510             }
     1511             
     1512             int ComputeSYMGS_BLOCK( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace ) {
     1513             
     1514             	assert(x.localLength >= A.localNumberOfColumns);
     1515             	
     1516             #ifndef HPCG_NO_MPI
     1517             	ExchangeHalo(A, x);
     1518             #endif
     1519             
     1520             	const local_int_t nrow = A.localNumberOfRows;
     1521             	double **matrixDiagonal = A.matrixDiagonal;
     1522             	const double * const rv = r.values;
     1523             	double * const xv = x.values;
     1524             
     1525             	local_int_t firstBlock = 0;
     1526    i        	local_int_t lastBlock = firstBlock + A.numberOfBlocksInColor[0];
     1527             	/*
     1528             	 * FORWARD
     1529             	 */
     1530             	for ( local_int_t color = 0; color < A.numberOfColors; color++ ) {
     1531             		if ( color > 0 ) {
     1532    i        			firstBlock += A.numberOfBlocksInColor[color-1];
     1533    i        			lastBlock = firstBlock + A.numberOfBlocksInColor[color];
     1534             		}
     1535             #ifndef HPCG_NO_OPENMP
     1536             #pragma omp parallel for SCHEDULE(runtime)
     1537             #endif
     1538   p         		for ( local_int_t block = firstBlock; block < lastBlock; block += A.chunkSize ) {
     1539   p         			local_int_t firstRow = block * A.blockSize;
     1540   p         			local_int_t firstChunk = firstRow / A.chunkSize;
     1541   p         			local_int_t lastChunk = (firstRow + A.blockSize*A.chunkSize) / A.chunkSize;
     1542             
     1543   p         			for ( local_int_t chunk = firstChunk; chunk < lastChunk; chunk++ ) {
     1544   p         				local_int_t first = A.chunkSize * chunk;
     1545   p         				local_int_t last = first + A.chunkSize;
     1546             
     1547             				//for ( local_int_t i = first; i < last; i+= (A.chunkSize/2)) {
     1548   p         				local_int_t i = first;
     1549   p         				if ( A.chunkSize == 4 ) {
     1550   p         					double sum0 = rv[i+0];
     1551   p         					double sum1 = rv[i+1];
     1552   p         					double sum2 = rv[i+2];
     1553   p         					double sum3 = rv[i+3];
     1554             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1555   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1556   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1557   p      s  						sum1 -= A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
     1558   p      s  						sum2 -= A.matrixValues[i+2][j] * xv[A.mtxIndL[i+2][j]];
     1559   p      s  						sum3 -= A.matrixValues[i+3][j] * xv[A.mtxIndL[i+3][j]];
     1560   pi     s  					}
     1561   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1562   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1563   p         					sum1 += matrixDiagonal[i+1][0] * xv[i+1];
     1564   p         					xv[i+1] = sum1 / matrixDiagonal[i+1][0];
     1565   p         					sum2 += matrixDiagonal[i+2][0] * xv[i+2];
     1566   p         					xv[i+2] = sum2 / matrixDiagonal[i+2][0];
     1567   p         					sum3 += matrixDiagonal[i+3][0] * xv[i+3];
     1568   p         					xv[i+3] = sum3 / matrixDiagonal[i+3][0];
     1569   p         				} else if ( A.chunkSize == 2 ) {
     1570   p         					double sum0 = rv[i+0];
     1571   p         					double sum1 = rv[i+1];
     1572             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1573   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1574   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1575   p      s  						sum1 -= A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
     1576   pi     s  					}
     1577   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1578   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1579   p         					sum1 += matrixDiagonal[i+1][0] * xv[i+1];
     1580   p         					xv[i+1] = sum1 / matrixDiagonal[i+1][0];
     1581   p         				} else { // A.chunkSize == 1
     1582   p         					double sum0 = rv[i+0];
     1583             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1584   pi     s  					for ( local_int_t j = 0; j < A.nonzerosInChunk[chunk]; j++ ) {
     1585   p      s  						sum0 -= A.matrixValues[i+0][j] * xv[A.mtxIndL[i+0][j]];
     1586   pi     s  					}
     1587   p         					sum0 += matrixDiagonal[i+0][0] * xv[i+0];
     1588   p         					xv[i+0] = sum0 / matrixDiagonal[i+0][0];
     1589   p         				}
     1590   p         			}
     1591   p         		}
     1592             	}
     1593             
     1594             	firstBlock = A.numberOfBlocks-1;
     1595    i        	lastBlock = firstBlock - A.numberOfBlocksInColor[A.numberOfColors-1];
     1596             	/*
     1597             	 * BACKWARD
     1598             	 */
     1599             	for ( local_int_t color = A.numberOfColors-1; color >= 0; color-- ) {
     1600             		if ( color < A.numberOfColors-1 ) {
     1601    i        			firstBlock -= A.numberOfBlocksInColor[color+1];
     1602    i        			lastBlock = firstBlock - A.numberOfBlocksInColor[color];
     1603             		}
     1604             #ifndef HPCG_NO_OPENMP
     1605             #pragma omp parallel for SCHEDULE(runtime)
     1606             #endif
     1607   p         		for ( local_int_t block = firstBlock; block > lastBlock; block -= A.chunkSize ) {
     1608   p         			local_int_t firstRow = ((block+1) * A.blockSize) - 1; // this is the last row of the last block
     1609   p         			local_int_t firstChunk = firstRow / A.chunkSize; // this is the  chunk of the row above
     1610   p         			local_int_t lastChunk = (firstRow - A.blockSize*A.chunkSize) / A.chunkSize; 
     1611             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1612   p         			for ( local_int_t chunk = firstChunk; chunk > lastChunk; chunk-- ) {
     1613   p         				local_int_t first = A.chunkSize * chunk;
     1614   p         				local_int_t last = first + A.chunkSize;
     1615             
     1616             				//for ( local_int_t i = last-1; i >= first; i -= (A.chunkSize/2)) {
     1617   p         				local_int_t i = last-1;
     1618   p         				if ( A.chunkSize == 4 ) {
     1619   p         					double sum3 = rv[i-3];
     1620   p         					double sum2 = rv[i-2];
     1621   p         					double sum1 = rv[i-1];
     1622   p         					double sum0 = rv[i  ];
     1623             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.28, ITR: 32, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1624   pi     v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1625   p      v  						sum3 -= A.matrixValues[i-3][j] * xv[A.mtxIndL[i-3][j]];
     1626   p      v  						sum2 -= A.matrixValues[i-2][j] * xv[A.mtxIndL[i-2][j]];
     1627   p      v  						sum1 -= A.matrixValues[i-1][j] * xv[A.mtxIndL[i-1][j]];
     1628   p      v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1629   p      v  					}
     1630   p         					sum3 += matrixDiagonal[i-3][0] * xv[i-3];
     1631   p         					xv[i-3] = sum3 / matrixDiagonal[i-3][0];
     1632             
     1633   p         					sum2 += matrixDiagonal[i-2][0] * xv[i-2];
     1634   p         					xv[i-2] = sum2 / matrixDiagonal[i-2][0];
     1635             
     1636   p         					sum1 += matrixDiagonal[i-1][0] * xv[i-1];
     1637   p         					xv[i-1] = sum1 / matrixDiagonal[i-1][0];
     1638             
     1639   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1640   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1641   p         				} else if ( A.chunkSize == 2 ) {
     1642   p         					double sum1 = rv[i-1];
     1643   p         					double sum0 = rv[i  ];
     1644             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 96, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1645   pi    4v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1646   p     4v  						sum1 -= A.matrixValues[i-1][j] * xv[A.mtxIndL[i-1][j]];
     1647   p     4v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1648   p     4v  					}
     1649             
     1650   p         					sum1 += matrixDiagonal[i-1][0] * xv[i-1];
     1651   p         					xv[i-1] = sum1 / matrixDiagonal[i-1][0];
     1652             
     1653   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1654   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1655   p         				} else { // A.chunkSize == 1
     1656   p         					double sum0 = rv[i  ];
     1657             
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
     1658   pi    8v  					for ( local_int_t j = A.nonzerosInChunk[chunk]-1; j >= 0; j-- ) {
     1659   p     8v  						sum0 -= A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
     1660   p     8v  					}
     1661             
     1662   p         					sum0 += matrixDiagonal[i  ][0] * xv[i  ];
     1663   p         					xv[i  ] = sum0 / matrixDiagonal[i  ][0];
     1664   p         				}
     1665   p         			}
     1666   p         		}
     1667             	}
     1668             
     1669             	return 0;
     1670             }
     1671             //#endif
     1672             
     1673             
     1674             
     1675             /*!
     1676               Routine to compute one step of symmetric Gauss-Seidel:
     1677             
     1678               Assumption about the structure of matrix A:
     1679               - Each row 'i' of the matrix has nonzero diagonal value whose address is matrixDiagonal[i]
     1680               - Entries in row 'i' are ordered such that:
     1681                    - lower triangular terms are stored before the diagonal element.
     1682                    - upper triangular terms are stored after the diagonal element.
     1683                    - No other assumptions are made about entry ordering.
     1684             
     1685               Symmetric Gauss-Seidel notes:
     1686               - We use the input vector x as the RHS and start with an initial guess for y of all zeros.
     1687               - We perform one forward sweep.  Since y is initially zero we can ignore the upper triangular terms of A.
     1688               - We then perform one back sweep.
     1689                    - For simplicity we include the diagonal contribution in the for-j loop, then correct the sum after
     1690             
     1691               @param[in] A the known system matrix
     1692               @param[in] r the input vector
     1693               @param[inout] x On entry, x should contain relevant values, on exit x contains the result of one symmetric GS sweep with r as the RHS.
     1694             
     1695               @return returns 0 upon success and non-zero otherwise
     1696             
     1697               @warning Early versions of this kernel (Version 1.1 and earlier) had the r and x arguments in reverse order, and out of sync with other kernels.
     1698             
     1699               @see ComputeSYMGS_ref
     1700             */
     1701             int ComputeSYMGS( const SparseMatrix & A, const Vector & r, Vector & x, TraceData& trace) {
     1702             
     1703             	// This function is just a stub right now which decides which implementation of the SYMGS will be executed (TDG or block coloring)
     1704             	if ( A.TDG ) {
     1705             #ifdef HPCG_USE_NEON
     1706             		return ComputeSYMGS_TDG_NEON(A, r, x);
     1707             #elif defined HPCG_USE_SVE
     1708    i        		return ComputeSYMGS_TDG_SVE(A, r, x, trace);
     1709             #else
     1710             		return ComputeSYMGS_TDG(A, r, x, trace);
     1711             #endif
     1712             	}
     1713             #ifdef HPCG_USE_NEON
     1714             	return ComputeSYMGS_BLOCK_NEON(A, r, x);
     1715             #elif defined HPCG_USE_SVE
     1716             	return ComputeSYMGS_BLOCK_SVE(A, r, x, trace);
     1717             #else
     1718             	return ComputeSYMGS_BLOCK(A, r, x, trace);
     1719             #endif
     1720             }
     1721             
     1722             inline void SYMGS_VERSION_1(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     1723             	
     1724             	double **matrixDiagonal = A.matrixDiagonal;
     1725             
     1726             	/*
     1727             	 * FORWARD SWEEP
     1728             	 */
     1729             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1730             		local_int_t tdgLevelSize = A.tdg[l].size();
     1731             		if((tdgLevelSize%2) == 0) {
     1732             #ifndef HPCG_NO_OPENMP
     1733             #pragma omp parallel for SCHEDULE(runtime)
     1734             #endif
     1735             		for ( local_int_t i = 0; i < tdgLevelSize; i+=2 ) {
     1736             			local_int_t row_1 = A.tdg[l][i];
     1737             			local_int_t row_2 = A.tdg[l][i+1];
     1738             			const double * const currentValues_1 = A.matrixValues[row_1];
     1739             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1740             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1741             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1742             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1743             
     1744             			const double * const currentValues_2 = A.matrixValues[row_2];
     1745             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1746             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1747             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1748             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1749             			
     1750             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     1751             
     1752             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1753             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1754             				
     1755             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1756             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1757             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1758             
     1759             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1760             
     1761             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1762             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1763             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1764             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1765             
     1766             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1767             			}
     1768             
     1769             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1770             			double sum_1 = rv[row_1] - totalContribution_1;
     1771             
     1772             			sum_1 += xv[row_1] * currentDiagonal_1;
     1773             			xv[row_1] = sum_1 / currentDiagonal_1;
     1774             
     1775             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1776             			double sum_2 = rv[row_2] - totalContribution_2;
     1777             
     1778             			sum_2 += xv[row_2] * currentDiagonal_2;
     1779             			xv[row_2] = sum_2 / currentDiagonal_2;
     1780             		}
     1781             		}
     1782             		else
     1783             		{
     1784             #ifndef HPCG_NO_OPENMP
     1785             #pragma omp parallel for SCHEDULE(runtime)
     1786             #endif
     1787             		for ( local_int_t i = 0; i < tdgLevelSize; i++ ) {
     1788             			local_int_t row = A.tdg[l][i];
     1789             			const double * const currentValues = A.matrixValues[row];
     1790             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1791             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1792             			const double currentDiagonal = matrixDiagonal[row][0];
     1793             			svfloat64_t contribs = svdup_f64(0.0);
     1794             
     1795             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1796             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1797             				
     1798             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1799             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1800             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1801             
     1802             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1803             			}
     1804             
     1805             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1806             			double sum = rv[row] - totalContribution;
     1807             
     1808             			sum += xv[row] * currentDiagonal;
     1809             			xv[row] = sum / currentDiagonal;
     1810             		}
     1811             		}
     1812             	}
     1813             
     1814             	/*
     1815             	 * BACKWARD SWEEP
     1816             	 */
     1817             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1818             		local_int_t tdgLevelSize = A.tdg[l].size();
     1819             		if((tdgLevelSize%2) == 0) {		
     1820             #ifndef HPCG_NO_OPENMP
     1821             #pragma omp parallel for SCHEDULE(runtime)
     1822             #endif
     1823             		for ( local_int_t i = tdgLevelSize-1; i >= 0; i-= 2 ) {
     1824             			local_int_t row_1 = A.tdg[l][i];
     1825             			local_int_t row_2 = A.tdg[l][i-1];
     1826             			const double * const currentValues_1 = A.matrixValues[row_1];
     1827             			const double * const currentValues_2 = A.matrixValues[row_2];
     1828             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1829             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1830             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1831             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1832             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1833             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1834             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1835             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1836             
     1837             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     1838             							
     1839             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1840             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1841             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1842             				
     1843             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1844             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1845             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1846             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1847             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1848             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1849             
     1850             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1851             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1852             			}
     1853             
     1854             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1855             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1856             			double sum_1 = rv[row_1] - totalContribution_1;
     1857             			double sum_2 = rv[row_2] - totalContribution_2;
     1858             
     1859             			sum_1 += xv[row_1] * currentDiagonal_1;
     1860             			sum_2 += xv[row_2] * currentDiagonal_2;
     1861             			xv[row_1] = sum_1 / currentDiagonal_1;
     1862             			xv[row_2] = sum_2 / currentDiagonal_2;
     1863             		}
     1864             		}
     1865             		else
     1866             		{
     1867             #ifndef HPCG_NO_OPENMP
     1868             #pragma omp parallel for SCHEDULE(runtime)
     1869             #endif
     1870             		for ( local_int_t i = tdgLevelSize-1; i >= 0; i-- ) {
     1871             			local_int_t row = A.tdg[l][i];
     1872             			const double * const currentValues = A.matrixValues[row];
     1873             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1874             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1875             			const double currentDiagonal = matrixDiagonal[row][0];
     1876             			svfloat64_t contribs = svdup_f64(0.0);
     1877             
     1878             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1879             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1880             				
     1881             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1882             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1883             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1884             
     1885             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1886             			}
     1887             
     1888             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1889             			double sum = rv[row] - totalContribution;
     1890             
     1891             			sum += xv[row] * currentDiagonal;
     1892             			xv[row] = sum / currentDiagonal;
     1893             		}
     1894             		}
     1895             	}
     1896             }
     1897             
     1898             inline void SYMGS_VERSION_2(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     1899             	
     1900             	double **matrixDiagonal = A.matrixDiagonal;
     1901             
     1902             	/*
     1903             	 * FORWARD SWEEP
     1904             	 */
     1905             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     1906             		local_int_t tdgLevelSize = A.tdg[l].size();
     1907             		local_int_t maxLevelSize = 2*(tdgLevelSize / 2);
     1908             
     1909             #ifndef HPCG_NO_OPENMP
     1910             	#pragma omp parallel
     1911             	{
     1912             	#pragma omp for nowait SCHEDULE(runtime)
     1913             #endif
     1914             		for ( local_int_t i = 0; i < maxLevelSize; i+=2 ) {
     1915             			local_int_t row_1 = A.tdg[l][i];
     1916             			local_int_t row_2 = A.tdg[l][i+1];
     1917             			const double * const currentValues_1 = A.matrixValues[row_1];
     1918             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     1919             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     1920             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     1921             			svfloat64_t contribs_1 = svdup_f64(0.0);
     1922             
     1923             			const double * const currentValues_2 = A.matrixValues[row_2];
     1924             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     1925             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     1926             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     1927             			svfloat64_t contribs_2 = svdup_f64(0.0);
     1928             			
     1929             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     1930             
     1931             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     1932             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     1933             				
     1934             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     1935             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     1936             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     1937             
     1938             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     1939             
     1940             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     1941             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     1942             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     1943             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     1944             
     1945             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     1946             			}
     1947             
     1948             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     1949             			double sum_1 = rv[row_1] - totalContribution_1;
     1950             
     1951             			sum_1 += xv[row_1] * currentDiagonal_1;
     1952             			xv[row_1] = sum_1 / currentDiagonal_1;
     1953             
     1954             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     1955             			double sum_2 = rv[row_2] - totalContribution_2;
     1956             
     1957             			sum_2 += xv[row_2] * currentDiagonal_2;
     1958             			xv[row_2] = sum_2 / currentDiagonal_2;
     1959             		}
     1960             
     1961             		#pragma omp single 
     1962             		if (maxLevelSize < tdgLevelSize) {
     1963             			local_int_t i = maxLevelSize;
     1964             
     1965             			local_int_t row = A.tdg[l][i];
     1966             			const double * const currentValues = A.matrixValues[row];
     1967             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     1968             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     1969             			const double currentDiagonal = matrixDiagonal[row][0];
     1970             			svfloat64_t contribs = svdup_f64(0.0);
     1971             
     1972             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     1973             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     1974             				
     1975             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     1976             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     1977             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     1978             
     1979             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     1980             			}
     1981             
     1982             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     1983             			double sum = rv[row] - totalContribution;
     1984             
     1985             			sum += xv[row] * currentDiagonal;
     1986             			xv[row] = sum / currentDiagonal;
     1987             		}
     1988             #ifndef HPCG_NO_OPENMP
     1989             	}
     1990             #endif
     1991             	}
     1992             
     1993             	/*
     1994             	 * BACKWARD SWEEP
     1995             	 */
     1996             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     1997             		local_int_t tdgLevelSize = A.tdg[l].size();
     1998             		local_int_t maxLevelSize = 2*(tdgLevelSize / 2);
     1999             
     2000             #ifndef HPCG_NO_OPENMP
     2001             #pragma omp parallel 
     2002             	{
     2003             		#pragma omp single nowait 
     2004             		{
     2005             #endif
     2006             		if (tdgLevelSize > maxLevelSize) {
     2007             			local_int_t i = maxLevelSize-1;
     2008             
     2009             			local_int_t row = A.tdg[l][i];
     2010             			const double * const currentValues = A.matrixValues[row];
     2011             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2012             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2013             			const double currentDiagonal = matrixDiagonal[row][0];
     2014             			svfloat64_t contribs = svdup_f64(0.0);
     2015             
     2016             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2017             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2018             				
     2019             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2020             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2021             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2022             
     2023             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2024             			}
     2025             
     2026             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2027             			double sum = rv[row] - totalContribution;
     2028             
     2029             			sum += xv[row] * currentDiagonal;
     2030             			xv[row] = sum / currentDiagonal;
     2031             		}
     2032             #ifndef HPCG_NO_OPENMP
     2033             		}
     2034             #pragma omp for SCHEDULE(runtime)
     2035             #endif
     2036             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 2 ) {
     2037             			local_int_t row_1 = A.tdg[l][i];
     2038             			local_int_t row_2 = A.tdg[l][i-1];
     2039             			const double * const currentValues_1 = A.matrixValues[row_1];
     2040             			const double * const currentValues_2 = A.matrixValues[row_2];
     2041             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2042             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2043             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2044             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2045             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2046             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2047             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2048             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2049             
     2050             			const int maxNumberOfNonzeros = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2051             							
     2052             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2053             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2054             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2055             				
     2056             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2057             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2058             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2059             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2060             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2061             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2062             
     2063             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2064             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2065             			}
     2066             
     2067             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2068             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2069             			double sum_1 = rv[row_1] - totalContribution_1;
     2070             			double sum_2 = rv[row_2] - totalContribution_2;
     2071             
     2072             			sum_1 += xv[row_1] * currentDiagonal_1;
     2073             			sum_2 += xv[row_2] * currentDiagonal_2;
     2074             			xv[row_1] = sum_1 / currentDiagonal_1;
     2075             			xv[row_2] = sum_2 / currentDiagonal_2;
     2076             		}
     2077             #ifndef HPCG_NO_OPENMP
     2078             	}
     2079             #endif
     2080             	}
     2081             }
     2082             
     2083             inline void SYMGS_VERSION_3(const SparseMatrix& A, double * const& prxv, const double * const& rv) {
     2084             	
     2085             	double **matrixDiagonal = A.matrixDiagonal;
     2086             	double *xv = prxv;
     2087             
     2088             //#pragma statement scache_isolate_way L2=10
     2089             //#pragma statement scache_isolate_assign xv
     2090             #ifndef HPCG_NO_OPENMP
     2091             	#pragma omp parallel
     2092             	{
     2093             		local_int_t numThreads = omp_get_num_threads();
     2094             #endif
     2095             	/*
     2096             	 * FORWARD SWEEP
     2097             	 */
     2098    i        	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     2099             		local_int_t tdgLevelSize = A.tdg[l].size();
     2100             		local_int_t maxLevelSize = 4*(tdgLevelSize / 4);
     2101             
     2102             #ifdef MANUAL_TASK_DISTRIBUTION
     2103             		//at least 8 tasks per thread 
     2104             		local_int_t maxTasksPerThread = MAXI((tdgLevelSize+numThreads-1)/numThreads, 8);
     2105             		local_int_t groupedTasksPerThread = ((maxTasksPerThread+7)/8)*8;
     2106             		local_int_t threadId = omp_get_thread_num();
     2107             		local_int_t minValue = groupedTasksPerThread*threadId;
     2108             		local_int_t maxValue = minValue+groupedTasksPerThread;
     2109             		maxLevelSize = MINI(maxValue, maxLevelSize);
     2110             		tdgLevelSize = MINI(maxValue, tdgLevelSize);
     2111             
     2112             		#pragma fj loop zfill			
     2113             		#pragma loop nounroll
     2114             		for ( local_int_t i = minValue; i < maxLevelSize; i+=4 ) {
     2115             #else
     2116             #ifndef HPCG_NO_OPENMP
     2117             	#pragma fj loop zfill			
     2118             	#pragma loop nounroll
     2119             	#pragma omp for nowait SCHEDULE(runtime)
     2120             #endif
     2121             		for ( local_int_t i = 0; i < maxLevelSize; i+=4 ) {
     2122             #endif
     2123             			local_int_t row_1 = A.tdg[l][i];
     2124             			local_int_t row_2 = A.tdg[l][i+1];
     2125             			local_int_t row_3 = A.tdg[l][i+2];
     2126             			local_int_t row_4 = A.tdg[l][i+3];
     2127             			const double * const currentValues_1 = A.matrixValues[row_1];
     2128             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2129             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2130             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2131             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2132             
     2133             			const double * const currentValues_2 = A.matrixValues[row_2];
     2134             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2135             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2136             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2137             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2138             
     2139             			const double * const currentValues_3 = A.matrixValues[row_3];
     2140             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2141             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2142             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2143             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2144             
     2145             			const double * const currentValues_4 = A.matrixValues[row_4];
     2146             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2147             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2148             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2149             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2150             
     2151             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     2152             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);
     2153             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);
     2154             
     2155          s  			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2156          s  				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2157             				
     2158          s  				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2159          s  				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2160          s  				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2161             
     2162          s  				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2163             
     2164          s  				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2165          s  				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2166          s  				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2167          s  				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2168             
     2169          s  				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2170             
     2171          s  				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2172          s  				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2173          s  				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2174          s  				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2175             
     2176          s  				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2177             
     2178          s  				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2179          s  				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2180          s  				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2181          s  				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2182             
     2183          s  				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2184          s  			}
     2185             
     2186             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2187             			double sum_1 = rv[row_1] - totalContribution_1;
     2188             
     2189             			sum_1 += xv[row_1] * currentDiagonal_1;
     2190             			xv[row_1] = sum_1 / currentDiagonal_1;
     2191             
     2192             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2193             			double sum_2 = rv[row_2] - totalContribution_2;
     2194             
     2195             			sum_2 += xv[row_2] * currentDiagonal_2;
     2196             			xv[row_2] = sum_2 / currentDiagonal_2;
     2197             
     2198             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2199             			double sum_3 = rv[row_3] - totalContribution_3;
     2200             
     2201             			sum_3 += xv[row_3] * currentDiagonal_3;
     2202             			xv[row_3] = sum_3 / currentDiagonal_3;
     2203             
     2204             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2205             			double sum_4 = rv[row_4] - totalContribution_4;
     2206             
     2207             			sum_4 += xv[row_4] * currentDiagonal_4;
     2208             			xv[row_4] = sum_4 / currentDiagonal_4;
     2209             		}
     2210             #ifdef MANUAL_TASK_DISTRIBUTION
     2211             		
     2212             #endif
     2213             //#pragma omp single
     2214             		if (maxLevelSize < tdgLevelSize) {
     2215             /************
     2216             #ifndef HPCG_NO_OPENMP
     2217             //#pragma loop nounroll
     2218             #pragma omp for SCHEDULE(runtime)
     2219             #endif
     2220             		for ( local_int_t i = maxLevelSize; i < tdgLevelSize; i++ ) {
     2221             
     2222             			local_int_t row = A.tdg[l][i];
     2223             			const double * const currentValues = A.matrixValues[row];
     2224             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2225             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2226             			const double currentDiagonal = matrixDiagonal[row][0];
     2227             			svfloat64_t contribs = svdup_f64(0.0);
     2228             
     2229             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2230             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2231             				
     2232             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2233             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2234             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2235             
     2236             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2237             			}
     2238             
     2239             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2240             			double sum = rv[row] - totalContribution;
     2241             
     2242             			sum += xv[row] * currentDiagonal;
     2243             			xv[row] = sum / currentDiagonal;
     2244             		}
     2245             *******/
     2246             		#pragma omp sections nowait
     2247   p         		{
     2248             			#pragma fj loop zfill			
     2249   p         			#pragma omp section 
     2250   p         			{
     2251   p         				local_int_t i = maxLevelSize;
     2252   p         				local_int_t row = A.tdg[l][i];
     2253   p         				const double * const currentValues = A.matrixValues[row];
     2254   p         				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2255   p         				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2256   p         				const double currentDiagonal = matrixDiagonal[row][0];
     2257   p         				svfloat64_t contribs = svdup_f64(0.0);
     2258             
     2259   p      s  				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2260   p      s  					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2261             					
     2262   p      s  					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2263   p      s  					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2264   p      s  					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2265             
     2266   p      s  					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2267   p      s  				}
     2268             
     2269   p         				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2270   p         				double sum = rv[row] - totalContribution;
     2271             
     2272   p         				sum += xv[row] * currentDiagonal;
     2273   p         				xv[row] = sum / currentDiagonal;
     2274   p         			}
     2275             			#pragma fj loop zfill			
     2276   p         			#pragma omp section 
     2277   p         			{
     2278   p         				local_int_t i = maxLevelSize + 1;
     2279   p         				if (i < tdgLevelSize) {
     2280   p         				local_int_t row = A.tdg[l][i];
     2281   p         				const double * const currentValues = A.matrixValues[row];
     2282   p         				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2283   p         				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2284   p         				const double currentDiagonal = matrixDiagonal[row][0];
     2285   p         				svfloat64_t contribs = svdup_f64(0.0);
     2286             
     2287   p      s  				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2288   p      s  					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2289             					
     2290   p      s  					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2291   p      s  					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2292   p      s  					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2293             
     2294   p      s  					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2295   p      s  				}
     2296             
     2297   p         				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2298   p         				double sum = rv[row] - totalContribution;
     2299             
     2300   p         				sum += xv[row] * currentDiagonal;
     2301   p         				xv[row] = sum / currentDiagonal;
     2302   p         				}
     2303   p         			}
     2304             			#pragma fj loop zfill			
     2305   p         			#pragma omp section 
     2306   p         			{
     2307   p         				local_int_t i = maxLevelSize + 2;
     2308   p         				if (i < tdgLevelSize) {
     2309   p         				local_int_t row = A.tdg[l][i];
     2310   p         				const double * const currentValues = A.matrixValues[row];
     2311   p         				const local_int_t * const currentColIndices = A.mtxIndL[row];
     2312   p         				const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2313   p         				const double currentDiagonal = matrixDiagonal[row][0];
     2314   p         				svfloat64_t contribs = svdup_f64(0.0);
     2315             
     2316   p      s  				for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2317   p      s  					svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2318             					
     2319   p      s  					svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2320   p      s  					svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2321   p      s  					svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2322             
     2323   p      s  					contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2324   p      s  				}
     2325             
     2326   p         				double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2327   p         				double sum = rv[row] - totalContribution;
     2328             
     2329   p         				sum += xv[row] * currentDiagonal;
     2330   p         				xv[row] = sum / currentDiagonal;
     2331   p         				}
     2332   p         			}
     2333   p         		}
     2334             
     2335             /***********/
     2336             #ifndef HPCG_NO_OPENMP
     2337             	}
     2338             	#pragma omp barrier
     2339             #endif
     2340             #ifdef MANUAL_TASK_DISTRIBUTION
     2341             
     2342             #endif
     2343    i        	}
     2344             
     2345             	/*
     2346             	 * BACKWARD SWEEP
     2347             	 */
     2348    i        	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     2349             		local_int_t tdgLevelSize = A.tdg[l].size();
     2350             		local_int_t maxLevelSize = 4*(tdgLevelSize / 4);
     2351             
     2352             #ifndef HPCG_NO_OPENMP
     2353             		//#pragma omp single nowait 
     2354             		//{
     2355             		#pragma fj loop zfill			
     2356             		#pragma loop nounroll
     2357             		#pragma omp for nowait SCHEDULE(runtime)
     2358             #endif
     2359   p         		for ( local_int_t i = tdgLevelSize-1; i >= maxLevelSize; i-- ) {
     2360             
     2361   p         			local_int_t row = A.tdg[l][i];
     2362   p         			const double * const currentValues = A.matrixValues[row];
     2363   p         			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2364   p         			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2365   p         			const double currentDiagonal = matrixDiagonal[row][0];
     2366   p         			svfloat64_t contribs = svdup_f64(0.0);
     2367             
     2368   p      s  			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2369   p      s  				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2370             				
     2371   p      s  				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2372   p      s  				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2373   p      s  				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2374             
     2375   p      s  				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2376   p      s  			}
     2377             
     2378   p         			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2379   p         			double sum = rv[row] - totalContribution;
     2380             
     2381   p         			sum += xv[row] * currentDiagonal;
     2382   p         			xv[row] = sum / currentDiagonal;
     2383   p         		}
     2384             #ifndef HPCG_NO_OPENMP
     2385             		//}
     2386             #pragma fj loop zfill			
     2387             #pragma loop nounroll
     2388             #pragma omp for SCHEDULE(runtime)
     2389             #endif
     2390   p         		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 4 ) {
     2391   p         			local_int_t row_1 = A.tdg[l][i];
     2392   p         			local_int_t row_2 = A.tdg[l][i-1];
     2393   p         			local_int_t row_3 = A.tdg[l][i-2];
     2394   p         			local_int_t row_4 = A.tdg[l][i-3];
     2395   p         			const double * const currentValues_1 = A.matrixValues[row_1];
     2396   p         			const double * const currentValues_2 = A.matrixValues[row_2];
     2397   p         			const double * const currentValues_3 = A.matrixValues[row_3];
     2398   p         			const double * const currentValues_4 = A.matrixValues[row_4];
     2399   p         			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2400   p         			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2401   p         			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2402   p         			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2403   p         			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2404   p         			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2405   p         			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2406   p         			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2407   p         			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2408   p         			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2409   p         			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2410   p         			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2411   p         			svfloat64_t contribs_1 = svdup_f64(0.0);
     2412   p         			svfloat64_t contribs_2 = svdup_f64(0.0);
     2413   p         			svfloat64_t contribs_3 = svdup_f64(0.0);
     2414   p         			svfloat64_t contribs_4 = svdup_f64(0.0);
     2415             
     2416   p         			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2417   p         			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);				
     2418   p         			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);				
     2419             							
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SPILLS :
                       <<<      GENERAL   : SPILL 0  FILL 1
                       <<<      SIMD&FP   : SPILL 0  FILL 0
                       <<<      SCALABLE  : SPILL 0  FILL 0
                       <<<      PREDICATE : SPILL 0  FILL 0
                       <<< Loop-information  End >>>
     2420   p      s  			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2421   p      s  				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2422   p      s  				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2423   p      s  				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2424   p      s  				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2425             				
     2426   p      s  				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2427   p      s  				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2428   p      s  				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2429   p      s  				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2430   p      s  				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2431   p      s  				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2432   p      s  				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2433   p      s  				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2434   p      s  				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2435   p      s  				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2436   p      s  				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2437   p      s  				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2438             
     2439   p      s  				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2440   p      s  				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2441   p      s  				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2442   p      s  				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2443   p      s  			}
     2444             
     2445   p         			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2446   p         			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2447   p         			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2448   p         			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2449   p         			double sum_1 = rv[row_1] - totalContribution_1;
     2450   p         			double sum_2 = rv[row_2] - totalContribution_2;
     2451   p         			double sum_3 = rv[row_3] - totalContribution_3;
     2452   p         			double sum_4 = rv[row_4] - totalContribution_4;
     2453             
     2454   p         			sum_1 += xv[row_1] * currentDiagonal_1;
     2455   p         			sum_2 += xv[row_2] * currentDiagonal_2;
     2456   p         			sum_3 += xv[row_3] * currentDiagonal_3;
     2457   p         			sum_4 += xv[row_4] * currentDiagonal_4;
     2458   p         			xv[row_1] = sum_1 / currentDiagonal_1;
     2459   p         			xv[row_2] = sum_2 / currentDiagonal_2;
     2460   p         			xv[row_3] = sum_3 / currentDiagonal_3;
     2461   p         			xv[row_4] = sum_4 / currentDiagonal_4;
     2462   p         		}
     2463             #ifndef HPCG_NO_OPENMP
     2464             	}
     2465             #endif
     2466             	}
     2467             
     2468             //#pragma statement end_scache_isolate_assign
     2469             //#pragma statement end_scache_isolate_way	
     2470             }
     2471             /////////////
     2472             inline void SYMGS_VERSION_4(const SparseMatrix& A, double * const& xv, const double * const& rv) {
     2473             	
     2474             	double **matrixDiagonal = A.matrixDiagonal;
     2475             
     2476             	/*
     2477             	 * FORWARD SWEEP
     2478             	 */
     2479             	for ( local_int_t l = 0; l < A.tdg.size(); l++ ) {
     2480             		local_int_t tdgLevelSize = A.tdg[l].size();
     2481             		local_int_t maxLevelSize = 6*(tdgLevelSize / 6);
     2482             
     2483             #ifndef HPCG_NO_OPENMP
     2484             	#pragma omp parallel
     2485             	{
     2486             	#pragma omp for nowait SCHEDULE(runtime)
     2487             #endif
     2488             		for ( local_int_t i = 0; i < maxLevelSize; i+=6 ) {
     2489             			local_int_t row_1 = A.tdg[l][i];
     2490             			local_int_t row_2 = A.tdg[l][i+1];
     2491             			local_int_t row_3 = A.tdg[l][i+2];
     2492             			local_int_t row_4 = A.tdg[l][i+3];
     2493             			local_int_t row_5 = A.tdg[l][i+4];
     2494             			local_int_t row_6 = A.tdg[l][i+5];
     2495             			const double * const currentValues_1 = A.matrixValues[row_1];
     2496             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2497             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2498             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2499             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2500             
     2501             			const double * const currentValues_2 = A.matrixValues[row_2];
     2502             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2503             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2504             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2505             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2506             
     2507             			const double * const currentValues_3 = A.matrixValues[row_3];
     2508             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2509             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2510             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2511             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2512             
     2513             			const double * const currentValues_4 = A.matrixValues[row_4];
     2514             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2515             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2516             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2517             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2518             
     2519             			const double * const currentValues_5 = A.matrixValues[row_5];
     2520             			const local_int_t * const currentColIndices_5 = A.mtxIndL[row_5];
     2521             			const int currentNumberOfNonzeros_5 = A.nonzerosInRow[row_5];
     2522             			const double currentDiagonal_5 = matrixDiagonal[row_5][0];
     2523             			svfloat64_t contribs_5 = svdup_f64(0.0);
     2524             
     2525             			const double * const currentValues_6 = A.matrixValues[row_6];
     2526             			const local_int_t * const currentColIndices_6 = A.mtxIndL[row_6];
     2527             			const int currentNumberOfNonzeros_6 = A.nonzerosInRow[row_6];
     2528             			const double currentDiagonal_6 = matrixDiagonal[row_6][0];
     2529             			svfloat64_t contribs_6 = svdup_f64(0.0);
     2530             
     2531             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);
     2532             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);
     2533             			const int maxNumberOfNonzeros3 = std::max(currentNumberOfNonzeros_5, currentNumberOfNonzeros_6);
     2534             			const int maxNumberOfNonzeros4 = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);
     2535             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros4, maxNumberOfNonzeros3);
     2536             
     2537             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2538             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);		
     2539             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2540             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2541             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2542             
     2543             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2544             
     2545             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2546             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2547             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2548             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2549             
     2550             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2551             
     2552             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2553             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2554             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2555             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2556             
     2557             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2558             
     2559             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2560             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2561             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2562             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2563             
     2564             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2565             
     2566             				svbool_t pg_5 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_5);
     2567             				svfloat64_t mtxValues_5 = svld1_f64(pg_5, &currentValues_5[j]);
     2568             				svuint64_t indices_5 = svld1sw_u64(pg_5, &currentColIndices_5[j]);
     2569             				svfloat64_t xvv_5 = svld1_gather_u64index_f64(pg_5, xv, indices_5);
     2570             
     2571             				contribs_5 = svmla_f64_m(pg_5, contribs_5, xvv_5, mtxValues_5);
     2572             
     2573             				svbool_t pg_6 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_6);
     2574             				svfloat64_t mtxValues_6 = svld1_f64(pg_6, &currentValues_6[j]);
     2575             				svuint64_t indices_6 = svld1sw_u64(pg_6, &currentColIndices_6[j]);
     2576             				svfloat64_t xvv_6 = svld1_gather_u64index_f64(pg_6, xv, indices_6);
     2577             
     2578             				contribs_6 = svmla_f64_m(pg_6, contribs_6, xvv_6, mtxValues_6);
     2579             			}
     2580             
     2581             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2582             			double sum_1 = rv[row_1] - totalContribution_1;
     2583             
     2584             			sum_1 += xv[row_1] * currentDiagonal_1;
     2585             			xv[row_1] = sum_1 / currentDiagonal_1;
     2586             
     2587             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2588             			double sum_2 = rv[row_2] - totalContribution_2;
     2589             
     2590             			sum_2 += xv[row_2] * currentDiagonal_2;
     2591             			xv[row_2] = sum_2 / currentDiagonal_2;
     2592             
     2593             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2594             			double sum_3 = rv[row_3] - totalContribution_3;
     2595             
     2596             			sum_3 += xv[row_3] * currentDiagonal_3;
     2597             			xv[row_3] = sum_3 / currentDiagonal_3;
     2598             
     2599             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2600             			double sum_4 = rv[row_4] - totalContribution_4;
     2601             
     2602             			sum_4 += xv[row_4] * currentDiagonal_4;
     2603             			xv[row_4] = sum_4 / currentDiagonal_4;
     2604             
     2605             			double totalContribution_5 = svaddv_f64(svptrue_b64(), contribs_5);
     2606             			double sum_5 = rv[row_5] - totalContribution_5;
     2607             
     2608             			sum_5 += xv[row_5] * currentDiagonal_5;
     2609             			xv[row_5] = sum_5 / currentDiagonal_5;
     2610             
     2611             			double totalContribution_6 = svaddv_f64(svptrue_b64(), contribs_6);
     2612             			double sum_6 = rv[row_6] - totalContribution_6;
     2613             
     2614             			sum_6 += xv[row_6] * currentDiagonal_6;
     2615             			xv[row_6] = sum_6 / currentDiagonal_6;
     2616             		}
     2617             
     2618             //#pragma omp single
     2619             		if (maxLevelSize < tdgLevelSize) {
     2620             #ifndef HPCG_NO_OPENMP
     2621             #pragma omp for SCHEDULE(runtime)
     2622             #endif
     2623             		for ( local_int_t i = maxLevelSize; i < tdgLevelSize; i++ ) {
     2624             
     2625             			local_int_t row = A.tdg[l][i];
     2626             			const double * const currentValues = A.matrixValues[row];
     2627             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2628             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2629             			const double currentDiagonal = matrixDiagonal[row][0];
     2630             			svfloat64_t contribs = svdup_f64(0.0);
     2631             
     2632             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2633             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2634             				
     2635             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2636             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2637             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2638             
     2639             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2640             			}
     2641             
     2642             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2643             			double sum = rv[row] - totalContribution;
     2644             
     2645             			sum += xv[row] * currentDiagonal;
     2646             			xv[row] = sum / currentDiagonal;
     2647             		}
     2648             		}
     2649             #ifndef HPCG_NO_OPENMP
     2650             	}
     2651             #endif
     2652             	}
     2653             
     2654             	/*
     2655             	 * BACKWARD SWEEP
     2656             	 */
     2657             	for ( local_int_t l = A.tdg.size()-1; l >= 0; l-- ) {
     2658             		local_int_t tdgLevelSize = A.tdg[l].size();
     2659             		local_int_t maxLevelSize = 6*(tdgLevelSize / 6);
     2660             
     2661             #ifndef HPCG_NO_OPENMP
     2662             #pragma omp parallel 
     2663             	{
     2664             		//#pragma omp single nowait 
     2665             		//{
     2666             		#pragma omp for nowait SCHEDULE(runtime)
     2667             #endif
     2668             		for ( local_int_t i = tdgLevelSize-1; i >= maxLevelSize; i-- ) {
     2669             
     2670             			local_int_t row = A.tdg[l][i];
     2671             			const double * const currentValues = A.matrixValues[row];
     2672             			const local_int_t * const currentColIndices = A.mtxIndL[row];
     2673             			const int currentNumberOfNonzeros = A.nonzerosInRow[row];
     2674             			const double currentDiagonal = matrixDiagonal[row][0];
     2675             			svfloat64_t contribs = svdup_f64(0.0);
     2676             
     2677             			for ( local_int_t j = 0; j < currentNumberOfNonzeros; j += svcntd()) {
     2678             				svbool_t pg = svwhilelt_b64_u64(j, currentNumberOfNonzeros);
     2679             				
     2680             				svfloat64_t mtxValues = svld1_f64(pg, &currentValues[j]);
     2681             				svuint64_t indices = svld1sw_u64(pg, &currentColIndices[j]);
     2682             				svfloat64_t xvv = svld1_gather_u64index_f64(pg, xv, indices);
     2683             
     2684             				contribs = svmla_f64_m(pg, contribs, xvv, mtxValues);
     2685             			}
     2686             
     2687             			double totalContribution = svaddv_f64(svptrue_b64(), contribs);
     2688             			double sum = rv[row] - totalContribution;
     2689             
     2690             			sum += xv[row] * currentDiagonal;
     2691             			xv[row] = sum / currentDiagonal;
     2692             		}
     2693             #ifndef HPCG_NO_OPENMP
     2694             		//}
     2695             #pragma omp for SCHEDULE(runtime)
     2696             #endif
     2697             		for ( local_int_t i = maxLevelSize-1; i >= 0; i-= 6 ) {
     2698             			local_int_t row_1 = A.tdg[l][i];
     2699             			local_int_t row_2 = A.tdg[l][i-1];
     2700             			local_int_t row_3 = A.tdg[l][i-2];
     2701             			local_int_t row_4 = A.tdg[l][i-3];
     2702             			local_int_t row_5 = A.tdg[l][i-4];
     2703             			local_int_t row_6 = A.tdg[l][i-5];
     2704             			const double * const currentValues_1 = A.matrixValues[row_1];
     2705             			const double * const currentValues_2 = A.matrixValues[row_2];
     2706             			const double * const currentValues_3 = A.matrixValues[row_3];
     2707             			const double * const currentValues_4 = A.matrixValues[row_4];
     2708             			const double * const currentValues_5 = A.matrixValues[row_5];
     2709             			const double * const currentValues_6 = A.matrixValues[row_6];
     2710             			const local_int_t * const currentColIndices_1 = A.mtxIndL[row_1];
     2711             			const local_int_t * const currentColIndices_2 = A.mtxIndL[row_2];
     2712             			const local_int_t * const currentColIndices_3 = A.mtxIndL[row_3];
     2713             			const local_int_t * const currentColIndices_4 = A.mtxIndL[row_4];
     2714             			const local_int_t * const currentColIndices_5 = A.mtxIndL[row_5];
     2715             			const local_int_t * const currentColIndices_6 = A.mtxIndL[row_6];
     2716             			const int currentNumberOfNonzeros_1 = A.nonzerosInRow[row_1];
     2717             			const int currentNumberOfNonzeros_2 = A.nonzerosInRow[row_2];
     2718             			const int currentNumberOfNonzeros_3 = A.nonzerosInRow[row_3];
     2719             			const int currentNumberOfNonzeros_4 = A.nonzerosInRow[row_4];
     2720             			const int currentNumberOfNonzeros_5 = A.nonzerosInRow[row_5];
     2721             			const int currentNumberOfNonzeros_6 = A.nonzerosInRow[row_6];
     2722             			const double currentDiagonal_1 = matrixDiagonal[row_1][0];
     2723             			const double currentDiagonal_2 = matrixDiagonal[row_2][0];
     2724             			const double currentDiagonal_3 = matrixDiagonal[row_3][0];
     2725             			const double currentDiagonal_4 = matrixDiagonal[row_4][0];
     2726             			const double currentDiagonal_5 = matrixDiagonal[row_5][0];
     2727             			const double currentDiagonal_6 = matrixDiagonal[row_6][0];
     2728             			svfloat64_t contribs_1 = svdup_f64(0.0);
     2729             			svfloat64_t contribs_2 = svdup_f64(0.0);
     2730             			svfloat64_t contribs_3 = svdup_f64(0.0);
     2731             			svfloat64_t contribs_4 = svdup_f64(0.0);
     2732             			svfloat64_t contribs_5 = svdup_f64(0.0);
     2733             			svfloat64_t contribs_6 = svdup_f64(0.0);
     2734             
     2735             			const int maxNumberOfNonzeros1 = std::max(currentNumberOfNonzeros_1, currentNumberOfNonzeros_2);				
     2736             			const int maxNumberOfNonzeros2 = std::max(currentNumberOfNonzeros_3, currentNumberOfNonzeros_4);				
     2737             			const int maxNumberOfNonzeros3 = std::max(currentNumberOfNonzeros_5, currentNumberOfNonzeros_6);				
     2738             			const int maxNumberOfNonzeros4 = std::max(maxNumberOfNonzeros1, maxNumberOfNonzeros2);				
     2739             			const int maxNumberOfNonzeros = std::max(maxNumberOfNonzeros3, maxNumberOfNonzeros4);				
     2740             							
     2741             			for ( local_int_t j = 0; j < maxNumberOfNonzeros; j += svcntd()) {
     2742             				svbool_t pg_1 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_1);
     2743             				svbool_t pg_2 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_2);
     2744             				svbool_t pg_3 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_3);
     2745             				svbool_t pg_4 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_4);
     2746             				svbool_t pg_5 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_5);
     2747             				svbool_t pg_6 = svwhilelt_b64_u64(j, currentNumberOfNonzeros_6);
     2748             				
     2749             				svfloat64_t mtxValues_1 = svld1_f64(pg_1, &currentValues_1[j]);
     2750             				svfloat64_t mtxValues_2 = svld1_f64(pg_2, &currentValues_2[j]);
     2751             				svfloat64_t mtxValues_3 = svld1_f64(pg_3, &currentValues_3[j]);
     2752             				svfloat64_t mtxValues_4 = svld1_f64(pg_4, &currentValues_4[j]);
     2753             				svfloat64_t mtxValues_5 = svld1_f64(pg_5, &currentValues_5[j]);
     2754             				svfloat64_t mtxValues_6 = svld1_f64(pg_6, &currentValues_6[j]);
     2755             				svuint64_t indices_1 = svld1sw_u64(pg_1, &currentColIndices_1[j]);
     2756             				svuint64_t indices_2 = svld1sw_u64(pg_2, &currentColIndices_2[j]);
     2757             				svuint64_t indices_3 = svld1sw_u64(pg_3, &currentColIndices_3[j]);
     2758             				svuint64_t indices_4 = svld1sw_u64(pg_4, &currentColIndices_4[j]);
     2759             				svuint64_t indices_5 = svld1sw_u64(pg_5, &currentColIndices_5[j]);
     2760             				svuint64_t indices_6 = svld1sw_u64(pg_6, &currentColIndices_6[j]);
     2761             				svfloat64_t xvv_1 = svld1_gather_u64index_f64(pg_1, xv, indices_1);
     2762             				svfloat64_t xvv_2 = svld1_gather_u64index_f64(pg_2, xv, indices_2);
     2763             				svfloat64_t xvv_3 = svld1_gather_u64index_f64(pg_3, xv, indices_3);
     2764             				svfloat64_t xvv_4 = svld1_gather_u64index_f64(pg_4, xv, indices_4);
     2765             				svfloat64_t xvv_5 = svld1_gather_u64index_f64(pg_5, xv, indices_5);
     2766             				svfloat64_t xvv_6 = svld1_gather_u64index_f64(pg_6, xv, indices_6);
     2767             
     2768             				contribs_1 = svmla_f64_m(pg_1, contribs_1, xvv_1, mtxValues_1);
     2769             				contribs_2 = svmla_f64_m(pg_2, contribs_2, xvv_2, mtxValues_2);
     2770             				contribs_3 = svmla_f64_m(pg_3, contribs_3, xvv_3, mtxValues_3);
     2771             				contribs_4 = svmla_f64_m(pg_4, contribs_4, xvv_4, mtxValues_4);
     2772             				contribs_5 = svmla_f64_m(pg_5, contribs_5, xvv_5, mtxValues_5);
     2773             				contribs_6 = svmla_f64_m(pg_6, contribs_6, xvv_6, mtxValues_6);
     2774             			}
     2775             
     2776             			double totalContribution_1 = svaddv_f64(svptrue_b64(), contribs_1);
     2777             			double totalContribution_2 = svaddv_f64(svptrue_b64(), contribs_2);
     2778             			double totalContribution_3 = svaddv_f64(svptrue_b64(), contribs_3);
     2779             			double totalContribution_4 = svaddv_f64(svptrue_b64(), contribs_4);
     2780             			double totalContribution_5 = svaddv_f64(svptrue_b64(), contribs_5);
     2781             			double totalContribution_6 = svaddv_f64(svptrue_b64(), contribs_6);
     2782             			double sum_1 = rv[row_1] - totalContribution_1;
     2783             			double sum_2 = rv[row_2] - totalContribution_2;
     2784             			double sum_3 = rv[row_3] - totalContribution_3;
     2785             			double sum_4 = rv[row_4] - totalContribution_4;
     2786             			double sum_5 = rv[row_5] - totalContribution_5;
     2787             			double sum_6 = rv[row_6] - totalContribution_6;
     2788             
     2789             			sum_1 += xv[row_1] * currentDiagonal_1;
     2790             			sum_2 += xv[row_2] * currentDiagonal_2;
     2791             			sum_3 += xv[row_3] * currentDiagonal_3;
     2792             			sum_4 += xv[row_4] * currentDiagonal_4;
     2793             			sum_5 += xv[row_5] * currentDiagonal_5;
     2794             			sum_6 += xv[row_6] * currentDiagonal_6;
     2795             			xv[row_1] = sum_1 / currentDiagonal_1;
     2796             			xv[row_2] = sum_2 / currentDiagonal_2;
     2797             			xv[row_3] = sum_3 / currentDiagonal_3;
     2798             			xv[row_4] = sum_4 / currentDiagonal_4;
     2799             			xv[row_5] = sum_5 / currentDiagonal_5;
     2800             			xv[row_6] = sum_6 / currentDiagonal_6;
     2801             		}
     2802             #ifndef HPCG_NO_OPENMP
     2803             	}
     2804             #endif
     2805             	}
     2806             }
Total prefetch num: 0
Optimization messages
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 301: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 305: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 305: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 306: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 306: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 313: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 313: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 328: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 328: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 329: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 334: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 338: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 338: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 339: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 339: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 346: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 346: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 388: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 397: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 398: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 411: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 434: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 434: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 490: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 490: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 526: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 526: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 551: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 557: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 558: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 572: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 595: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 595: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 651: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 651: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 687: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 687: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1383: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1387: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1387: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1388: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1388: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1395: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1395: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1395: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1397: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1401: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1401: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1402: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1407: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1411: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1411: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1412: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1412: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1419: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1419: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1419: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1421: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1456: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1461: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1461: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1462: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1462: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1469: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1469: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1469: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1471: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1475: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1475: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1476: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1481: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1486: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1486: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1487: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1487: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1494: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1494: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1494: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1496: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1526: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1532: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1533: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1555: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1555: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1555: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1560: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1573: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1573: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1573: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1576: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1584: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 1584: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 1584: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1586: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1595: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1601: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1602: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1624: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1624: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1624: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1624: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 32.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1645: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1645: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1645: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1645: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 96.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1646: Method of calculating sum or product is changed.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1647: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1658: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6004s-i  "../src/ComputeSYMGS.cpp", line 1658: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSYMGS.cpp", line 1658: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSYMGS.cpp", line 1658: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSYMGS.cpp", line 1659: Method of calculating sum or product is changed.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 1708: Inline expansion is applied to the user defined function '_Z20ComputeSYMGS_TDG_SVERK19SparseMatrix_STRUCTRK13Vector_STRUCTRS2_R9TraceData'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2098: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2099: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2099: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2123: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2123: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2124: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2124: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2125: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2125: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2126: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2126: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2151: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiEERKT_S3_S3_'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2152: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiEERKT_S3_S3_'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2153: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiEERKT_S3_S3_'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 2155: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 2155: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2252: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2252: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 2259: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 2259: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2280: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2280: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 2287: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 2287: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2309: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2309: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 2316: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 2316: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2343: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2348: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2349: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2349: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEE4sizeEv'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2361: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2361: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 2368: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 2368: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2391: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2391: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2392: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2392: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2393: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2393: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2394: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorINS0_IiNS_9allocatorIiEEEENS1_IS3_EEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2394: Inline expansion is applied to the user defined function '_ZNKSt3__16vectorIiNS_9allocatorIiEEEixEm'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2416: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiEERKT_S3_S3_'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2417: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiEERKT_S3_S3_'.
  jwd8101o-i  "../src/ComputeSYMGS.cpp", line 2418: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiEERKT_S3_S3_'.
  jwd6142s-i  "../src/ComputeSYMGS.cpp", line 2420: SIMD conversion is not applied to this loop because the iteration count is uncertainty.
  jwd8671o-i  "../src/ComputeSYMGS.cpp", line 2420: This loop cannot be software pipelined because the shape of the loop is not covered by software pipelining.
  jwd8101o-i  "/opt/FJSVstclanga/cp-1.0.21.01/bin/../include/libc++/v371/algorithm", line 2659: Inline expansion is applied to the user defined function '_ZNKSt3__16__lessIiiEclERKiS3_'.
  jwd8101o-i  "/opt/FJSVstclanga/cp-1.0.21.01/bin/../include/libc++/v371/algorithm", line 2667: Inline expansion is applied to the user defined function '_ZNSt3__13maxIiNS_6__lessIiiEEEERKT_S5_S5_T0_'.
Statistics information
  Option information
    Command line options : -c -DHPCG_CONTIGUOUS_ARRAYS -DHPCG_NO_MPI -DENABLE_MG_COUNTERS -DHPCG_USE_SVE -DHPCG_MAN_OPT_DDOT -DDDOT_2_UNROLL -DWAXPBY_AUTO_OPT -HPCG_MAN_OPT_SCHEDULE_ON -DHPCG_MAN_OPT_SPMV_UNROLL -DSPMV_4_UNROLL -Khpctag -Kzfill -I./src -I./src/OOKAMI_OMP_FJ -Kfast -KSVE -Kopenmp -ffast-math -funroll-loops -std=c++11 -ffp-contract=fast -march=armv8.2-a+sve -Kocl -Koptmsg=2 -Nlst=t -I../src -o src/ComputeSYMGS.o
    Effective options    : -g0 -mt -Qy -std=gnu++11 -x- -x=quick -O3 -Knoalias_const
                           -Kalign_loops -Knoarray_declaration_opt -Kassume=noshortloop
                           -Kassume=nomemory_bandwidth -Kassume=notime_saving_compilation
                           -Kcmodel=small -Keval -Keval_noconcurrent
                           -Knoextract_stride_store -Kfast_matmul -Knofenv_access
                           -Kfp_contract -Kfp_relaxed -Kfsimple -Kfz -Khpctag
                           -Kilfunc=procedure -Klargepage -Klib -Kloop_blocking
                           -Kloop_fission -Kloop_nofission_stripmining
                           -Kloop_fission_threshold=50 -Kloop_fusion -Kloop_interchange
                           -Kloop_part_simd -Kloop_perfect_nest -Kloop_noversioning
                           -Klooptype=f -Knomemalias -Kmfunc=1 -Kocl -Komitfp -Kopenmp
                           -Kopenmp_noassume_norecurrence
                           -Kopenmp_nocollapse_except_innermost
                           -Kopenmp_loop_variable=private -Kopenmp_noordered_reduction
                           -Knoopenmp_simd -Knooptlib_string -Koptmsg=2
                           -Knopc_relative_literal_loads -Knoparallel
                           -Kparallel_nofp_precision -Knopreex -Kprefetch_cache_level=all
                           -Kprefetch_noconditional -Kprefetch_noindirect -Kprefetch_noinfer
                           -Kprefetch_sequential=auto -Kprefetch_nostride -Kprefetch_strong
                           -Kprefetch_strong_L2 -Knopreload -Krdconv=1
                           -Kremove_inlinefunction -Knorestp -Ksch_post_ra -Ksch_pre_ra
                           -Ksibling_calls -Ksimd=auto -Ksimd_packed_promotion
                           -Ksimd_reduction_product -Ksimd_reg_size=512
                           -Ksimd_nouncounted_loop -Ksimd_use_multiple_structures
                           -Knostrict_aliasing -Knostriping -KA64FX -KARMV8_2_A -KSVE -Kswp
                           -Kswp_freg_rate=100 -Kswp_ireg_rate=100 -Kswp_preg_rate=100
                           -Kswp_policy=auto -Kunroll -Knounroll_and_jam -Kzfill
                           -Ncancel_overtime_compilation -Nnocoverage -Nexceptions -Nnofjcex
                           -Nfjprof -Nnohook_func -Nnohook_time -Nlibomp -Nline -Nlst=p
                           -Nlst=t -Nquickdbg=noheapchk -Nquickdbg=nosubchk -NRnotrap
                           -Nnoreordered_variable_stack -Nrt_notune -Nsetvalue=noheap
                           -Nsetvalue=nostack -Nsetvalue=noscalar -Nsetvalue=noarray
                           -Nsetvalue=nostruct -Nsrc -Nsta
