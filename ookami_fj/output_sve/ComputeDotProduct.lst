Fujitsu C/C++ Version 4.7.0   Wed Nov  2 15:04:14 2022
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj
  Source file       : ../src/ComputeDotProduct.cpp
(line-no.)(optimize)
        1             /*
        2              *
        3              *  SPDX-License-Identifier: Apache-2.0
        4              *
        5              *  Copyright (C) 2019, Arm Limited and contributors
        6              *
        7              *  Licensed under the Apache License, Version 2.0 (the "License");
        8              *  you may not use this file except in compliance with the License.
        9              *  You may obtain a copy of the License at
       10              *
       11              *      http://www.apache.org/licenses/LICENSE-2.0
       12              *
       13              *  Unless required by applicable law or agreed to in writing, software
       14              *  distributed under the License is distributed on an "AS IS" BASIS,
       15              *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       16              *  See the License for the specific language governing permissions and
       17              *  limitations under the License.
       18              *
       19              */
       20             
       21             //@HEADER
       22             // ***************************************************
       23             //
       24             // HPCG: High Performance Conjugate Gradient Benchmark
       25             //
       26             // Contact:
       27             // Michael A. Heroux ( maherou@sandia.gov)
       28             // Jack Dongarra     (dongarra@eecs.utk.edu)
       29             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       30             //
       31             // ***************************************************
       32             //@HEADER
       33             
       34             /*!
       35              @file ComputeDotProduct.cpp
       36             
       37              HPCG routine
       38              */
       39             
       40             #ifndef HPCG_NO_MPI
       41             #include <mpi.h>
       42             #include "mytimer.hpp"
       43             #endif
       44             #ifndef HPCG_NO_OPENMP
       45             #include <omp.h>
       46             #endif
       47             
       48             #include "ComputeDotProduct.hpp"
       49             #include "ComputeDotProduct_ref.hpp"
       50             #include <cassert>
       51             #ifdef HPCG_USE_DDOT_ARMPL
       52             #include "armpl.h"
       53             #endif
       54             #ifdef HPCG_USE_SVE
       55             #include "arm_sve.h"
       56             #endif
       57             
       58             /*!
       59               Routine to compute the dot product of two vectors.
       60             
       61               This routine calls the reference dot-product implementation by default, but
       62               can be replaced by a custom routine that is optimized and better suited for
       63               the target system.
       64             
       65               @param[in]  n the number of vector elements (on this processor)
       66               @param[in]  x, y the input vectors
       67               @param[out] result a pointer to scalar value, on exit will contain the result.
       68               @param[out] time_allreduce the time it took to perform the communication between processes
       69               @param[out] isOptimized should be set to false if this routine uses the reference implementation (is not optimized); otherwise leave it unchanged
       70             
       71               @return returns 0 upon success and non-zero otherwise
       72             
       73               @see ComputeDotProduct_ref
       74             */
       75             #ifndef HPCG_MAN_OPT_DDOT_INTRINSICS
       76             int ComputeDotProduct(const local_int_t n, const Vector & x, const Vector & y,
       77                 double & result, double & time_allreduce, bool & isOptimized) {
       78             
       79             	assert(x.localLength >= n);
       80             	assert(y.localLength >= n);
       81             
       82             	double *xv = x.values;
       83             	double *yv = y.values;
       84             	double local_result = 0.0;
       85             
       86             #if defined HPCG_USE_SVE && !defined HPCG_MAN_OPT_DDOT
       87             	if ( xv == yv ) {
       88             #ifndef HPCG_NO_OPENMP
       89          s  #pragma omp parallel for reduction(+:local_result)
       90             #endif
       91    i        		for ( local_int_t i = 0; i < n; i += svcntd()) {
       92    i        			svbool_t pg = svwhilelt_b64(i, n);
       93             			svfloat64_t svx = svld1_f64(pg, &xv[i]);
       94             
       95    i                    svfloat64_t svlr = svmul_f64_z(pg, svx, svx);
       96             
       97    i                    local_result += svaddv_f64(svptrue_b64(), svlr);
       98             		}
       99             	} else {
      100             #ifndef HPCG_NO_OPENMP
      101          s  #pragma omp parallel for reduction(+:local_result)
      102             #endif
      103    i        		for ( local_int_t i = 0; i < n; i += svcntd()) {
      104    i        			svbool_t pg = svwhilelt_b64_u64(i, n);
      105             			svfloat64_t svx = svld1_f64(pg, &xv[i]);
      106             			svfloat64_t svy = svld1_f64(pg, &yv[i]);
      107             
      108    i                    svfloat64_t svlr = svmul_f64_z(pg, svx, svy);
      109                         
      110    i                    local_result += svaddv_f64(svptrue_b64(), svlr);
      111             		}
      112             	}
      113             #elif defined HPCG_USE_DDOT_ARMPL
      114             	local_result = cblas_ddot(n, xv, 1, yv, 1);
      115             #elif defined HPCG_MAN_OPT_DDOT
      116             	double local_result0 = 0.0, local_result1=0.0, local_result2=0.0, local_result3=0.0;
      117             	if (yv == xv) {
      118             #ifndef HPCG_NO_OPENMP
      119             #pragma omp parallel for reduction (+:local_result0,local_result1,local_result2,local_result3)
      120             #endif //HPCG_NO_OPENMP
      121             		for ( local_int_t i = 0; i < n; i+=4 ) {
      122                         local_result0 += xv[i+0] * xv[i+0];
      123                         local_result1 += xv[i+1] * xv[i+1];
      124                         local_result2 += xv[i+2] * xv[i+2];
      125                         local_result3 += xv[i+3] * xv[i+3];
      126                     }
      127             		local_result += local_result0+local_result1+local_result2+local_result3;
      128             	}
      129             	else {
      130             #ifndef HPCG_NO_OPENMP
      131             #pragma omp parallel for reduction (+:local_result0,local_result1,local_result2,local_result3)
      132             #endif //HPCG_NO_OPENMP
      133             		for ( local_int_t i = 0; i < n; i+=4 ) {
      134             			local_result0 += xv[i] * yv[i];
      135             			local_result1 += xv[i+1] * yv[i+1];
      136             			local_result2 += xv[i+2] * yv[i+2];
      137             			local_result3 += xv[i+3] * yv[i+3];
      138             		}
      139             		local_result += local_result0+local_result1+local_result2+local_result3;
      140             	}
      141             #else //HPCG_USE_DDOT_ARMPL
      142             	if ( yv == xv ) {
      143             #ifndef HPCG_NO_OPENMP
      144             #pragma omp parallel for reduction (+:local_result)
      145             #endif //HPCG_NO_OPENMP
      146             		for ( local_int_t i = 0; i < n; i++ ) {
      147                         local_result += xv[i] * xv[i];
      148                     }
      149             	} else {
      150             #ifndef HPCG_NO_OPENMP
      151             #pragma omp parallel for reduction (+:local_result)
      152             #endif //HPCG_NO_OPENMP
      153             		for ( local_int_t i = 0; i < n; i++ ) {
      154             			local_result += xv[i] * yv[i];
      155             		}
      156             	}
      157             #endif //HPCG_USE_DDOT_ARMPL
      158             
      159             #ifndef HPCG_NO_MPI
      160             	// Use MPI's reduce function to collect all partial sums
      161             	double t0 = mytimer();
      162             	double global_result = 0.0;
      163             	MPI_Allreduce(&local_result, &global_result, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
      164             	result = global_result;
      165             	time_allreduce += mytimer() - t0;
      166             #else //HPCG_NO_MPI
      167             	time_allreduce += 0.0;
      168             	result = local_result;
      169             #endif //HPCG_NO_MPI
      170             
      171             	return 0;
      172             }
      173             #endif
      174             
      175             //2-way unrolling using intrinsics
      176             #ifdef HPCG_MAN_OPT_DDOT_INTRINSICS
      177             int ComputeDotProduct(const local_int_t n, const Vector & x, const Vector & y,
      178                 double & result, double & time_allreduce, bool & isOptimized) {
      179             
      180             	assert(x.localLength >= n);
      181             	assert(y.localLength >= n);
      182             
      183             	double *xv = x.values;
      184             	double *yv = y.values;
      185             	double local_result=0, local_result0 = 0.0, local_result1 = 0.0, local_result2=0, local_result3 =0;
      186             
      187             	if ( xv == yv ) {
      188             		uint64_t vz = svcntd();
      189             #ifndef HPCG_NO_OPENMP
      190             #pragma omp parallel for reduction(+:local_result0, local_result1, local_result2, local_result3)
      191             #endif
      192             		for ( local_int_t i = 0; i < n; i += 4*vz) {
      193             			svbool_t pg0 = svwhilelt_b64(i, n);
      194             			local_int_t ij=i+vz,i2j=i+2*vz,i3j=i+3*vz;
      195             			svbool_t pg1 = svwhilelt_b64(ij, n);
      196             			svbool_t pg2 = svwhilelt_b64(i2j, n);
      197             			svbool_t pg3 = svwhilelt_b64(i3j, n);
      198             			svfloat64_t svx0 = svld1_f64(pg0, &xv[i]);
      199             			svfloat64_t svx1 = svld1_f64(pg1, &xv[ij]);
      200             			svfloat64_t svx2 = svld1_f64(pg2, &xv[i2j]);
      201             			svfloat64_t svx3 = svld1_f64(pg3, &xv[i3j]);
      202             
      203                         svfloat64_t svlr0 = svmul_f64_z(pg0, svx0, svx0);
      204                         svfloat64_t svlr1 = svmul_f64_z(pg1, svx1, svx1);
      205                         svfloat64_t svlr2 = svmul_f64_z(pg2, svx2, svx2);
      206                         svfloat64_t svlr3 = svmul_f64_z(pg3, svx3, svx3);
      207             
      208                         local_result0 += svaddv_f64(svptrue_b64(), svlr0);
      209                         local_result1 += svaddv_f64(svptrue_b64(), svlr1);
      210                         local_result2 += svaddv_f64(svptrue_b64(), svlr2);
      211                         local_result3 += svaddv_f64(svptrue_b64(), svlr3);
      212             		}
      213             		local_result = local_result0+local_result1+local_result2+local_result3;
      214             	} else {
      215             #ifndef HPCG_NO_OPENMP
      216             #pragma omp parallel for reduction(+:local_result0, local_result1, local_result2, local_result3)
      217             #endif
      218             		for ( local_int_t i = 0; i < n; i += 4*svcntd()) {
      219             			svbool_t pg0 = svwhilelt_b64_u64(i, n);
      220             			svbool_t pg1 = svwhilelt_b64_u64(i+svcntd(), n);
      221             			svbool_t pg2 = svwhilelt_b64_u64(i+2*svcntd(), n);
      222             			svbool_t pg3 = svwhilelt_b64_u64(i+3*svcntd(), n);
      223             			svfloat64_t svx0 = svld1_f64(pg0, &xv[i]);
      224             			svfloat64_t svx1 = svld1_f64(pg1, &xv[i+svcntd()]);
      225             			svfloat64_t svx2 = svld1_f64(pg2, &xv[i+2*svcntd()]);
      226             			svfloat64_t svx3 = svld1_f64(pg3, &xv[i+3*svcntd()]);
      227             			svfloat64_t svy0 = svld1_f64(pg0, &yv[i]);
      228             			svfloat64_t svy1 = svld1_f64(pg1, &yv[i+svcntd()]);
      229             			svfloat64_t svy2 = svld1_f64(pg2, &yv[i+2*svcntd()]);
      230             			svfloat64_t svy3 = svld1_f64(pg3, &yv[i+3*svcntd()]);
      231             
      232                         svfloat64_t svlr0 = svmul_f64_z(pg0, svx0, svy0);
      233                         svfloat64_t svlr1 = svmul_f64_z(pg1, svx1, svy1);
      234                         svfloat64_t svlr2 = svmul_f64_z(pg2, svx2, svy2);
      235                         svfloat64_t svlr3 = svmul_f64_z(pg3, svx3, svy3);
      236                         
      237                         local_result0 += svaddv_f64(svptrue_b64(), svlr0);
      238                         local_result1 += svaddv_f64(svptrue_b64(), svlr1);
      239                         local_result2 += svaddv_f64(svptrue_b64(), svlr2);
      240                         local_result3 += svaddv_f64(svptrue_b64(), svlr3);
      241             		}
      242             		local_result = local_result0+local_result1+local_result2+local_result3;
      243             	}
      244             
      245             #ifndef HPCG_NO_MPI
      246             	// Use MPI's reduce function to collect all partial sums
      247             	double t0 = mytimer();
      248             	double global_result = 0.0;
      249             	MPI_Allreduce(&local_result, &global_result, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
      250             	result = global_result;
      251             	time_allreduce += mytimer() - t0;
      252             #else //HPCG_NO_MPI
      253             	time_allreduce += 0.0;
      254             	result = local_result;
      255             #endif //HPCG_NO_MPI
      256             
      257             	return 0;
      258             }
      259             #endif //HPCG_MAN_OPT_DDOT_INTRINSICS
