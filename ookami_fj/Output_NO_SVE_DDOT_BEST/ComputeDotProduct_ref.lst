Fujitsu C/C++ Version 4.7.0   Wed Nov  2 15:46:50 2022
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj
  Source file       : ../src/ComputeDotProduct_ref.cpp
(line-no.)(optimize)
        1             
        2             //@HEADER
        3             // ***************************************************
        4             //
        5             // HPCG: High Performance Conjugate Gradient Benchmark
        6             //
        7             // Contact:
        8             // Michael A. Heroux ( maherou@sandia.gov)
        9             // Jack Dongarra     (dongarra@eecs.utk.edu)
       10             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       11             //
       12             // ***************************************************
       13             //@HEADER
       14             
       15             /*!
       16              @file ComputeDotProduct_ref.cpp
       17             
       18              HPCG routine
       19              */
       20             
       21             #ifndef HPCG_NO_MPI
       22             #include <mpi.h>
       23             #include "mytimer.hpp"
       24             #endif
       25             #ifndef HPCG_NO_OPENMP
       26             #include <omp.h>
       27             #endif
       28             #include <cassert>
       29             #include "ComputeDotProduct_ref.hpp"
       30             
       31             /*!
       32               Routine to compute the dot product of two vectors where:
       33             
       34               This is the reference dot-product implementation.  It _CANNOT_ be modified for the
       35               purposes of this benchmark.
       36             
       37               @param[in] n the number of vector elements (on this processor)
       38               @param[in] x, y the input vectors
       39               @param[in] result a pointer to scalar value, on exit will contain result.
       40               @param[out] time_allreduce the time it took to perform the communication between processes
       41             
       42               @return returns 0 upon success and non-zero otherwise
       43             
       44               @see ComputeDotProduct
       45             */
       46             int ComputeDotProduct_ref(const local_int_t n, const Vector & x, const Vector & y,
       47                 double & result, double & time_allreduce) {
       48               assert(x.localLength>=n); // Test vector lengths
       49               assert(y.localLength>=n);
       50             
       51               double local_result = 0.0;
       52               double * xv = x.values;
       53               double * yv = y.values;
       54               if (yv==xv) {
       55             #ifndef HPCG_NO_OPENMP
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: AGNOSTIC; VL: 2 in 128-bit Interleave: 1)
                       <<< Loop-information End >>>
       56          m      #pragma omp parallel for reduction (+:local_result)
       57             #endif
       58                 for (local_int_t i=0; i<n; i++) local_result += xv[i]*xv[i];
       59               } else {
       60             #ifndef HPCG_NO_OPENMP
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: AGNOSTIC; VL: 2 in 128-bit Interleave: 1)
                       <<< Loop-information End >>>
       61          m      #pragma omp parallel for reduction (+:local_result)
       62             #endif
       63                 for (local_int_t i=0; i<n; i++) local_result += xv[i]*yv[i];
       64               }
       65             
       66             #ifndef HPCG_NO_MPI
       67               // Use MPI's reduce function to collect all partial sums
       68               double t0 = mytimer();
       69               double global_result = 0.0;
       70               MPI_Allreduce(&local_result, &global_result, 1, MPI_DOUBLE, MPI_SUM,
       71                   MPI_COMM_WORLD);
       72               result = global_result;
       73               time_allreduce += mytimer() - t0;
       74             #else
       75               time_allreduce += 0.0;
       76               result = local_result;
       77             #endif
       78             
       79               return 0;
       80             }
