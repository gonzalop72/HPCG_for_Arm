### Starting TaskPrologue of job 592418 on f0328 at Wed May 17 12:23:23 CEST 2023
#   SLURM_JOB_NODELIST=f0328
#   SLURM_JOB_NUM_NODES=1
#   SLURM_NTASKS=10
#   SLURM_NPROCS=10
#   SLURM_TASKS_PER_NODE=10
#   SLURM_JOB_CPUS_PER_NODE=72
#   SLURM_EXPORT_ENV=
Running on cores 0-71 with governor powersave
### Finished TaskPrologue
params: --nx=48 --ny=480 --nz=480 --npx=10 --npy=1 --npz=1
(f0328:0,1,2,3,4,5,6,7,8,9)

MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
Abort(127) on node 1 (rank 1 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 1
Abort(127) on node 2 (rank 2 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 2
Abort(127) on node 3 (rank 3 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 3
Abort(127) on node 4 (rank 4 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 4
Abort(127) on node 5 (rank 5 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 5
Abort(127) on node 6 (rank 6 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 6
Abort(127) on node 7 (rank 7 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 7
Abort(127) on node 8 (rank 8 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 8
Abort(127) on node 9 (rank 9 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 9
=== JOB_STATISTICS ===
=== current date     : Wed May 17 12:23:26 CEST 2023
= Job-ID             : 592418 on fritz
= Job-Name           : job_mpi.sh
= Job-Command        : /home/hpc/ihpc/ihpc061h/arm_code/optimized_version/HPCG_for_Arm/test_003_Convergence/test_1/job_mpi.sh
= Initial workdir    : /home/hpc/ihpc/ihpc061h/arm_code/optimized_version/HPCG_for_Arm/test_003_Convergence/test_1
= Queue/Partition    : singlenode
= Slurm account      : ihpc with QOS=normal
= Features           : hwperf
= Requested resources: cpu=72,node=1,billing=72 for 00:30:00
= Elapsed runtime    : 00:00:04
= Total RAM usage    : 0.0 GiB 
= Node list          : f0328
= Subm/Elig/Start/End: 2023-05-17T11:08:31 / 2023-05-17T11:08:31 / 2023-05-17T12:23:22 / 2023-05-17T12:23:26
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           23.1G    52.4G   104.9G        N/A  39,176      500K   1,000K        N/A    
    /home/woody          4.0K   500.0G   750.0G        N/A       1                           N/A    
    /home/vault          0.0K   524.3G  1048.6G        N/A       1      200K     400K        N/A    
    /lustre             12.0K     0.0K     0.0K        N/A       1   20,000      250K        N/A    
======================
