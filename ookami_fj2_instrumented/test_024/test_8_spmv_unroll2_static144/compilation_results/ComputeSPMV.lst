Fujitsu C/C++ Version 4.7.0   Wed Nov 16 15:39:40 2022
Compilation information
  Current directory : /lustre/home/gapinzon/arm_code/HPCG_for_Arm/ookami_fj2_instrumented
  Source file       : ../src/ComputeSPMV.cpp
(line-no.)(optimize)
        1             /*
        2              *
        3              *  SPDX-License-Identifier: Apache-2.0
        4              *
        5              *  Copyright (C) 2019, Arm Limited and contributors
        6              *
        7              *  Licensed under the Apache License, Version 2.0 (the "License");
        8              *  you may not use this file except in compliance with the License.
        9              *  You may obtain a copy of the License at
       10              *
       11              *      http://www.apache.org/licenses/LICENSE-2.0
       12              *
       13              *  Unless required by applicable law or agreed to in writing, software
       14              *  distributed under the License is distributed on an "AS IS" BASIS,
       15              *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
       16              *  See the License for the specific language governing permissions and
       17              *  limitations under the License.
       18              *
       19              */
       20             
       21             
       22             //@HEADER
       23             // ***************************************************
       24             //
       25             // HPCG: High Performance Conjugate Gradient Benchmark
       26             //
       27             // Contact:
       28             // Michael A. Heroux ( maherou@sandia.gov)
       29             // Jack Dongarra     (dongarra@eecs.utk.edu)
       30             // Piotr Luszczek    (luszczek@eecs.utk.edu)
       31             //
       32             // ***************************************************
       33             //@HEADER
       34             
       35             /*!
       36               @file ComputeSPMV.cpp
       37             
       38               HPCG routine
       39               */
       40             
       41             #include "ComputeSPMV.hpp"
       42             #include "ComputeSPMV_ref.hpp"
       43             #include <cassert>
       44             #ifndef HPCG_NO_MPI
       45             #include "ExchangeHalo.hpp"
       46             #endif
       47             #ifdef HPCG_USE_NEON
       48             #include "arm_neon.h"
       49             #endif
       50             #ifdef HPCG_USE_SVE
       51             #include "arm_sve.h"
       52             #endif
       53             #ifdef HPCG_USE_ARMPL_SPMV
       54             #include "armpl_sparse.h"
       55             #endif
       56             
       57             /*!
       58               Routine to compute sparse matrix vector product y = Ax where:
       59             Precondition: First call exchange_externals to get off-processor values of x
       60             
       61             This routine calls the reference SpMV implementation by default, but
       62             can be replaced by a custom, optimized routine suited for
       63             the target system.
       64             
       65             @param[in]  A the known system matrix
       66             @param[in]  x the known vector
       67             @param[out] y the On exit contains the result: Ax.
       68             
       69             @return returns 0 upon success and non-zero otherwise
       70             
       71             @see ComputeSPMV_ref
       72             */
       73             
       74             #ifdef HPCG_MAN_OPT_SCHEDULE_ON
       75             	#define SCHEDULE(T)	schedule(T)
       76             #else
       77             	#define SCHEDULE(T)
       78             #endif
       79             
       80             inline void ComputeSPMV_unroll2(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow);
       81             /*#ifdef SPMV_2_UNROLL
       82             #define ComputeSPMV_manual	ComputeSPMV_unroll2
       83             #elif defined SPMV_4_UNROLL
       84             #define ComputeSPMV_manual	ComputeSPMV_unroll4
       85             inline void ComputeSPMV_unroll4(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow);
       86             #endif
       87             */
       88             
       89             int ComputeSPMV( const SparseMatrix & A, Vector & x, Vector & y) {
       90             
       91             	assert(x.localLength >= A.localNumberOfColumns);
       92             	assert(y.localLength >= A.localNumberOfRows);
       93             
       94             #ifndef HPCG_NO_MPI
       95             	ExchangeHalo(A,x);
       96             #endif
       97             	const double * const xv = x.values;
       98             	double * const yv = y.values;
       99             	const local_int_t nrow = A.localNumberOfRows;
      100             
      101             #if defined(HPCG_USE_NEON) && !defined(HPCG_USE_ARMPL_SPMV)
      102             #ifndef HPCG_NO_OPENMP
      103             #pragma omp parallel for SCHEDULE(runtime)
      104             #endif
      105             	for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
      106             		float64x2_t sum0 = vdupq_n_f64(0.0);
      107             		float64x2_t sum1 = vdupq_n_f64(0.0);
      108             		if ( A.nonzerosInRow[i] == A.nonzerosInRow[i+1] ) {
      109             			local_int_t j = 0;
      110             			for ( j = 0; j < A.nonzerosInRow[i]-1; j+=2 ) {
      111             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      112             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      113             
      114             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      115             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      116             
      117             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      118             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      119             
      120             			}
      121             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      122             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      123             
      124             			if ( j < A.nonzerosInRow[i] ) {
      125             				s0 += A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
      126             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      127             			}
      128             			yv[i  ] = s0;
      129             			yv[i+1] = s1;
      130             		} else if ( A.nonzerosInRow[i] > A.nonzerosInRow[i+1] ) {
      131             			local_int_t j = 0;
      132             			for ( j = 0; j < A.nonzerosInRow[i+1]-1; j+=2 ) {
      133             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      134             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      135             
      136             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      137             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      138             
      139             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      140             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      141             
      142             			}
      143             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      144             			if ( j < A.nonzerosInRow[i+1] ) {
      145             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      146             			}
      147             
      148             			for ( ; j < A.nonzerosInRow[i]-1; j+=2 ) {
      149             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      150             
      151             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      152             
      153             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      154             			}
      155             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      156             			if ( j < A.nonzerosInRow[i] ) {
      157             				s0 += A.matrixValues[i  ][j] * xv[A.mtxIndL[i  ][j]];
      158             			}
      159             			yv[i  ] = s0;
      160             			yv[i+1] = s1;
      161             		} else { // A.nonzerosInRow[i] < A.nonzerosInRow[i+1]
      162             			local_int_t j = 0;
      163             			for ( j = 0; j < A.nonzerosInRow[i]-1; j+=2 ) {
      164             				float64x2_t values0 = vld1q_f64(&A.matrixValues[i  ][j]);
      165             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      166             
      167             				float64x2_t xvValues0 = { xv[A.mtxIndL[i  ][j]], xv[A.mtxIndL[i  ][j+1]] };
      168             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      169             
      170             				sum0 = vfmaq_f64(sum0, values0, xvValues0);
      171             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      172             
      173             			}
      174             			double s0 = vgetq_lane_f64(sum0, 0) + vgetq_lane_f64(sum0, 1);
      175             			if ( j < A.nonzerosInRow[i] ) {
      176             				s0 += A.matrixValues[i][j] * xv[A.mtxIndL[i][j]];
      177             			}
      178             
      179             			for ( ; j < A.nonzerosInRow[i+1]-1; j+=2 ) {
      180             				float64x2_t values1 = vld1q_f64(&A.matrixValues[i+1][j]);
      181             
      182             				float64x2_t xvValues1 = { xv[A.mtxIndL[i+1][j]], xv[A.mtxIndL[i+1][j+1]] };
      183             
      184             				sum1 = vfmaq_f64(sum1, values1, xvValues1);
      185             			}
      186             			double s1 = vgetq_lane_f64(sum1, 0) + vgetq_lane_f64(sum1, 1);
      187             			if ( j < A.nonzerosInRow[i+1] ) {
      188             				s1 += A.matrixValues[i+1][j] * xv[A.mtxIndL[i+1][j]];
      189             			}
      190             			yv[i  ] = s0;
      191             			yv[i+1] = s1;
      192             		}
      193             	}
      194             #elif defined(HPCG_USE_SVE) && !defined(HPCG_USE_ARMPL_SPMV)
      195             
      196             	if ( nrow % 4 == 0 ) {
      197             #ifndef HPCG_NO_OPENMP
      198             #pragma omp parallel for SCHEDULE(runtime)
      199             #endif
      200             		for ( local_int_t i = 0; i < nrow-3; i+=4 ) {
      201             			local_int_t maxnnz01 = A.nonzerosInRow[i  ] > A.nonzerosInRow[i+1] ? A.nonzerosInRow[i  ] : A.nonzerosInRow[i+1];
      202             			local_int_t maxnnz23 = A.nonzerosInRow[i+2] > A.nonzerosInRow[i+3] ? A.nonzerosInRow[i+2] : A.nonzerosInRow[i+3];
      203             			local_int_t maxnnz = maxnnz01 > maxnnz23 ? maxnnz01 : maxnnz23;
      204             			svfloat64_t svsum0 = svdup_f64(0.0);
      205             			svfloat64_t svsum1 = svdup_f64(0.0);
      206             			svfloat64_t svsum2 = svdup_f64(0.0);
      207             			svfloat64_t svsum3 = svdup_f64(0.0);
      208             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      209             				svbool_t pg0 = svwhilelt_b64(j, A.nonzerosInRow[i  ]);
      210             				svbool_t pg1 = svwhilelt_b64(j, A.nonzerosInRow[i+1]);
      211             				svbool_t pg2 = svwhilelt_b64(j, A.nonzerosInRow[i+2]);
      212             				svbool_t pg3 = svwhilelt_b64(j, A.nonzerosInRow[i+3]);
      213             				svfloat64_t values0 = svld1_f64(pg0, &A.matrixValues[i  ][j]);
      214             				svfloat64_t values1 = svld1_f64(pg1, &A.matrixValues[i+1][j]);
      215             				svfloat64_t values2 = svld1_f64(pg2, &A.matrixValues[i+2][j]);
      216             				svfloat64_t values3 = svld1_f64(pg3, &A.matrixValues[i+3][j]);
      217             				svuint64_t indices0 = svld1sw_u64(pg0, &A.mtxIndL[i  ][j]);
      218             				svuint64_t indices1 = svld1sw_u64(pg1, &A.mtxIndL[i+1][j]);
      219             				svuint64_t indices2 = svld1sw_u64(pg2, &A.mtxIndL[i+2][j]);
      220             				svuint64_t indices3 = svld1sw_u64(pg3, &A.mtxIndL[i+3][j]);
      221             
      222             				svfloat64_t svxv0 = svld1_gather_u64index_f64(pg0, xv, indices0);
      223             				svfloat64_t svxv1 = svld1_gather_u64index_f64(pg1, xv, indices1);
      224             				svfloat64_t svxv2 = svld1_gather_u64index_f64(pg2, xv, indices2);
      225             				svfloat64_t svxv3 = svld1_gather_u64index_f64(pg3, xv, indices3);
      226             				svsum0 = svmla_f64_m(pg0, svsum0, values0, svxv0);
      227             				svsum1 = svmla_f64_m(pg1, svsum1, values1, svxv1);
      228             				svsum2 = svmla_f64_m(pg2, svsum2, values2, svxv2);
      229             				svsum3 = svmla_f64_m(pg3, svsum3, values3, svxv3);
      230             			}
      231             			yv[i  ] = svaddv(svptrue_b64(), svsum0);
      232             			yv[i+1] = svaddv(svptrue_b64(), svsum1);
      233             			yv[i+2] = svaddv(svptrue_b64(), svsum2);
      234             			yv[i+3] = svaddv(svptrue_b64(), svsum3);
      235             		}
      236             	} else if ( nrow % 2 == 0 ) {
      237             #ifndef HPCG_NO_OPENMP
      238             #pragma omp parallel for SCHEDULE(runtime)
      239             #endif
      240             		for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
      241             			local_int_t maxnnz = A.nonzerosInRow[i] > A.nonzerosInRow[i+1] ? A.nonzerosInRow[i] : A.nonzerosInRow[i+1];
      242             			svfloat64_t svsum0 = svdup_f64(0.0);
      243             			svfloat64_t svsum1 = svdup_f64(0.0);
      244             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      245             				svbool_t pg0 = svwhilelt_b64(j, A.nonzerosInRow[i]);
      246             				svbool_t pg1 = svwhilelt_b64(j, A.nonzerosInRow[i+1]);
      247             				svfloat64_t values0 = svld1_f64(pg0, &A.matrixValues[i][j]);
      248             				svfloat64_t values1 = svld1_f64(pg1, &A.matrixValues[i+1][j]);
      249             				svuint64_t indices0 = svld1sw_u64(pg0, &A.mtxIndL[i][j]);
      250             				svuint64_t indices1 = svld1sw_u64(pg1, &A.mtxIndL[i+1][j]);
      251             
      252             				svfloat64_t svxv0 = svld1_gather_u64index_f64(pg0, xv, indices0);
      253             				svfloat64_t svxv1 = svld1_gather_u64index_f64(pg1, xv, indices1);
      254             				svsum0 = svmla_f64_m(pg0, svsum0, values0, svxv0);
      255             				svsum1 = svmla_f64_m(pg1, svsum1, values1, svxv1);
      256             			}
      257             			yv[i] = svaddv(svptrue_b64(), svsum0);
      258             			yv[i+1] = svaddv(svptrue_b64(), svsum1);
      259             		}
      260             	} else {
      261             #ifndef HPCG_NO_OPENMP
      262             #pragma omp parallel for SCHEDULE(runtime)
      263             #endif
      264             		for ( local_int_t i = 0; i < nrow; i++ ) {
      265             			local_int_t maxnnz = A.nonzerosInRow[i];
      266             			svfloat64_t svsum = svdup_f64(0.0);
      267             			for ( local_int_t j = 0; j < maxnnz; j += svcntd() ) {
      268             				svbool_t pg = svwhilelt_b64(j, maxnnz);
      269             				svfloat64_t values = svld1_f64(pg, &A.matrixValues[i][j]);
      270             				svuint64_t indices = svld1sw_u64(pg, &A.mtxIndL[i][j]);
      271             
      272             				svfloat64_t svxv = svld1_gather_u64index_f64(pg, xv, indices);
      273             				svsum = svmla_f64_m(pg, svsum, values, svxv);
      274             			}
      275             			yv[i] = svaddv(svptrue_b64(), svsum);
      276             		}
      277             	}
      278             #elif defined(HPCG_USE_ARMPL_SPMV)
      279             	double alpha = 1.0;
      280             	double beta = 0.0;
      281             
      282             	armpl_spmv_exec_d(ARMPL_SPARSE_OPERATION_NOTRANS, alpha, A.armpl_mat, xv, beta, yv);
      283             
      284             #elif defined(HPCG_MAN_OPT_SPMV_UNROLL)
      285             	ComputeSPMV_unroll2(A, xv, yv, nrow);
      286             /*#ifndef HPCG_NO_OPENMP
      287             #pragma omp parallel for SCHEDULE(runtime)
      288             #endif
      289             	for ( local_int_t i = 0; i < nrow-1; ++i ) {
      290             		double sum0 = 0.0;
      291             		double sum1 = 0.0;
      292             		if (A.nonzerosInRow[i] == A.nonzerosInRow[i+1]) {
      293             			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      294             				local_int_t curCol0 = A.mtxIndL[i][j];
      295             				local_int_t curCol1 = A.mtxIndL[i+1][j];
      296             
      297             				sum0 += A.matrixValues[i][j] * xv[curCol0];
      298             				sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      299             			}
      300             			yv[i] = sum0;
      301             			yv[i+1] = sum1;
      302             			++i;
      303             		}
      304             		else {
      305             			double sum = 0.0;
      306             			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      307             				local_int_t curCol = A.mtxIndL[i][j];
      308             				sum += A.matrixValues[i][j] * xv[curCol];
      309             			}
      310             			yv[i] = sum;
      311             			++i;
      312             			//if (i < nrow) {
      313             				sum = 0.0;
      314             				for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      315             					local_int_t curCol = A.mtxIndL[i][j];
      316             					sum += A.matrixValues[i][j] * xv[curCol];
      317             				}
      318             				yv[i] = sum;
      319             			//}
      320             		}
      321             	}*/
      322             #else
      323             #ifndef HPCG_NO_OPENMP
      324             #pragma omp parallel for SCHEDULE(runtime)
      325             #endif
      326             	for ( local_int_t i = 0; i < nrow; i++ ) {
      327             		double sum = 0.0;
      328             		for ( local_int_t j = 0; j < A.nonzerosInRow[i]; j++ ) {
      329             			local_int_t curCol = A.mtxIndL[i][j];
      330             			sum += A.matrixValues[i][j] * xv[curCol];
      331             		}
      332             		yv[i] = sum;
      333             	}
      334             #endif
      335             
      336             	return 0;
      337             }
      338             
      339             #ifdef HPCG_MAN_OPT_SPMV_UNROLL
      340             inline void ComputeSPMV_unroll2(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow) {
      341             #ifndef HPCG_NO_OPENMP
      342             #pragma omp parallel for SCHEDULE(runtime)
      343             #endif
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      344   p         	for ( local_int_t i = 0; i < nrow-1; i+=2 ) {
      345   p         		double sum0 = 0.0;
      346   p         		double sum1 = 0.0;
      347   p         		if (A.nonzerosInRow[i] == A.nonzerosInRow[i+1]) {
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.22, ITR: 96, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      348   p     4v  			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      349   p     4v  				local_int_t curCol0 = A.mtxIndL[i][j];
      350   p     4v  				local_int_t curCol1 = A.mtxIndL[i+1][j];
      351             
      352   p     4v  				sum0 += A.matrixValues[i][j] * xv[curCol0];
      353   p     4v  				sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      354   p     4v  			}
      355   p         			yv[i] = sum0;
      356   p         			yv[i+1] = sum1;
      357   p         		}
      358   p         		else {
      359   p         			double sum = 0.0;
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      360   p     8v  			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      361   p     8v  				local_int_t curCol = A.mtxIndL[i][j];
      362   p     8v  				sum += A.matrixValues[i][j] * xv[curCol];
      363   p     8v  			}
      364   p         			yv[i] = sum;
      365             
      366   p         			sum = 0.0;
                       <<< Loop-information Start >>>
                       <<<  [OPTIMIZATION]
                       <<<    SIMD(VL: 8)
                       <<<    SOFTWARE PIPELINING(IPC: 1.16, ITR: 192, MVE: 2, POL: S)
                       <<<    PREFETCH(HARD) Expected by compiler :
                       <<<      (unknown)
                       <<< Loop-information  End >>>
      367   p     8v  			for ( local_int_t j = 0; j < A.nonzerosInRow[i+1]; ++j ) {
      368   p     8v  				local_int_t curCol = A.mtxIndL[i+1][j];
      369   p     8v  				sum += A.matrixValues[i+1][j] * xv[curCol];
      370   p     8v  			}
      371   p         			yv[i+1] = sum;
      372   p         		}
      373   p         	}
      374             }
      375             
      376             inline void ComputeSPMV_unroll4(const SparseMatrix & A, const double * const xv, double * const yv,	const local_int_t nrow) {
      377             
      378             #ifndef HPCG_NO_OPENMP
      379             #pragma omp parallel for SCHEDULE(runtime)
      380             #endif
      381             	for ( local_int_t i = 0; i < nrow-3; i+=4) {
      382             		double sum0 = 0.0, sum1 = 0.0, sum2 = 0.0, sum3 = 0.0;
      383             
      384             		if ((A.nonzerosInRow[i] == A.nonzerosInRow[i+1]) && (A.nonzerosInRow[i] == A.nonzerosInRow[i+2]) && (A.nonzerosInRow[i] == A.nonzerosInRow[i+3]) ){
      385             			for ( local_int_t j = 0; j < A.nonzerosInRow[i]; ++j ) {
      386             				local_int_t curCol0 = A.mtxIndL[i][j];
      387             				local_int_t curCol1 = A.mtxIndL[i+1][j];
      388             				local_int_t curCol2 = A.mtxIndL[i+2][j];
      389             				local_int_t curCol3 = A.mtxIndL[i+3][j];
      390             
      391             				sum0 += A.matrixValues[i][j] * xv[curCol0];
      392             				sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      393             				sum2 += A.matrixValues[i+2][j] * xv[curCol2];
      394             				sum3 += A.matrixValues[i+3][j] * xv[curCol3];
      395             			}
      396             
      397             			yv[i] = sum0;
      398             			yv[i+1] = sum1;
      399             			yv[i+2] = sum2;
      400             			yv[i+3] = sum3;
      401             		}
      402             		else {
      403             			for ( local_int_t ix = 0; ix < 4; ++ix) {
      404             				double sum = 0.0;
      405             				for ( local_int_t j = 0; j < A.nonzerosInRow[i+ix]; ++j ) {
      406             					local_int_t curCol = A.mtxIndL[i+ix][j];
      407             					sum += A.matrixValues[i+ix][j] * xv[curCol];
      408             				}
      409             				yv[i+ix] = sum;
      410             			}
      411             		}
      412             /*
      413             		local_int_t max1 = A.nonzerosInRow[i  ];
      414             		local_int_t min1 = A.nonzerosInRow[i+1];
      415             		local_int_t tmp = max1;
      416             		if (max1 < min1) { max1 = min1; min1 = tmp; } 
      417             
      418             		local_int_t max2 = A.nonzerosInRow[i+2];
      419             		local_int_t min2 = A.nonzerosInRow[i+3];
      420             		tmp = max2;
      421             		if (max2 < min2) { max2 = min2; min2 = tmp; }
      422             
      423             		local_int_t max = max2 > max1 ? max2 : max1;
      424             		local_int_t min = min1 < min2 ? min1 : min2;
      425             
      426             		for ( local_int_t j = 0; j < min; ++j ) {
      427             			local_int_t curCol0 = A.mtxIndL[i][j];
      428             			local_int_t curCol1 = A.mtxIndL[i+1][j];
      429             			local_int_t curCol2 = A.mtxIndL[i+2][j];
      430             			local_int_t curCol3 = A.mtxIndL[i+3][j];
      431             
      432             			sum0 += A.matrixValues[i][j] * xv[curCol0];
      433             			sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      434             			sum2 += A.matrixValues[i+2][j] * xv[curCol2];
      435             			sum3 += A.matrixValues[i+3][j] * xv[curCol3];
      436             		}
      437             		for ( local_int_t j=min; j < max; ++j ) {
      438             			local_int_t curCol0 = (j<A.nonzerosInRow[i  ]) ? A.mtxIndL[i][j] : 0;
      439             			local_int_t curCol1 = (j<A.nonzerosInRow[i+1]) ? A.mtxIndL[i+1][j] : 0;
      440             			local_int_t curCol2 = (j<A.nonzerosInRow[i+2]) ? A.mtxIndL[i+2][j] : 0;
      441             			local_int_t curCol3 = (j<A.nonzerosInRow[i+3]) ? A.mtxIndL[i+3][j] : 0;
      442             
      443             			sum0 += A.matrixValues[i][j] * xv[curCol0];
      444             			sum1 += A.matrixValues[i+1][j] * xv[curCol1];
      445             			sum2 += A.matrixValues[i+2][j] * xv[curCol2];
      446             			sum3 += A.matrixValues[i+3][j] * xv[curCol3];
      447             		}
      448             
      449             		yv[i] = sum0;
      450             		yv[i+1] = sum1;
      451             		yv[i+2] = sum2;
      452             		yv[i+3] = sum3;*/
      453             	}			
      454             }
      455             #endif //HPCG_MAN_OPT_SPMV_UNROLL
Total prefetch num: 0
Optimization messages
  jwd6004s-i  "../src/ComputeSPMV.cpp", line 348: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSPMV.cpp", line 348: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSPMV.cpp", line 348: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 96.
  jwd8208o-i  "../src/ComputeSPMV.cpp", line 352: Method of calculating sum or product is changed.
  jwd8208o-i  "../src/ComputeSPMV.cpp", line 353: Method of calculating sum or product is changed.
  jwd6004s-i  "../src/ComputeSPMV.cpp", line 360: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSPMV.cpp", line 360: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSPMV.cpp", line 360: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSPMV.cpp", line 362: Method of calculating sum or product is changed.
  jwd6004s-i  "../src/ComputeSPMV.cpp", line 367: SIMD conversion is applied to this loop with the loop variable 'j'. The loop contains a reduction operation.
  jwd8204o-i  "../src/ComputeSPMV.cpp", line 367: This loop is software pipelined.
  jwd8205o-i  "../src/ComputeSPMV.cpp", line 367: The software-pipelined loop is chosen at run time when the iteration count is greater than or equal to 192.
  jwd8208o-i  "../src/ComputeSPMV.cpp", line 369: Method of calculating sum or product is changed.
Statistics information
  Option information
    Command line options : -c -DHPCG_CONTIGUOUS_ARRAYS -DHPCG_NO_MPI -DHPCG_MAN_OPT_SPMV_UNROLL -DSPMV_2_UNROLL -DHPCG_MAN_OPT_SCHEDULE_ON -I./src -I./src/OOKAMI_OMP_FJ -fopenmp -pthread -DLIKWID_PERFMON -DLIKWID_INSTRUMENTATION -Kfast -KSVE -Kopenmp -ffast-math -funroll-loops -std=c++11 -ffp-contract=fast -march=armv8.2-a+sve -Koptmsg=2 -Nlst=t -I../src -o src/ComputeSPMV.o
    Effective options    : -g0 -mt -Qy -std=gnu++11 -x- -x=quick -O3 -Knoalias_const
                           -Kalign_loops -Knoarray_declaration_opt -Kassume=noshortloop
                           -Kassume=nomemory_bandwidth -Kassume=notime_saving_compilation
                           -Kcmodel=small -Keval -Keval_noconcurrent
                           -Knoextract_stride_store -Kfast_matmul -Knofenv_access
                           -Kfp_contract -Kfp_relaxed -Kfsimple -Kfz -Khpctag
                           -Kilfunc=procedure -Klargepage -Klib -Kloop_blocking
                           -Kloop_fission -Kloop_nofission_stripmining
                           -Kloop_fission_threshold=50 -Kloop_fusion -Kloop_interchange
                           -Kloop_part_simd -Kloop_perfect_nest -Kloop_noversioning
                           -Klooptype=f -Knomemalias -Kmfunc=1 -Knoocl -Komitfp -Kopenmp
                           -Kopenmp_noassume_norecurrence
                           -Kopenmp_nocollapse_except_innermost
                           -Kopenmp_loop_variable=private -Kopenmp_noordered_reduction
                           -Knoopenmp_simd -Knooptlib_string -Koptmsg=2
                           -Knopc_relative_literal_loads -Knoparallel
                           -Kparallel_nofp_precision -Knopreex -Kprefetch_cache_level=all
                           -Kprefetch_noconditional -Kprefetch_noindirect -Kprefetch_noinfer
                           -Kprefetch_sequential=auto -Kprefetch_nostride -Kprefetch_strong
                           -Kprefetch_strong_L2 -Knopreload -Krdconv=1
                           -Kremove_inlinefunction -Knorestp -Ksch_post_ra -Ksch_pre_ra
                           -Ksibling_calls -Ksimd=auto -Ksimd_packed_promotion
                           -Ksimd_reduction_product -Ksimd_reg_size=512
                           -Ksimd_nouncounted_loop -Ksimd_use_multiple_structures
                           -Knostrict_aliasing -Knostriping -KA64FX -KARMV8_2_A -KSVE -Kswp
                           -Kswp_freg_rate=100 -Kswp_ireg_rate=100 -Kswp_preg_rate=100
                           -Kswp_policy=auto -Kunroll -Knounroll_and_jam -Knozfill
                           -Ncancel_overtime_compilation -Nnocoverage -Nexceptions -Nnofjcex
                           -Nfjprof -Nnohook_func -Nnohook_time -Nlibomp -Nline -Nlst=p
                           -Nlst=t -Nquickdbg=noheapchk -Nquickdbg=nosubchk -NRnotrap
                           -Nnoreordered_variable_stack -Nrt_notune -Nsetvalue=noheap
                           -Nsetvalue=nostack -Nsetvalue=noscalar -Nsetvalue=noarray
                           -Nsetvalue=nostruct -Nsrc -Nsta
