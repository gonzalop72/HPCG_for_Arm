### Starting TaskPrologue of job 112176 on f1104 at Wed Jul 27 18:37:43 CEST 2022
#   SLURM_JOB_NODELIST=f1104
#   SLURM_JOB_NUM_NODES=1
#   SLURM_NTASKS=37
#   SLURM_NPROCS=37
#   SLURM_TASKS_PER_NODE=37
#   SLURM_JOB_CPUS_PER_NODE=72
#   SLURM_EXPORT_ENV=
Running on cores 0-71 with govenor powersave
### Finished TaskPrologue
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
MPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found
Abort(127) on node 1 (rank 1 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 1
Abort(127) on node 2 (rank 2 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 2
Abort(127) on node 4 (rank 4 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 4
Abort(127) on node 5 (rank 5 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 5
Abort(127) on node 6 (rank 6 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 6
Abort(127) on node 7 (rank 7 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 7
Abort(127) on node 8 (rank 8 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 8
Abort(127) on node 9 (rank 9 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 9
Abort(127) on node 10 (rank 10 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 10
Abort(127) on node 11 (rank 11 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 11
Abort(127) on node 12 (rank 12 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 12
Abort(127) on node 13 (rank 13 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 13
Abort(127) on node 14 (rank 14 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 14
Abort(127) on node 15 (rank 15 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 15
Abort(127) on node 16 (rank 16 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 16
Abort(127) on node 17 (rank 17 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 17
Abort(127) on node 18 (rank 18 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 18
Abort(127) on node 19 (rank 19 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 19
Abort(127) on node 20 (rank 20 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 20
Abort(127) on node 21 (rank 21 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 21
Abort(127) on node 22 (rank 22 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 22
Abort(127) on node 23 (rank 23 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 23
Abort(127) on node 24 (rank 24 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 24
Abort(127) on node 25 (rank 25 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 25
Abort(127) on node 26 (rank 26 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 26
Abort(127) on node 27 (rank 27 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 27
Abort(127) on node 29 (rank 29 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 29
Abort(127) on node 30 (rank 30 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 30
Abort(127) on node 31 (rank 31 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 31
Abort(127) on node 32 (rank 32 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 32
Abort(127) on node 33 (rank 33 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 33
Abort(127) on node 3 (rank 3 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 3
Abort(127) on node 28 (rank 28 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 28
Abort(127) on node 34 (rank 34 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 34
Abort(127) on node 35 (rank 35 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 35
Abort(127) on node 36 (rank 36 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 127) - process 36
=== JOB_STATISTICS ===
=== current date     : Wed Jul 27 18:37:47 CEST 2022
= Job-ID             : 112176 on fritz
= Job-Name           : job_mpi.sh
= Job-Command        : /home/hpc/ihpc/ihpc061h/arm_code/v2022/HPCG_for_Arm/Fritz_Intel/bin/job_mpi.sh
= Initial workdir    : /home/hpc/ihpc/ihpc061h/arm_code/v2022/HPCG_for_Arm/Fritz_Intel/bin
= Queue/Partition    : singlenode
= Slurm account      : ihpc with QOS=normal
= Requested resources: cpu=72,node=1,billing=72 for 00:30:00
= Elapsed runtime    : 00:00:05
= Total RAM usage    : 0.0 GiB 
= Node list          : f1104
= Subm/Elig/Start/End: 2022-07-27T18:25:32 / 2022-07-27T18:25:32 / 2022-07-27T18:37:42 / 2022-07-27T18:37:47
======================
=== Quota infos ======
    Path              Used     SoftQ    HardQ    Gracetime  Filec    FileQ    FiHaQ    FileGrace    
    /home/hpc           22.7G    52.4G   104.9G        N/A  14,397      500K   1,000K        N/A    
    /home/woody          4.0K   500.0G   999.0G        N/A       1                           N/A    
    /home/vault          0.0K   524.3G  1048.6G        N/A       1      200K     400K        N/A    
======================
